<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d2" for="node" attr.name="path" attr.type="string"/>
<key id="d1" for="node" attr.name="content" attr.type="string"/>
<key id="d0" for="node" attr.name="type" attr.type="string"/>
<graph edgedefault="directed"><node id="add_rate_card">
  <data key="d0">module</data>
  <data key="d1">"""Add a new rate card to the database."""
from datetime import datetime, timezone
from decimal import Decimal

from warehouse_quote_app.models import RateCard, RateCardSettings, CustomerRateCard
from .base_script import DatabaseScript

def add_rate_cards():
    """Add rate cards to the database."""
    script = DatabaseScript(models=[RateCard, RateCardSettings, CustomerRateCard])
    
    with script as db:
        # Create settings first
        settings = RateCardSettings(
            name="Default Settings",
            description="Default rate card settings",
            is_active=True,
            minimum_charge=Decimal("0.00"),
            handling_fee_percentage=Decimal("0.00"),
            tax_rate=Decimal("0.10"),
            volume_discount_tiers={
                "tier1": {"min_amount": 1000, "discount": 0.05},
                "tier2": {"min_amount": 5000, "discount": 0.10},
                "tier3": {"min_amount": 10000, "discount": 0.15}
            }
        )
        script.add_and_flush(db, settings)

        # Create rate cards
        cards = [
            RateCard(
                name="Standard Pallet Storage",
                description="Internal Racked Storage for Standard Pallet (1.2 x 1.2 x 1.65m)",
                is_active=True,
                effective_from=datetime.now(timezone.utc),
                settings_id=settings.id,
                rates={
                    "storage": {
                        "pallet": {
                            "rate": "4.50",
                            "unit": "per pallet per day",
                            "min_units": 1
                        }
                    },
                    "handling": {
                        "inbound": {
                            "rate": "6.00",
                            "unit": "per pallet",
                            "min_units": 1
                        },
                        "outbound": {
                            "rate": "6.00",
                            "unit": "per pallet",
                            "min_units": 1
                        }
                    }
                }
            ),
            RateCard(
                name="Bulk Storage",
                description="Floor Space Storage for Bulk Items",
                is_active=True,
                effective_from=datetime.now(timezone.utc),
                settings_id=settings.id,
                rates={
                    "storage": {
                        "floor_space": {
                            "rate": "2.50",
                            "unit": "per square meter per day",
                            "min_units": 10
                        }
                    },
                    "handling": {
                        "inbound": {
                            "rate": "45.00",
                            "unit": "per hour",
                            "min_units": 1
                        },
                        "outbound": {
                            "rate": "45.00",
                            "unit": "per hour",
                            "min_units": 1
                        }
                    }
                }
            )
        ]
        script.bulk_save(db, cards)

if __name__ == "__main__":
    add_rate_cards()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\deployment\scripts\db\add_rate_card.py</data>
</node>
<node id="base_script">
  <data key="d0">module</data>
  <data key="d1">"""Base script for database operations."""
import sys
from pathlib import Path
from typing import List, Type, Optional
from sqlalchemy.orm import Session

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from warehouse_quote_app.database import SessionLocal, engine
from warehouse_quote_app.models.base import BaseModel

class DatabaseScript:
    """Base class for database scripts."""

    def __init__(self, models: Optional[List[Type[BaseModel]]] = None):
        """Initialize script with models to create."""
        self.models = models or []
        self.db: Optional[Session] = None

    def __enter__(self) -&gt; Session:
        """Create tables and return session."""
        # Create tables for specified models
        for model in self.models:
            model.metadata.create_all(bind=engine)
        
        # Create and return session
        self.db = SessionLocal()
        return self.db

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Close session."""
        if self.db:
            if exc_type:
                self.db.rollback()
            else:
                self.db.commit()
            self.db.close()

    @staticmethod
    def add_and_flush(db: Session, *objects: BaseModel) -&gt; None:
        """Add objects to session and flush."""
        for obj in objects:
            db.add(obj)
        db.flush()

    @staticmethod
    def bulk_save(db: Session, objects: List[BaseModel]) -&gt; None:
        """Bulk save objects to database."""
        db.bulk_save_objects(objects)
        db.flush()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\deployment\scripts\db\base_script.py</data>
</node>
<node id="create_initial_rates">
  <data key="d0">module</data>
  <data key="d1">"""Create initial rate cards."""
from datetime import datetime, timezone
from decimal import Decimal
import sys
import os
from pathlib import Path

# Add the parent directory to Python path to import app modules
parent_dir = str(Path(__file__).resolve().parent.parent)
sys.path.append(parent_dir)

from app.database import SessionLocal, Base, engine
from app.models.rate_card import RateCard, RateCardSettings
from app.models.customer import Customer
from app.models.quote import Quote
from app.models.user import User

# Create tables
Base.metadata.create_all(bind=engine)

def create_initial_rates():
    """Create initial rate cards."""
    db = SessionLocal()
    try:
        # First create rate card settings
        settings = RateCardSettings(
            name="Default Settings",
            description="Default rate card settings",
            is_active=True,
            minimum_charge=Decimal("0.00"),
            handling_fee_percentage=Decimal("0.00"),
            tax_rate=Decimal("0.10")
        )
        db.add(settings)
        db.flush()  # This will assign an ID to settings

        # Create standard rate card
        standard_rate = RateCard(
            name="Standard Pallet Storage",
            description="Internal Racked Storage for Standard Pallet (1.2 x 1.2 x 1.65m)",
            is_active=True,
            effective_from=datetime.now(timezone.utc),
            rates={
                "storage": {
                    "pallet": {
                        "rate": "4.50",
                        "unit": "per_day",
                        "min_quantity": 1
                    }
                }
            },
            settings_id=settings.id
        )
        db.add(standard_rate)

        # Create premium rate card
        premium_rate = RateCard(
            name="Premium Climate-Controlled Storage",
            description="Temperature and Humidity Controlled Storage for Sensitive Items",
            is_active=True,
            effective_from=datetime.now(timezone.utc),
            rates={
                "storage": {
                    "pallet": {
                        "rate": "6.50",
                        "unit": "per_day",
                        "min_quantity": 1
                    }
                }
            },
            settings_id=settings.id
        )
        db.add(premium_rate)

        # Create enterprise rate card
        enterprise_rate = RateCard(
            name="Enterprise Bulk Storage",
            description="Large Scale Warehouse Storage Solutions",
            is_active=True,
            effective_from=datetime.now(timezone.utc),
            rates={
                "storage": {
                    "pallet": {
                        "rate": "4.00",
                        "unit": "per_day",
                        "min_quantity": 10
                    },
                    "square_meter": {
                        "rate": "2.50",
                        "unit": "per_day",
                        "min_quantity": 100
                    }
                }
            },
            settings_id=settings.id
        )
        db.add(enterprise_rate)

        db.commit()
        print("Successfully created initial rate cards!")
        
    except Exception as e:
        print(f"Error creating rate cards: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    create_initial_rates()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\deployment\scripts\db\create_initial_rates.py</data>
</node>
<node id="populate_rates">
  <data key="d0">module</data>
  <data key="d1">from decimal import Decimal
from sqlalchemy.orm import Session
from app.database import SessionLocal
from app.crud.rate_card import RateCardService
from app.schemas.rate_card import RateCardCreate, RateType, RateCategory, ClientType

def populate_rates(db: Session):
    rate_service = RateCardService()
    
    # Storage and Handling Rates
    rates = [
        # Internal Storage Rates
        RateCardCreate(
            name="Internal Pallet Storage",
            description="Pallet Storage - Internal Racked 1.2 x 1.2 x 1.65m",
            rate_type=RateType.STORAGE,
            category=RateCategory.PALLET,
            base_rate=Decimal("5.00"),
            min_charge=Decimal("40.00"),
            per_unit="per_pallet_per_week",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        RateCardCreate(
            name="External Pallet Storage",
            description="Pallet Storage - External Covered Area",
            rate_type=RateType.STORAGE,
            category=RateCategory.PALLET,
            base_rate=Decimal("3.50"),
            min_charge=Decimal("30.00"),
            per_unit="per_pallet_per_week",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        RateCardCreate(
            name="Bulk Storage",
            description="Bulk Storage Area - Floor Space",
            rate_type=RateType.STORAGE,
            category=RateCategory.BULK,
            base_rate=Decimal("2.00"),
            min_charge=Decimal("100.00"),
            per_unit="per_sqm_per_week",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        
        # Handling Rates
        RateCardCreate(
            name="Pallet In",
            description="Unloading and Put-away of Pallets",
            rate_type=RateType.HANDLING,
            category=RateCategory.INBOUND,
            base_rate=Decimal("6.50"),
            min_charge=Decimal("50.00"),
            per_unit="per_pallet",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        RateCardCreate(
            name="Pallet Out",
            description="Picking and Loading of Pallets",
            rate_type=RateType.HANDLING,
            category=RateCategory.OUTBOUND,
            base_rate=Decimal("6.50"),
            min_charge=Decimal("50.00"),
            per_unit="per_pallet",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        
        # Value-Added Services
        RateCardCreate(
            name="Labeling",
            description="Product Labeling Service",
            rate_type=RateType.VALUE_ADDED,
            category=RateCategory.LABELING,
            base_rate=Decimal("0.50"),
            min_charge=Decimal("25.00"),
            per_unit="per_item",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        RateCardCreate(
            name="Repacking",
            description="Product Repacking Service",
            rate_type=RateType.VALUE_ADDED,
            category=RateCategory.REPACKING,
            base_rate=Decimal("1.00"),
            min_charge=Decimal("30.00"),
            per_unit="per_item",
            client_type=ClientType.ALL,
            conditions=None,
            is_active=True
        ),
        
        # Transport Rates
        RateCardCreate(
            name="Local Delivery - Small Van",
            description="Local Delivery Service using Small Van",
            rate_type=RateType.TRANSPORT,
            category=RateCategory.LOCAL_DELIVERY,
            base_rate=Decimal("80.00"),
            min_charge=Decimal("80.00"),
            per_unit="per_trip",
            client_type=ClientType.ALL,
            conditions={"max_weight": 1000, "max_pallets": 2},
            is_active=True
        ),
        RateCardCreate(
            name="Local Delivery - Medium Truck",
            description="Local Delivery Service using Medium Truck",
            rate_type=RateType.TRANSPORT,
            category=RateCategory.LOCAL_DELIVERY,
            base_rate=Decimal("120.00"),
            min_charge=Decimal("120.00"),
            per_unit="per_trip",
            client_type=ClientType.ALL,
            conditions={"max_weight": 4000, "max_pallets": 6},
            is_active=True
        ),
        
        # Special Rates for Corporate Clients
        RateCardCreate(
            name="Corporate Bulk Storage",
            description="Bulk Storage Area - Floor Space (Corporate Rate)",
            rate_type=RateType.STORAGE,
            category=RateCategory.BULK,
            base_rate=Decimal("1.80"),
            min_charge=Decimal("90.00"),
            per_unit="per_sqm_per_week",
            client_type=ClientType.CORPORATE,
            conditions={"min_volume": 100},
            is_active=True
        ),
        RateCardCreate(
            name="Corporate Pallet Storage",
            description="Pallet Storage - Internal (Corporate Rate)",
            rate_type=RateType.STORAGE,
            category=RateCategory.PALLET,
            base_rate=Decimal("4.50"),
            min_charge=Decimal("35.00"),
            per_unit="per_pallet_per_week",
            client_type=ClientType.CORPORATE,
            conditions={"min_pallets": 50},
            is_active=True
        ),
    ]
    
    # Create rates in database
    for rate in rates:
        try:
            rate_service.create_rate(db, rate)
            print(f"Created rate: {rate.name}")
        except Exception as e:
            print(f"Error creating rate {rate.name}: {str(e)}")

if __name__ == "__main__":
    db = SessionLocal()
    try:
        populate_rates(db)
    finally:
        db.close()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\deployment\scripts\db\populate_rates.py</data>
</node>
<node id="seed_rate_cards">
  <data key="d0">module</data>
  <data key="d1">#!/usr/bin/env python
"""
Rate Card Seed Script
This script populates the database with initial rate card data for AU Logistics.
"""

import sys
import os
from pathlib import Path
from decimal import Decimal
from datetime import datetime, timezone

# Add the parent directory to Python path to import app modules
parent_dir = str(Path(__file__).resolve().parent.parent)
sys.path.append(parent_dir)

from app.database import Base, get_db, SessionLocal, engine
# Import all models to register them with SQLAlchemy
from app.models import rate_card, customer, quote, user  # noqa: F401
from app.models.rate_card import RateCard
from app.models.types import RateType, RateCategory
from sqlalchemy.exc import SQLAlchemyError
import logging
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create all tables
Base.metadata.create_all(bind=engine)

rate_cards_data: List[Dict[str, Any]] = [
    # Storage Rates
    {
        "name": "Standard Pallet Storage",
        "description": "Internal Racked Storage for Standard Pallet (1.2 x 1.2 x 1.65m)",
        "rate_type": RateType.STORAGE,
        "category": RateCategory.STANDARD,
        "base_rates": {
            "pallet": Decimal("4.50")
        },
        "is_active": True,
        "effective_from": datetime.now(timezone.utc)
    },
    {
        "name": "Premium Pallet Storage",
        "description": "Climate Controlled Storage for Sensitive Items",
        "rate_type": RateType.STORAGE,
        "category": RateCategory.PREMIUM,
        "base_rates": {
            "pallet": Decimal("6.50")
        },
        "is_active": True,
        "effective_from": datetime.now(timezone.utc)
    },
    {
        "name": "Enterprise Bulk Storage",
        "description": "Large Scale Warehouse Storage Solutions",
        "rate_type": RateType.STORAGE,
        "category": RateCategory.ENTERPRISE,
        "base_rates": {
            "sqm": Decimal("2.50"),
            "pallet": Decimal("4.00")
        },
        "is_active": True,
        "effective_from": datetime.now(timezone.utc)
    }
]

def validate_rate_card(data: Dict[str, Any]) -&gt; None:
    """Validate rate card data before insertion."""
    required_fields = ["name", "description", "rate_type", "category", "base_rates", "is_active", "effective_from"]
    for field in required_fields:
        if field not in data:
            raise ValueError(f"Missing required field: {field}")

def seed_rate_cards() -&gt; None:
    """Seed the rate cards table with initial data."""
    logger.info("Starting rate card seeding process...")
    
    # Create database session
    db = SessionLocal()
    try:
        # Deactivate all existing rate cards
        existing_rate_cards = db.query(RateCard).filter(RateCard.is_active == True).all()
        for rate_card in existing_rate_cards:
            rate_card.is_active = False
        
        # Insert new rate cards
        for data in rate_cards_data:
            try:
                validate_rate_card(data)
                rate_card = RateCard(
                    name=data["name"],
                    description=data["description"],
                    rate_type=data["rate_type"],
                    category=data["category"],
                    base_rates=data["base_rates"],
                    is_active=data["is_active"],
                    effective_from=data["effective_from"]
                )
                db.add(rate_card)
                logger.info(f"Added rate card: {data['name']}")
            except Exception as e:
                logger.error(f"Error adding rate card {data.get('name', 'unknown')}: {str(e)}")
                continue
        
        db.commit()
        logger.info("Rate card seeding completed successfully")
    
    except SQLAlchemyError as e:
        logger.error(f"Database error: {str(e)}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    try:
        seed_rate_cards()
    except Exception as e:
        logger.error(f"Failed to seed rate cards: {str(e)}")
        sys.exit(1)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\deployment\scripts\db\seed_rate_cards.py</data>
</node>
<node id="analyze_health">
  <data key="d0">module</data>
  <data key="d1">"""
Analyze codebase health and generate a report
"""

import sys
from pathlib import Path
import json
from datetime import datetime
import argparse
import logging
from tqdm import tqdm
import time

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from kg.semantic_context_manager import SemanticContextManager
from kg.cleanup.code_health_manager import CodeHealthManager, HealthIssueType, SeverityLevel

def setup_logging():
    """Configure logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('analyze_health.log')
        ]
    )

def format_health_summary(health_report, duration: float) -&gt; str:
    """Format health analysis summary"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Group issues by type and severity
    issues_by_type = {}
    issues_by_severity = {}
    
    for issue in health_report.issues:
        # Group by type
        if issue.type not in issues_by_type:
            issues_by_type[issue.type] = []
        issues_by_type[issue.type].append(issue)
        
        # Group by severity
        if issue.severity not in issues_by_severity:
            issues_by_severity[issue.severity] = []
        issues_by_severity[issue.severity].append(issue)
    
    # Format summary
    summary = [
        "Codebase Health Analysis Complete",
        "=" * 50,
        f"Health Score: {health_report.health_score:.2f}%",
        f"Analysis Duration: {duration:.1f} seconds\n",
        
        "Reports generated:",
        f"- Detailed report: health_report_{timestamp}.json",
        f"- Summary report: health_summary_{timestamp}.md\n",
        
        "Key Findings:",
        f"- Found {len(health_report.issues)} potential issues"
    ]
    
    # Add issue counts by type if any exist
    if issues_by_type:
        summary.append("\nIssues by Type:")
        for issue_type, issues in issues_by_type.items():
            summary.append(f"- {issue_type.value}: {len(issues)}")
    
    # Add issue counts by severity if any exist
    if issues_by_severity:
        summary.append("\nIssues by Severity:")
        for severity, issues in issues_by_severity.items():
            summary.append(f"- {severity.value}: {len(issues)}")
    
    # Add recommendations if health score is below threshold
    if health_report.health_score &lt; 95:
        summary.extend([
            "\nNext Steps:",
            "1. Review the summary report for an overview",
            "2. Check the detailed report for comprehensive findings",
            "3. Address any high/critical priority issues"
        ])
    
    return "\n".join(summary)

def main():
    parser = argparse.ArgumentParser(description='Analyze codebase health')
    parser.add_argument('--report', action='store_true', help='Generate detailed report')
    parser.add_argument('--path', default='.', help='Path to analyze')
    args = parser.parse_args()

    setup_logging()
    logging.info("Starting codebase health analysis...")
    start_time = time.time()

    try:
        # Initialize managers with progress feedback
        logging.info("Initializing semantic context manager...")
        semantic_manager = SemanticContextManager(Path("./semantic_context"))
        
        logging.info("Initializing health manager...")
        health_manager = CodeHealthManager(semantic_manager)
        
        # Run health analysis
        logging.info("Running health analysis...")
        health_report = health_manager.analyze_codebase(args.path)
        
        # Calculate duration
        duration = time.time() - start_time
        
        # Generate summary
        summary = format_health_summary(health_report, duration)
        print("\n" + summary + "\n")
        
        # Generate detailed report if requested
        if args.report:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = Path(f"health_report_{timestamp}.json")
            summary_file = Path(f"health_summary_{timestamp}.md")
            
            logging.info(f"Saving detailed report to {report_file}")
            with open(report_file, 'w') as f:
                json.dump(health_report.to_dict(), f, indent=2)
            
            logging.info(f"Saving summary to {summary_file}")
            with open(summary_file, 'w') as f:
                f.write(summary)
        
        logging.info(f"Analysis completed in {duration:.2f} seconds")
        
    except Exception as e:
        logging.error(f"Error during analysis: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\analyze_health.py</data>
</node>
<node id="update_kg">
  <data key="d0">module</data>
  <data key="d1">"""
Update Knowledge Graph
"""

import sys
from pathlib import Path

# Add tools directory to Python path
tools_dir = Path(__file__).parent
if str(tools_dir) not in sys.path:
    sys.path.append(str(tools_dir))

from kg.generate_graph import generate_knowledge_graph

if __name__ == "__main__":
    generate_knowledge_graph()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\update_kg.py</data>
</node>
<node id="watch_codebase">
  <data key="d0">module</data>
  <data key="d1">"""
Start the codebase watcher for real-time health and dependency analysis
"""

import os
import sys
import logging
from pathlib import Path

# Add tools directory to Python path
tools_dir = Path(__file__).parent
if str(tools_dir) not in sys.path:
    sys.path.append(str(tools_dir))

from kg.file_watcher import CodeWatcher

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    # Get the project root directory (where this script is located)
    root_dir = Path(__file__).parent.parent
    context_dir = root_dir / "semantic_context"
    
    # Create context directory if it doesn't exist
    context_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"Starting watcher for {root_dir}")
    logger.info(f"Context directory: {context_dir}")
    
    # Start the watcher
    watcher = CodeWatcher(str(root_dir), str(context_dir))
    try:
        watcher.start()
        logger.info("Watcher started successfully")
    except Exception as e:
        logger.error(f"Failed to start watcher: {e}")
        raise

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\watch_codebase.py</data>
</node>
<node id="file_watcher">
  <data key="d0">module</data>
  <data key="d1">"""
Real-time file watcher for code health and dependency analysis
"""

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time
from pathlib import Path
import logging
from typing import Set, Optional, Dict, Any
import threading
import queue
import os

from .semantic_context_manager import SemanticContextManager
from .cleanup.code_health_manager import CodeHealthManager

logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CodeChangeHandler(FileSystemEventHandler):
    def __init__(self, root_dir: str, context_dir: str):
        super().__init__()
        self.root_dir = Path(root_dir)
        self.context_dir = Path(context_dir)
        self.semantic_manager = SemanticContextManager(self.context_dir)
        self.health_manager = CodeHealthManager(self.semantic_manager)
        self.change_queue = queue.Queue()
        self.changed_files: Set[Path] = set()
        self.last_analysis_time = 0
        self.analysis_cooldown = 5  # seconds
        self.running = True
        self.analysis_thread = threading.Thread(target=self._process_changes, daemon=True)
        self.analysis_thread.start()
        
    def on_modified(self, event):
        if event.is_directory:
            return
            
        file_path = Path(event.src_path)
        if not self._should_process_file(file_path):
            return
            
        logger.debug(f"File modified: {file_path}")
        self.change_queue.put(("modified", file_path))
        
    def on_created(self, event):
        if event.is_directory:
            return
            
        file_path = Path(event.src_path)
        if not self._should_process_file(file_path):
            return
            
        logger.debug(f"File created: {file_path}")
        self.change_queue.put(("created", file_path))
        
    def on_deleted(self, event):
        if event.is_directory:
            return
            
        file_path = Path(event.src_path)
        if not self._should_process_file(file_path):
            return
            
        logger.debug(f"File deleted: {file_path}")
        self.change_queue.put(("deleted", file_path))
        
    def _should_process_file(self, file_path: Path) -&gt; bool:
        """Check if the file should be processed based on extension and location"""
        try:
            if not os.path.exists(file_path) or os.path.isdir(file_path):
                return False
                
            # Convert to relative path for easier checking
            try:
                rel_path = file_path.relative_to(self.root_dir)
            except ValueError:
                # File is outside of root directory
                return False
                
            # Ignore certain directories
            ignore_dirs = {
                '.git', '__pycache__', 'node_modules', 'venv', '.venv',
                'dist', 'build', 'coverage', '.pytest_cache', '.mypy_cache',
                '.idea', '.vs', '.vscode', 'env', 'migrations'
            }
            
            # Check if file is in ignored directory
            parts = rel_path.parts
            if any(part.startswith('.') for part in parts):  # Ignore all hidden directories
                return False
            if any(ignore_dir in parts for ignore_dir in ignore_dirs):
                return False
                
            # Ignore certain file types
            ignore_extensions = {
                '.pyc', '.pyo', '.pyd', '.git', '.swp', '.swo',
                '.coverage', '.DS_Store', '.log', '.tmp', '.temp',
                '.cache', '.bak', '.pkl', '.ipynb', '.lock', '.map'
            }
            
            if file_path.suffix.lower() in ignore_extensions:
                return False
                
            # Only process certain file types (source code files)
            allowed_extensions = {
                '.py', '.js', '.ts', '.tsx', '.jsx', '.css', '.scss',
                '.html', '.htm', '.xml', '.json', '.yaml', '.yml',
                '.md', '.rst', '.txt', '.sql'
            }
            
            return file_path.suffix.lower() in allowed_extensions
        except Exception as e:
            logger.error(f"Error in _should_process_file: {e}")
            return False
        
    def _process_changes(self):
        """Process file changes in a separate thread"""
        while self.running:
            try:
                # Wait for changes to accumulate
                time.sleep(self.analysis_cooldown)
                
                # Process all pending changes
                changed = False
                while not self.change_queue.empty():
                    try:
                        change_type, file_path = self.change_queue.get_nowait()
                        self.changed_files.add(file_path)
                        changed = True
                    except queue.Empty:
                        break
                    
                if changed:
                    current_time = time.time()
                    if current_time - self.last_analysis_time &gt; self.analysis_cooldown:
                        self._analyze_changes()
                        self.changed_files.clear()
                        self.last_analysis_time = current_time
                    
            except Exception as e:
                logger.error(f"Error processing changes: {e}")
                
    def _analyze_changes(self):
        """Analyze the accumulated changes"""
        try:
            file_count = len(self.changed_files)
            logger.info(f"Analyzing {file_count} changed files...")
            
            # Update semantic context
            self.semantic_manager.analyze_codebase_health(str(self.root_dir))
            
            # Run health analysis
            health_report = self.health_manager.analyze_health()
            
            # Log results
            score = health_report.get('health_score', 0)
            issues = health_report.get('issues', [])
            
            logger.info(f"Health Score: {score:.2f}%")
            
            if issues:
                logger.warning("Potential issues detected:")
                for issue in issues:
                    logger.warning(f"- {issue['description']}")
                    if 'affected_files' in issue:
                        for file in issue['affected_files'][:5]:  # Limit to first 5 files
                            logger.warning(f"  * {file}")
                        if len(issue['affected_files']) &gt; 5:
                            logger.warning(f"  * and {len(issue['affected_files']) - 5} more files")
                            
            if score &lt; 90:
                logger.warning("Health score is below 90%. Consider addressing the issues above.")
                
        except UnicodeDecodeError as e:
            logger.debug(f"Encoding error while processing file: {e}")
        except Exception as e:
            logger.error(f"Error during analysis: {e}", exc_info=True)
            
    def stop(self):
        """Stop the analysis thread"""
        self.running = False
        if self.analysis_thread.is_alive():
            self.analysis_thread.join(timeout=1.0)


class CodeWatcher:
    def __init__(self, root_dir: str, context_dir: Optional[str] = None):
        self.root_dir = Path(root_dir).resolve()  # Use absolute path
        self.context_dir = Path(context_dir).resolve() if context_dir else self.root_dir / "semantic_context"
        self.context_dir.mkdir(parents=True, exist_ok=True)  # Ensure context directory exists
        
        logger.info(f"Initializing CodeWatcher for {self.root_dir}")
        logger.info(f"Using context directory: {self.context_dir}")
        
        self.event_handler = CodeChangeHandler(str(self.root_dir), str(self.context_dir))
        self.observer = Observer()
        
    def start(self):
        """Start watching for file changes"""
        try:
            self.observer.schedule(self.event_handler, str(self.root_dir), recursive=True)
            self.observer.start()
            logger.info(f"Started watching {self.root_dir}")
            
            # Run until interrupted
            while True:
                time.sleep(1)
                
        except KeyboardInterrupt:
            logger.info("Keyboard interrupt received")
            self.stop()
        except Exception as e:
            logger.error(f"Error in watcher: {e}", exc_info=True)
            self.stop()
            raise
            
    def stop(self):
        """Stop watching for file changes"""
        logger.info("Stopping watcher...")
        self.event_handler.stop()
        self.observer.stop()
        self.observer.join()
        logger.info("Watcher stopped")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) &lt; 2:
        print("Usage: python file_watcher.py &lt;root_dir&gt; [context_dir]")
        sys.exit(1)
        
    root_dir = sys.argv[1]
    context_dir = sys.argv[2] if len(sys.argv) &gt; 2 else None
    
    watcher = CodeWatcher(root_dir, context_dir)
    watcher.start()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\file_watcher.py</data>
</node>
<node id="generate_graph">
  <data key="d0">module</data>
  <data key="d1">"""
Knowledge Graph Generation Script
Generates an optimized knowledge graph for the AUL Quote App
"""

import logging
from pathlib import Path
import networkx as nx
from datetime import datetime
import json

from .kg_generator import build_knowledge_graph
from .config.kg_config import PROJECT_ROOT

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def prepare_graph_for_export(graph: nx.DiGraph) -&gt; nx.DiGraph:
    """
    Prepare graph for GraphML export by converting non-compatible data types
    """
    def serialize_value(value):
        if isinstance(value, (str, int, float, bool)):
            return value
        elif value is None:
            return ""
        else:
            try:
                return json.dumps(value)
            except:
                return str(value)

    # Create a new graph for the serialized data
    new_graph = nx.DiGraph()
    
    # Process nodes
    for node, data in graph.nodes(data=True):
        serialized_data = {k: serialize_value(v) for k, v in data.items()}
        new_graph.add_node(node, **serialized_data)
    
    # Process edges
    for u, v, data in graph.edges(data=True):
        serialized_data = {k: serialize_value(v) for k, v in data.items()}
        new_graph.add_edge(u, v, **serialized_data)
    
    return new_graph

def generate_knowledge_graph() -&gt; None:
    """Generate and save the knowledge graph"""
    start_time = datetime.now()
    logger.info("Starting knowledge graph generation...")
    
    try:
        # Build the graph
        graph = build_knowledge_graph(PROJECT_ROOT)
        
        # Prepare graph for export
        export_graph = prepare_graph_for_export(graph)
        
        # Generate timestamp for versioning
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save both GraphML and JSON versions
        graphml_path = PROJECT_ROOT / f"code_knowledge_graph_{timestamp}.graphml"
        json_path = PROJECT_ROOT / f"code_knowledge_graph_{timestamp}.json"
        
        # Save as GraphML
        nx.write_graphml(export_graph, graphml_path)
        
        # Save as JSON for backup and easier parsing
        json_data = {
            'nodes': [{
                'id': node,
                **data
            } for node, data in graph.nodes(data=True)],
            'edges': [{
                'source': u,
                'target': v,
                **data
            } for u, v, data in graph.edges(data=True)]
        }
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # Log statistics
        num_nodes = graph.number_of_nodes()
        num_edges = graph.number_of_edges()
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"Knowledge graph generated successfully:")
        logger.info(f"- Nodes: {num_nodes}")
        logger.info(f"- Edges: {num_edges}")
        logger.info(f"- Duration: {duration:.2f} seconds")
        logger.info(f"- GraphML Output: {graphml_path}")
        logger.info(f"- JSON Output: {json_path}")
        
    except Exception as e:
        logger.error(f"Error generating knowledge graph: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    generate_knowledge_graph()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\generate_graph.py</data>
</node>
<node id="graph_updater">
  <data key="d0">module</data>
  <data key="d1">"""
Automated Knowledge Graph Updater
Watches for file changes and updates the knowledge graph accordingly
Enhanced with semantic context tracking
"""

import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import networkx as nx
from pathlib import Path
import logging
from typing import Set, Dict, Any, Optional, List, Tuple
import ast
from concurrent.futures import ThreadPoolExecutor
import threading
import hashlib
import json
from dataclasses import dataclass
from enum import Enum
import re
from datetime import datetime, timedelta
import tempfile
from copy import deepcopy

from .ts_analyzer import TypeScriptAnalyzer
from .semantic_context_manager import SemanticContextManager
from .semantic_context_checker import SemanticContextChecker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UpdateType(Enum):
    """Types of updates that can occur"""
    SYNTAX_ONLY = "syntax_only"  # Only whitespace or comments changed
    MINOR = "minor"  # Small changes that don't affect structure
    STRUCTURAL = "structural"  # Changes that affect code structure
    BREAKING = "breaking"  # Changes that could break dependencies
    CIRCULAR = "circular"  # Introduces circular dependencies
    SECURITY = "security"  # Potential security issues
    PERFORMANCE = "performance"  # Performance impact changes
    SEMANTIC = "semantic"  # Changes affecting semantic relationships
    HEALTH = "health"  # Changes affecting codebase health metrics

@dataclass
class UpdateValidation:
    """Validation result for a file update"""
    is_valid: bool
    update_type: UpdateType
    changes: Dict[str, Any]
    error: Optional[str] = None
    warnings: List[str] = None
    impact_score: float = 0.0  # 0.0 to 1.0, higher means more impact
    affected_files: List[str] = None
    semantic_changes: Dict[str, Any] = None
    health_impact: Optional[Dict[str, float]] = None  # Impact on health metrics

class UpdateThrottler:
    """Prevents too frequent updates to the same file"""
    def __init__(self, min_interval: int = 5):
        self.last_updates = {}
        self.min_interval = min_interval  # minimum seconds between updates
        
    def can_update(self, file_path: str) -&gt; bool:
        now = datetime.now()
        if file_path in self.last_updates:
            if now - self.last_updates[file_path] &lt; timedelta(seconds=self.min_interval):
                return False
        self.last_updates[file_path] = now
        return True

class SecurityChecker:
    """Checks for potential security issues in code changes"""
    SECURITY_PATTERNS = {
        'hardcoded_secrets': r'(?i)(password|secret|key|token|api_key)\s*=\s*[\'"][^\'"]+[\'"]',
        'sql_injection': r'(?i)execute\s*\(\s*.*\+.*\)',
        'command_injection': r'(?i)(os\.system|subprocess\.run|eval|exec)\s*\(',
        'unsafe_deserialization': r'(?i)(pickle\.loads|yaml\.load\s*\([^)])',
        'file_access': r'(?i)(open|file)\s*\([\'"][^\'"]+[\'"]'
    }
    
    @classmethod
    def check_security(cls, content: str) -&gt; List[str]:
        issues = []
        for issue_type, pattern in cls.SECURITY_PATTERNS.items():
            if re.search(pattern, content):
                issues.append(f"Potential {issue_type} detected")
        return issues

class CircularDependencyChecker:
    """Checks for circular dependencies in the code"""
    def __init__(self, graph_manager):
        self.graph_manager = graph_manager
        
    def check_circular(self, new_graph: nx.DiGraph) -&gt; List[List[str]]:
        """Returns list of circular dependency chains"""
        # Combine existing graph with new changes
        full_graph = self.graph_manager.export_graph()
        test_graph = nx.compose(full_graph, new_graph)
        
        try:
            cycles = list(nx.simple_cycles(test_graph))
            return cycles
        except Exception as e:
            logger.error(f"Error checking circular dependencies: {e}")
            return []

class PerformanceAnalyzer:
    """Analyzes potential performance impacts of changes"""
    def __init__(self):
        self.complexity_threshold = 10  # McCabe complexity threshold
        
    def analyze_performance(self, tree: ast.AST) -&gt; List[str]:
        issues = []
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.For, ast.While)):
                # Check for nested loops
                if any(isinstance(child, (ast.For, ast.While)) for child in ast.walk(node)):
                    issues.append("Nested loop detected - potential O(n²) complexity")
                    
            elif isinstance(node, ast.ListComp):
                # Check for nested list comprehensions
                if any(isinstance(child, ast.ListComp) for child in ast.walk(node)):
                    issues.append("Nested list comprehension detected")
                    
            elif isinstance(node, ast.Call):
                # Check for potential memory issues
                if isinstance(node.func, ast.Name):
                    if node.func.id in ['range', 'list', 'dict', 'set']:
                        # Check if large collection is being created
                        if node.args and isinstance(node.args[0], ast.Num):
                            if node.args[0].n &gt; 10000:
                                issues.append(f"Large collection creation: {node.func.id}({node.args[0].n})")
                                
        return issues

class CodeChangeHandler(FileSystemEventHandler):
    def __init__(self, graph_manager, ts_analyzer):
        """Initialize the change handler"""
        self.graph_manager = graph_manager
        self.ts_analyzer = ts_analyzer
        self.update_queue = set()
        self.queue_lock = threading.Lock()
        self.file_hashes = {}  # Store file hashes to detect real changes
        self.backup_folder = Path("./backups")
        self.backup_folder.mkdir(exist_ok=True)
        self.throttler = UpdateThrottler()
        self.circular_checker = CircularDependencyChecker(graph_manager)
        self.performance_analyzer = PerformanceAnalyzer()
        
        # Initialize semantic managers
        self.semantic_manager = SemanticContextManager(Path("./semantic_context"))
        self.context_checker = SemanticContextChecker(self.semantic_manager)
        
        # Track health metrics
        self.last_health_score = None
        self.health_threshold = 0.7  # Minimum acceptable health score
        
        self.update_thread = threading.Thread(target=self._process_updates, daemon=True)
        self.update_thread.start()

    def _backup_file(self, file_path: str):
        """Create a backup of the file before updating"""
        try:
            backup_path = self.backup_folder / f"{Path(file_path).name}.{int(time.time())}.bak"
            with open(file_path, 'rb') as src, open(backup_path, 'wb') as dst:
                dst.write(src.read())
            return backup_path
        except Exception as e:
            logger.error(f"Error creating backup for {file_path}: {e}")
            return None
            
    def _restore_backup(self, backup_path: Path):
        """Restore file from backup"""
        try:
            original_name = backup_path.stem.rsplit('.', 1)[0]
            original_path = Path(original_name)
            with open(backup_path, 'rb') as src, open(original_path, 'wb') as dst:
                dst.write(src.read())
            logger.info(f"Restored backup for {original_path}")
        except Exception as e:
            logger.error(f"Error restoring backup {backup_path}: {e}")
            
    def _get_file_hash(self, file_path: str) -&gt; str:
        """Get hash of file contents"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"Error hashing file {file_path}: {e}")
            return None
            
    def _validate_update(self, file_path: str) -&gt; UpdateValidation:
        """Validate if a file update should be processed"""
        try:
            # First, check semantic context
            with open(file_path, 'r', encoding='utf-8') as f:
                new_content = f.read()
                
            context_validation = self.context_checker.validate_change(file_path, new_content)
            if not context_validation.is_valid:
                return UpdateValidation(
                    False,
                    UpdateType.BREAKING,
                    {},
                    context_validation.error,
                    [],
                    1.0,
                    []
                )
                
            # Add semantic validation warnings
            warnings = list(context_validation.warnings)
            
            # Check if file still exists
            if not Path(file_path).exists():
                return UpdateValidation(
                    False,
                    UpdateType.BREAKING,
                    {},
                    "File no longer exists",
                    warnings,
                    1.0,
                    []
                )
                
            # Check update frequency
            if not self.throttler.can_update(file_path):
                return UpdateValidation(
                    False,
                    UpdateType.MINOR,
                    {},
                    "Update throttled - too frequent changes",
                    warnings,
                    0.0,
                    []
                )
                
            # Get old content from backup if available
            old_content = ""
            backup_files = list(self.backup_folder.glob(f"{Path(file_path).name}.*"))
            if backup_files:
                latest_backup = max(backup_files, key=lambda x: int(x.suffix[1:]))
                with open(latest_backup, 'r', encoding='utf-8') as f:
                    old_content = f.read()
                    
            # Use semantic context for update type and impact
            update_type = UpdateType.SEMANTIC if context_validation.impact_assessment["impact_score"] &gt; 0.3 else UpdateType.MINOR
            
            return UpdateValidation(
                True,
                update_type,
                {"content_changed": bool(old_content != new_content)},
                None,
                warnings,
                context_validation.impact_assessment["impact_score"],
                list(context_validation.related_files),
                context_validation.context_data
            )
            
        except Exception as e:
            logger.error(f"Error validating update for {file_path}: {e}")
            return UpdateValidation(False, UpdateType.BREAKING, {}, str(e), [], 1.0, [])
            
    def _determine_update_type(self, old_content: str, new_content: str, semantic_changes: Dict) -&gt; Tuple[UpdateType, Dict]:
        """Determine the type of update and its health impact"""
        # Get current health metrics
        health_before = self.semantic_manager.analyze_codebase_health()
        
        # Analyze changes
        security_issues = SecurityChecker.check_security(new_content)
        performance_impact = self.performance_analyzer.analyze_performance(ast.parse(new_content))
        circular_deps = self.circular_checker.check_circular(self.graph_manager.graph)
        
        # Get updated health metrics
        health_after = self.semantic_manager.analyze_codebase_health()
        
        # Calculate health impact
        health_impact = {
            'score_change': health_after['health_score'] - health_before['health_score'],
            'duplication_change': health_after['metrics']['duplication_rate'] - health_before['metrics']['duplication_rate'],
            'orphan_change': health_after['metrics']['orphan_rate'] - health_before['metrics']['orphan_rate'],
            'divergence_change': health_after['metrics']['divergence_rate'] - health_before['metrics']['divergence_rate']
        }
        
        # Determine update type based on all factors
        if security_issues:
            return UpdateType.SECURITY, health_impact
        elif circular_deps:
            return UpdateType.CIRCULAR, health_impact
        elif performance_impact &gt; 0.5:  # High performance impact
            return UpdateType.PERFORMANCE, health_impact
        elif abs(health_impact['score_change']) &gt; 0.1:  # Significant health change
            return UpdateType.HEALTH, health_impact
        elif semantic_changes:
            return UpdateType.SEMANTIC, health_impact
        else:
            return UpdateType.MINOR, health_impact
        
    def _calculate_impact_score(self, update_type: UpdateType, semantic_changes: Dict) -&gt; float:
        """Calculate impact score based on update type and semantic changes"""
        base_scores = {
            UpdateType.SYNTAX_ONLY: 0.1,
            UpdateType.MINOR: 0.3,
            UpdateType.STRUCTURAL: 0.6,
            UpdateType.SEMANTIC: 0.7,
            UpdateType.BREAKING: 1.0,
            UpdateType.CIRCULAR: 0.9,
            UpdateType.SECURITY: 0.8,
            UpdateType.PERFORMANCE: 0.5,
            UpdateType.HEALTH: 0.8
        }
        
        base_score = base_scores.get(update_type, 0.5)
        
        # Adjust score based on semantic changes
        if semantic_changes:
            removed = len(semantic_changes.get("removed_relationships", []))
            new = len(semantic_changes.get("new_relationships", []))
            maintained = len(semantic_changes.get("maintained_relationships", []))
            
            if removed &gt; 0:
                base_score += 0.2  # Breaking existing relationships is high impact
            if new &gt; maintained:
                base_score += 0.1  # More new than maintained relationships
                
        return min(base_score, 1.0)
        
    def _get_affected_files(self, file_path: str, semantic_changes: Dict) -&gt; List[str]:
        """Get list of files affected by the changes"""
        affected = set()
        
        # Add direct dependencies from graph
        for edge in self.graph_manager.graph.edges(file_path):
            affected.add(edge[1])
            
        # Add semantically related files
        if semantic_changes:
            affected.update(semantic_changes.get("removed_relationships", []))
            affected.update(semantic_changes.get("new_relationships", []))
            affected.update(semantic_changes.get("maintained_relationships", []))
            
        return list(affected)
        
    def on_modified(self, event):
        """Handle file modification events"""
        if event.is_directory:
            return
            
        file_path = event.src_path
        if not any(file_path.endswith(ext) for ext in ['.py', '.ts', '.tsx', '.js', '.jsx']):
            return
            
        with self.queue_lock:
            self.update_queue.add(file_path)
            
    def _process_file_update(self, file_path: str):
        """Process a single file update"""
        try:
            # Validate update
            validation = self._validate_update(file_path)
            if not validation.is_valid:
                logger.warning(f"Update validation failed for {file_path}: {validation.error}")
                return
                
            # Create backup
            backup_path = self._backup_file(file_path)
            
            # Update graph
            self.graph_manager.update_file(file_path)
            
            # Update semantic context
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            self.semantic_manager.update_context(
                file_path,
                content,
                {"last_modified": datetime.now().isoformat()}
            )
            
            # Log update details
            logger.info(f"Processed update for {file_path}")
            logger.info(f"Update type: {validation.update_type}")
            logger.info(f"Impact score: {validation.impact_score}")
            if validation.semantic_changes:
                logger.info("Semantic changes detected:")
                logger.info(f"- New relationships: {len(validation.semantic_changes['new_relationships'])}")
                logger.info(f"- Removed relationships: {len(validation.semantic_changes['removed_relationships'])}")
                
        except Exception as e:
            logger.error(f"Error processing update for {file_path}: {e}")
            if backup_path:
                self._restore_backup(backup_path)

    def _process_updates(self):
        """Process queued file updates"""
        while True:
            time.sleep(1)  # Wait for more changes to accumulate
            
            with self.queue_lock:
                current_updates = self.update_queue.copy()
                self.update_queue.clear()
                
            if current_updates:
                for file_path in current_updates:
                    self._process_file_update(file_path)
                    
class GraphUpdater:
    def __init__(self, graph_manager, watch_paths: list):
        """Initialize the graph updater"""
        self.graph_manager = graph_manager
        self.watch_paths = watch_paths
        self.ts_analyzer = TypeScriptAnalyzer()
        self.observer = Observer()
        self.handler = CodeChangeHandler(graph_manager, self.ts_analyzer)
        
    def start(self):
        """Start watching for file changes"""
        for path in self.watch_paths:
            self.observer.schedule(self.handler, path, recursive=True)
        self.observer.start()
        logger.info(f"Started watching {len(self.watch_paths)} paths for changes")
        
    def stop(self):
        """Stop watching for file changes"""
        self.observer.stop()
        self.observer.join()
        logger.info("Stopped watching for changes")
        
def main():
    """Test the graph updater"""
    from .graph_db import GraphDatabaseManager
    
    # Initialize components
    db = GraphDatabaseManager()
    watch_paths = ["."]  # Watch current directory
    updater = GraphUpdater(db, watch_paths)
    
    try:
        # Start watching for changes
        updater.start()
        
        # Keep running until interrupted
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        logger.info("Stopping graph updater...")
        updater.stop()
        
    finally:
        db.close()

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\graph_updater.py</data>
</node>
<node id="kg_generator">
  <data key="d0">module</data>
  <data key="d1">"""
Knowledge Graph Generator
Builds a knowledge graph focusing on interfaces and semantic relationships
"""

import ast
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Set, Optional
import networkx as nx
from sentence_transformers import SentenceTransformer
import json
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import logging
import os
import matplotlib.pyplot as plt

from .config.kg_config import (
    get_high_priority_paths,
    should_exclude_path,
    get_relationship_config,
    get_code_health_rules,
    get_domain_group,
    get_interface_type,
    PROJECT_ROOT,
    DomainGroup,
    InterfaceType,
    INTERFACE_DEFINITIONS,
    SEMANTIC_GROUPS,
    INCLUDED_DIRS,
    INCLUDED_PATTERNS
)

logger = logging.getLogger(__name__)

@dataclass
class CodeNode:
    """Represents a node in the code knowledge graph"""
    name: str
    type: str
    file: str
    line_number: int
    docstring: Optional[str] = None
    interface_type: Optional[InterfaceType] = None
    domain_group: Optional[DomainGroup] = None
    semantic_group: Optional[str] = None
    content: Optional[str] = None

@dataclass
class CodeEdge:
    """Represents an edge in the code knowledge graph"""
    source: str
    target: str
    type: str
    weight: float = 1.0
    properties: Optional[Dict] = None

class InterfaceAnalyzer(ast.NodeVisitor):
    """Analyzes interface definitions and implementations"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.interfaces: Dict[str, CodeNode] = {}
        self.implementations: Dict[str, CodeNode] = {}
        self.edges: List[CodeEdge] = []
        
    def visit_ClassDef(self, node: ast.ClassDef) -&gt; None:
        """Process class definitions to identify interfaces and implementations"""
        # Check if this is an interface
        is_interface = any(
            base.id.endswith('Interface')
            for base in node.bases
            if isinstance(base, ast.Name)
        )
        
        if is_interface:
            self.interfaces[node.name] = CodeNode(
                name=node.name,
                type='interface',
                file=self.file_path,
                line_number=node.lineno,
                docstring=ast.get_docstring(node),
                interface_type=get_interface_type(self.file_path),
                domain_group=get_domain_group(self.file_path),
                content=self._get_node_content(node)
            )
        else:
            # Check if this class implements any interfaces
            for base in node.bases:
                if isinstance(base, ast.Name) and base.id in self.interfaces:
                    self.implementations[node.name] = CodeNode(
                        name=node.name,
                        type='class',
                        file=self.file_path,
                        line_number=node.lineno,
                        docstring=ast.get_docstring(node),
                        interface_type=get_interface_type(self.file_path),
                        domain_group=get_domain_group(self.file_path),
                        content=self._get_node_content(node)
                    )
                    
                    self.edges.append(CodeEdge(
                        source=node.name,
                        target=base.id,
                        type='implements_interface',
                        weight=2.0
                    ))
        
        self.generic_visit(node)
        
    def _get_node_content(self, node: ast.AST) -&gt; str:
        """Extract the content of a node"""
        return ast.unparse(node)

class SemanticAnalyzer:
    """Analyzes semantic relationships between code elements"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        self.text_splitter = None
        
    def analyze_semantic_relationships(
        self,
        graph: nx.DiGraph,
        min_similarity: float = 0.7
    ) -&gt; List[CodeEdge]:
        """Analyze semantic relationships between nodes
        
        Args:
            graph: Knowledge graph
            min_similarity: Minimum cosine similarity threshold
            
        Returns:
            List of semantic relationship edges
        """
        edges = []
        nodes_data = []
        node_texts = []
        
        # Collect node data and texts
        for node_name in graph.nodes():
            node_data = graph.nodes[node_name]
            if not isinstance(node_data, dict):
                continue
                
            # Get text content for semantic analysis
            text_content = []
            if 'docstring' in node_data and node_data['docstring']:
                text_content.append(node_data['docstring'])
            if 'content' in node_data and node_data['content']:
                text_content.append(node_data['content'])
                
            if text_content:
                nodes_data.append({
                    'name': node_name,
                    'type': node_data.get('node_type', 'unknown'),
                    'file': node_data.get('file', ''),
                    'line_number': node_data.get('line_number', 0),
                    'docstring': node_data.get('docstring', ''),
                    'interface_type': node_data.get('interface_type'),
                    'domain_group': node_data.get('domain_group'),
                    'content': node_data.get('content', '')
                })
                node_texts.append(' '.join(text_content))
        
        if not node_texts:
            return edges
            
        # Calculate embeddings
        embeddings = self.model.encode(node_texts)
        
        # Calculate pairwise similarities
        similarities = cosine_similarity(embeddings)
        
        # Create edges for similar nodes
        for i in range(len(nodes_data)):
            for j in range(i + 1, len(nodes_data)):
                similarity = similarities[i][j]
                if similarity &gt;= min_similarity:
                    source_node = nodes_data[i]
                    target_node = nodes_data[j]
                    
                    edges.append(CodeEdge(
                        source=source_node['name'],
                        target=target_node['name'],
                        type='semantic_relation',
                        weight=float(similarity),
                        properties={'similarity': float(similarity)}
                    ))
                    
        return edges

class CodeHealthAnalyzer:
    """Analyzes code health issues"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.rules = get_code_health_rules()
        self.model = SentenceTransformer(model_name)
        
    def analyze_health(self, graph: nx.DiGraph) -&gt; Dict:
        """Analyze code health issues in the graph"""
        return {
            'duplication': self.find_code_duplicates(graph.nodes(data=True)),
            'divergence': self.find_interface_divergence(graph),
            'orphans': self._find_orphans(graph)
        }
    
    def find_code_duplicates(
        self,
        nodes: Dict[str, Dict]
    ) -&gt; List[Dict]:
        """Find potential code duplicates
        
        Args:
            nodes: Dictionary of node data
            
        Returns:
            List of duplicate code issues
        """
        duplicates = []
        rules = self.rules['duplication']
        
        # Get implementation nodes
        impl_nodes = [
            (name, data) for name, data in nodes.items()
            if data.get('interface_type') != InterfaceType.DATA_MODEL.value
        ]
        
        # Create embeddings for content comparison
        contents = []
        node_pairs = []
        
        # Collect content pairs to compare
        for i, (name1, data1) in enumerate(impl_nodes):
            if not data1.get('content'):
                continue
                
            for name2, data2 in impl_nodes[i+1:]:
                if not data2.get('content'):
                    continue
                    
                if self._should_check_duplication(name1, name2):
                    contents.extend([data1['content'], data2['content']])
                    node_pairs.append((name1, name2))
                    
        if not contents:
            return duplicates
            
        # Calculate embeddings and similarities in batch
        embeddings = self.model.encode(contents)
        
        # Process similarities
        for i in range(0, len(embeddings), 2):
            if i + 1 &gt;= len(embeddings):
                break
                
            vec1 = embeddings[i].reshape(1, -1)
            vec2 = embeddings[i + 1].reshape(1, -1)
            similarity = cosine_similarity(vec1, vec2)[0][0]
            
            if similarity &gt; rules['max_similarity']:
                name1, name2 = node_pairs[i // 2]
                duplicates.append({
                    'type': 'code_duplication',
                    'severity': 'warning',
                    'nodes': [name1, name2],
                    'similarity': float(similarity)
                })
                
        return duplicates
        
    def _should_check_duplication(self, name1: str, name2: str) -&gt; bool:
        """Check if two nodes should be compared for duplication"""
        # Skip test files comparing with each other
        if 'test' in name1.lower() and 'test' in name2.lower():
            return False
            
        # Skip obvious utility/helper functions
        skip_patterns = ['util', 'helper', 'common']
        if any(pattern in name1.lower() for pattern in skip_patterns) and \
           any(pattern in name2.lower() for pattern in skip_patterns):
            return False
            
        return True
        
    def find_interface_divergence(self, graph: nx.DiGraph) -&gt; List[Dict]:
        """Find implementations that have diverged from their interfaces"""
        divergences = []
        rules = self.rules['divergence']
        
        for node, data in graph.nodes(data=True):
            if data.get('interface_type') != InterfaceType.DATA_MODEL.value:
                # Find implemented interfaces
                interfaces = [
                    edge[1]
                    for edge in graph.out_edges(node)
                    if graph.edges[edge]['edge_type'] == 'implements_interface'
                ]
                
                for interface in interfaces:
                    drift = self.calculate_interface_drift(
                        graph.nodes[interface],
                        data
                    )
                    
                    if drift &gt; rules['interface_drift_threshold']:
                        divergences.append({
                            'type': 'interface_divergence',
                            'severity': 'warning',
                            'interface': interface,
                            'implementation': node,
                            'drift': drift
                        })
        
        return divergences
    
    def calculate_interface_drift(
        self,
        interface_node: Dict,
        implementation_node: Dict
    ) -&gt; float:
        """Calculate interface drift between interface and implementation
        
        Args:
            interface_node: Interface node data
            implementation_node: Implementation node data
            
        Returns:
            Drift score between 0 and 1
        """
        impl_data = implementation_node
        interface_data = interface_node
        
        if not impl_data.get('content') or not interface_data.get('content'):
            return 0.0
            
        # Calculate embeddings for both contents
        contents = [impl_data['content'], interface_data['content']]
        embeddings = self.model.encode(contents)
        
        # Calculate cosine similarity and convert to drift
        impl_vec = embeddings[0].reshape(1, -1)
        interface_vec = embeddings[1].reshape(1, -1)
        similarity = cosine_similarity(impl_vec, interface_vec)[0][0]
        drift = 1.0 - similarity
        
        return float(drift)
    
    def _find_orphans(self, graph: nx.DiGraph) -&gt; List[Dict]:
        """Find orphaned components"""
        orphans = []
        rules = self.rules['orphan']
        
        for node, data in graph.nodes(data=True):
            if not self._should_check_orphan(node):
                continue
                
            references = len(list(graph.in_edges(node)))
            if references &lt; rules['min_references']:
                orphans.append({
                    'type': 'orphan',
                    'severity': 'warning',
                    'node': node,
                    'references': references
                })
        
        return orphans
    
    def _should_check_orphan(self, name: str) -&gt; bool:
        """Check if a component should be checked for orphan status"""
        rules = self.rules['orphan']
        for pattern in rules['exclude_patterns']:
            if re.match(pattern, name):
                return False
        return True

class TypeScriptAnalyzer:
    """Analyzes TypeScript/TSX files"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.nodes: Dict[str, CodeNode] = {}
        self.edges: List[CodeEdge] = []
        
    def analyze(self, content: str) -&gt; None:
        """Analyze TypeScript/TSX content"""
        # Extract imports
        imports = self._extract_imports(content)
        
        # Extract exports (interfaces, types, classes, etc.)
        exports = self._extract_exports(content)
        
        # Extract JSDoc comments
        docs = self._extract_docs(content)
        
        # Create nodes for each export
        for export_name, export_type in exports.items():
            node = CodeNode(
                name=export_name,
                type=export_type,
                file=self.file_path,
                line_number=1,  # Actual line numbers would require more parsing
                docstring=docs.get(export_name, ""),
                interface_type=self._get_interface_type(export_type),
                domain_group=get_domain_group(self.file_path)
            )
            self.nodes[export_name] = node
            
        # Create edges for imports
        for imp in imports:
            edge = CodeEdge(
                source=Path(self.file_path).stem,
                target=imp,
                type="imports"
            )
            self.edges.append(edge)
            
    def _extract_imports(self, content: str) -&gt; List[str]:
        """Extract import statements"""
        imports = []
        import_pattern = r'import\s+(?:{[^}]*}|\*\s+as\s+\w+|\w+)\s+from\s+[\'"]([^\'"]+)[\'"]'
        
        for match in re.finditer(import_pattern, content):
            imports.append(match.group(1))
            
        return imports
        
    def _extract_exports(self, content: str) -&gt; Dict[str, str]:
        """Extract exported declarations"""
        exports = {}
        patterns = [
            (r'export\s+interface\s+(\w+)', 'interface'),
            (r'export\s+type\s+(\w+)', 'type'),
            (r'export\s+class\s+(\w+)', 'class'),
            (r'export\s+const\s+(\w+)', 'const'),
            (r'export\s+function\s+(\w+)', 'function'),
            (r'export\s+enum\s+(\w+)', 'enum')
        ]
        
        for pattern, type_ in patterns:
            for match in re.finditer(pattern, content):
                exports[match.group(1)] = type_
                
        return exports
        
    def _extract_docs(self, content: str) -&gt; Dict[str, str]:
        """Extract JSDoc comments"""
        docs = {}
        doc_pattern = r'/\*\*[\s\S]*?\*/\s*export\s+(?:interface|type|class|const|function|enum)\s+(\w+)'
        
        for match in re.finditer(doc_pattern, content):
            doc = match.group(0)
            name = match.group(1)
            # Clean up doc comment
            doc = re.sub(r'/\*\*|\*/|^\s*\*', '', doc, flags=re.MULTILINE)
            docs[name] = doc.strip()
            
        return docs
        
    def _get_interface_type(self, export_type: str) -&gt; Optional[InterfaceType]:
        """Map export type to interface type"""
        type_map = {
            'interface': InterfaceType.DATA_MODEL,
            'type': InterfaceType.DATA_MODEL,
            'class': InterfaceType.SERVICE_INTERFACE,
            'function': InterfaceType.API_CONTRACT
        }
        return type_map.get(export_type)

def build_knowledge_graph(root_dir: Path = PROJECT_ROOT) -&gt; nx.DiGraph:
    """Build knowledge graph from codebase
    
    Args:
        root_dir: Root directory to start graph generation from
        
    Returns:
        nx.DiGraph: Generated knowledge graph
    """
    graph = nx.DiGraph()
    
    # Initialize analyzers
    interface_analyzer = InterfaceAnalyzer(str(root_dir))
    semantic_analyzer = SemanticAnalyzer()
    health_analyzer = CodeHealthAnalyzer()
    
    # Get high priority paths to process
    priority_paths = get_high_priority_paths()
    
    # Process high priority paths first
    for path in priority_paths:
        if path.exists():
            process_path(path, graph, interface_analyzer)
    
    # Process remaining paths that aren't excluded
    for path in root_dir.rglob("*"):
        if should_process_path(path, priority_paths):
            process_path(path, graph, interface_analyzer)
    
    # Add semantic relationships
    semantic_edges = semantic_analyzer.analyze_semantic_relationships(graph)
    for edge in semantic_edges:
        graph.add_edge(
            edge.source,
            edge.target,
            edge_type=edge.type,
            weight=edge.weight,
            properties=edge.properties
        )
    
    # Analyze code health
    health_issues = health_analyzer.analyze_health(graph)
    
    # Add health metadata to graph
    graph.graph['health_issues'] = health_issues
            
    return graph

def should_process_path(path: Path, priority_paths: List[Path]) -&gt; bool:
    """
    Determine if a path should be processed
    
    Args:
        path: Path to check
        priority_paths: List of high priority paths
        
    Returns:
        bool: True if path should be processed
    """
    # Skip directories
    if path.is_dir():
        return False
        
    # Convert to string for pattern matching
    path_str = str(path).replace('\\', '/')
    
    # Check if path is in included directories
    in_included_dir = False
    for included_dir in INCLUDED_DIRS:
        included_str = str(included_dir).replace('\\', '/')
        if path_str.startswith(included_str):
            in_included_dir = True
            break
            
    if not in_included_dir:
        return False
    
    # Check if path matches included patterns
    matches_pattern = False
    for pattern in INCLUDED_PATTERNS:
        if path.match(pattern):
            matches_pattern = True
            break
            
    if not matches_pattern:
        return False
    
    # Check exclusions
    if should_exclude_path(path_str):
        return False
        
    return True

def process_path(path: Path, graph: nx.DiGraph, analyzer: InterfaceAnalyzer) -&gt; None:
    """Process a path and add nodes/edges to graph
    
    Args:
        path: Path to process
        graph: Graph to add nodes/edges to
        analyzer: Interface analyzer instance
    """
    if path.is_file():
        process_file(graph, path)
    else:
        process_directory(path, graph, analyzer)

def process_file(graph: nx.DiGraph, file_path: str):
    """Process a single file and add its nodes and edges to the graph"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # Add file node
        file_node = file_path
        graph.add_node(file_node, type='file', path=file_path)
        
        # Parse Python files
        if file_path.endswith('.py'):
            tree = ast.parse(content)
            
            # Track imports
            imports = []
            
            # Process all nodes
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_node = f"{file_path}::{node.name}"
                    graph.add_node(class_node, 
                             type='class',
                             name=node.name,
                             file=file_path,
                             line_number=node.lineno,
                             docstring=ast.get_docstring(node))
                    graph.add_edge(file_node, class_node, edge_type='contains')
                    
                    # Process methods
                    for item in node.body:
                        if isinstance(item, ast.FunctionDef):
                            method_node = f"{file_path}::{node.name}::{item.name}"
                            graph.add_node(method_node,
                                     type='method',
                                     name=item.name,
                                     file=file_path,
                                     line_number=item.lineno,
                                     docstring=ast.get_docstring(item))
                            graph.add_edge(class_node, method_node, edge_type='contains')
                            
                elif isinstance(node, ast.FunctionDef):
                    if not isinstance(node.parent, ast.ClassDef):  # Only top-level functions
                        func_node = f"{file_path}::{node.name}"
                        graph.add_node(func_node,
                                 type='function',
                                 name=node.name,
                                 file=file_path,
                                 line_number=node.lineno,
                                 docstring=ast.get_docstring(node))
                        graph.add_edge(file_node, func_node, edge_type='contains')
                        
                elif isinstance(node, ast.Import):
                    for name in node.names:
                        imports.append(name.name.split('.')[0])
                        
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module.split('.')[0])
                        
            # Add import edges
            for imp in imports:
                # Find all files that might contain this import
                for n in graph.nodes():
                    if isinstance(n, str) and n.endswith(f"/{imp}.py"):
                        graph.add_edge(file_node, n, edge_type='imports')
                        
        # Process TypeScript/JavaScript files (basic version)
        elif file_path.endswith(('.ts', '.js')):
            # Add basic file info
            graph.add_node(file_node, 
                      type='file',
                      path=file_path,
                      language='typescript' if file_path.endswith('.ts') else 'javascript')
            
            # TODO: Add proper TypeScript/JavaScript parsing
            
    except Exception as e:
        logger.error(f"Error processing file {file_path}: {str(e)}")
        raise

def process_directory(path: Path, graph: nx.DiGraph, analyzer: InterfaceAnalyzer) -&gt; None:
    """Process a directory and add nodes/edges to graph"""
    # Process all files in the directory
    for file in path.rglob("*"):
        if file.is_file() and file.suffix in [".py", ".ts", ".tsx", ".js", ".jsx"]:
            process_file(graph, file)

def main():
    """Main entry point for knowledge graph generation"""
    logger.info("Starting knowledge graph generation...")
    
    # Initialize graph
    G = nx.DiGraph()
    
    # Define paths to analyze
    app_paths = [
        os.path.join(PROJECT_ROOT, "warehouse_quote_app"),
        os.path.join(PROJECT_ROOT, "tools"),
        os.path.join(PROJECT_ROOT, "tests")
    ]
    
    # Define exclusion patterns
    exclude_patterns = [
        r".*\.venv.*",
        r".*\.git.*",
        r".*__pycache__.*",
        r".*\.pytest_cache.*",
        r".*\.mypy_cache.*",
        r".*\.coverage.*",
        r".*\.idea.*",
        r".*\.vs.*",
        r".*node_modules.*",
        r".*\blib\b.*",
        r".*\bLibs\b.*",
        r".*\bsite-packages\b.*"
    ]
    
    # Process each path
    for path in app_paths:
        if not os.path.exists(path):
            logger.warning(f"Path does not exist: {path}")
            continue
            
        logger.info(f"Processing path: {path}")
        for root, dirs, files in os.walk(path):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if not any(re.match(pattern, d) for pattern in exclude_patterns)]
            
            # Process Python files
            for file in files:
                if file.endswith(('.py', '.ts', '.js')):
                    file_path = os.path.join(root, file)
                    if any(re.match(pattern, file_path) for pattern in exclude_patterns):
                        continue
                        
                    try:
                        process_file(G, file_path)
                    except Exception as e:
                        logger.error(f"Error processing {file_path}: {str(e)}")
                        continue
    
    # Save graph
    logger.info(f"Generated graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    
    # Save as GraphML
    graphml_path = os.path.join(PROJECT_ROOT, "code_knowledge_graph.graphml")
    nx.write_graphml(G, graphml_path)
    logger.info(f"Saved GraphML to {graphml_path}")
    
    # Save as JSON for easier debugging
    json_path = os.path.join(PROJECT_ROOT, "code_knowledge_graph.json")
    json_graph = nx.node_link_data(G)
    with open(json_path, 'w') as f:
        json.dump(json_graph, f, indent=2)
    logger.info(f"Saved JSON to {json_path}")
    
    # Generate visualization
    try:
        plt.figure(figsize=(20, 20))
        pos = nx.spring_layout(G)
        nx.draw(G, pos, with_labels=True, node_color='lightblue', 
               node_size=1500, font_size=8, font_weight='bold')
        plt.savefig(os.path.join(PROJECT_ROOT, 'code_knowledge_graph.png'))
        plt.close()
        logger.info("Saved visualization to code_knowledge_graph.png")
    except Exception as e:
        logger.warning(f"Could not save graph visualization: {str(e)}")

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\kg_generator.py</data>
</node>
<node id="memory_enforcer">
  <data key="d0">module</data>
  <data key="d1">"""
Memory Enforcer
Ensures strict compliance with system memories and policies
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Set, Any
from dataclasses import dataclass
from datetime import datetime
import json
import re

logger = logging.getLogger(__name__)

@dataclass
class MemoryValidation:
    """Result of memory compliance validation"""
    is_valid: bool
    checked_memories: List[str]
    violations: List[str]
    warnings: List[str]
    validation_data: Dict[str, Any]

class MemoryEnforcer:
    """
    Enforces strict compliance with system memories and policies.
    Acts as a gatekeeper for all operations.
    """
    
    def __init__(self, memories_path: Path):
        self.memories_path = memories_path
        self.memories_path.mkdir(exist_ok=True)
        self.validation_history: List[Dict] = []
        self.load_memories()
        
    def load_memories(self):
        """Load all memories from storage"""
        self.memories = {}
        try:
            memory_files = list(self.memories_path.glob("*.json"))
            for mf in memory_files:
                with open(mf, 'r') as f:
                    memory = json.load(f)
                    self.memories[memory['id']] = memory
        except Exception as e:
            logger.error(f"Error loading memories: {e}")
            raise
            
    def validate_operation(self, operation_type: str, context: Dict) -&gt; MemoryValidation:
        """
        Validate an operation against all applicable memories
        Returns MemoryValidation with detailed results
        """
        checked_memories = []
        violations = []
        warnings = []
        validation_data = {}
        
        for memory_id, memory in self.memories.items():
            checked_memories.append(memory_id)
            
            # Check if memory applies to this operation
            if not self._memory_applies(memory, operation_type, context):
                continue
                
            # Validate against memory requirements
            memory_violations = self._check_memory_compliance(memory, context)
            if memory_violations:
                violations.extend(memory_violations)
                
            # Gather validation data
            validation_data[memory_id] = {
                'title': memory.get('title'),
                'checked_rules': len(memory.get('content', '').split('\n')),
                'violations': memory_violations
            }
            
        # Record validation
        self._record_validation(operation_type, context, violations)
        
        return MemoryValidation(
            is_valid=len(violations) == 0,
            checked_memories=checked_memories,
            violations=violations,
            warnings=warnings,
            validation_data=validation_data
        )
        
    def _memory_applies(self, memory: Dict, operation_type: str, context: Dict) -&gt; bool:
        """Check if a memory applies to the current operation"""
        # Global memories always apply
        if 'user_global' in memory.get('tags', []):
            return True
            
        # Check operation-specific memories
        if operation_type in memory.get('tags', []):
            return True
            
        # Check context-specific memories
        if any(tag in context.get('tags', []) for tag in memory.get('tags', [])):
            return True
            
        return False
        
    def _check_memory_compliance(self, memory: Dict, context: Dict) -&gt; List[str]:
        """Check compliance with a specific memory"""
        violations = []
        content = memory.get('content', '')
        
        # Parse requirements from memory content
        requirements = self._parse_requirements(content)
        
        for req in requirements:
            if not self._requirement_satisfied(req, context):
                violations.append(f"Violation of {memory['title']}: {req}")
                
        return violations
        
    def _parse_requirements(self, content: str) -&gt; List[str]:
        """Parse requirements from memory content"""
        requirements = []
        
        # Split on numbered lists
        numbered = re.split(r'\d+\.\s+', content)
        if len(numbered) &gt; 1:
            requirements.extend([req.strip() for req in numbered[1:]])
            
        # Split on bullet points
        bulleted = re.split(r'[-•]\s+', content)
        if len(bulleted) &gt; 1:
            requirements.extend([req.strip() for req in bulleted[1:]])
            
        # If no structured lists found, treat each line as a requirement
        if not requirements:
            requirements = [line.strip() for line in content.split('\n') if line.strip()]
            
        return requirements
        
    def _requirement_satisfied(self, requirement: str, context: Dict) -&gt; bool:
        """Check if a specific requirement is satisfied"""
        # Convert requirement to lowercase for matching
        req_lower = requirement.lower()
        
        # Check for file operations
        if 'file' in req_lower:
            if 'file_operations' not in context:
                return False
            if 'readme.md' in req_lower and not any('readme.md' in op.lower() for op in context['file_operations']):
                return False
                
        # Check for search operations
        if 'search' in req_lower:
            if 'search_performed' not in context or not context['search_performed']:
                return False
                
        # Check for validation
        if 'validate' in req_lower or 'check' in req_lower:
            if 'validation_performed' not in context or not context['validation_performed']:
                return False
                
        # Check for semantic context
        if 'semantic' in req_lower or 'context' in req_lower:
            if 'semantic_context_checked' not in context or not context['semantic_context_checked']:
                return False
                
        return True
        
    def _record_validation(self, operation_type: str, context: Dict, violations: List[str]):
        """Record validation for history and analysis"""
        record = {
            'timestamp': datetime.now().isoformat(),
            'operation_type': operation_type,
            'context_summary': {k: str(v) for k, v in context.items()},
            'violations': violations
        }
        
        self.validation_history.append(record)
        
        # Keep only last 1000 validations
        if len(self.validation_history) &gt; 1000:
            self.validation_history = self.validation_history[-1000:]
            
    def get_validation_history(self, operation_type: Optional[str] = None) -&gt; List[Dict]:
        """Get validation history, optionally filtered by operation type"""
        if operation_type:
            return [
                record for record in self.validation_history
                if record['operation_type'] == operation_type
            ]
        return self.validation_history
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\memory_enforcer.py</data>
</node>
<node id="semantic_context_checker">
  <data key="d0">module</data>
  <data key="d1">"""
Semantic Context Checker
Enforces the policy that semantic context must be consulted before any code changes
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from datetime import datetime

from .semantic_context_manager import SemanticContextManager

logger = logging.getLogger(__name__)

@dataclass
class ContextValidation:
    """Result of semantic context validation"""
    is_valid: bool
    context_data: Dict
    related_files: Set[str]
    impact_assessment: Dict
    error: Optional[str] = None
    warnings: List[str] = []

class SemanticContextChecker:
    """
    Enforces the policy that semantic context must be consulted before any code changes.
    This class acts as a pre-validation step for all code modifications.
    """
    
    def __init__(self, semantic_manager: SemanticContextManager):
        self.semantic_manager = semantic_manager
        self.validation_history: List[Tuple[str, ContextValidation]] = []
        
    def validate_change(self, file_path: str, proposed_content: str) -&gt; ContextValidation:
        """
        Validates a proposed change against the semantic context.
        Must be called before any code modifications.
        """
        try:
            # Get current semantic context
            current_context = self.semantic_manager.get_context(file_path)
            if not current_context:
                return ContextValidation(
                    False,
                    {},
                    set(),
                    {},
                    "No semantic context found for file"
                )
                
            # Analyze semantic impact
            semantic_impact = self.semantic_manager.analyze_changes(
                current_context.get("content", ""),
                proposed_content
            )
            
            # Get related files
            related_files = set()
            for relationship_type in ["new_relationships", "removed_relationships", "maintained_relationships"]:
                related_files.update(semantic_impact.get(relationship_type, []))
                
            # Assess impact
            impact_assessment = self._assess_impact(semantic_impact)
            
            # Record validation
            validation = ContextValidation(
                True,
                current_context,
                related_files,
                impact_assessment
            )
            
            # Add warnings for high impact changes
            if impact_assessment["impact_score"] &gt; 0.7:
                validation.warnings.append(
                    f"High impact change detected (score: {impact_assessment['impact_score']:.2f})"
                )
                
            # Record validation history
            self._record_validation(file_path, validation)
            
            return validation
            
        except Exception as e:
            logger.error(f"Error validating semantic context for {file_path}: {e}")
            return ContextValidation(
                False,
                {},
                set(),
                {},
                str(e)
            )
            
    def _assess_impact(self, semantic_impact: Dict) -&gt; Dict:
        """Assess the impact of semantic changes"""
        impact = {
            "impact_score": 0.0,
            "breaking_changes": [],
            "new_patterns": [],
            "affected_patterns": []
        }
        
        # Calculate impact score
        removed = len(semantic_impact.get("removed_relationships", []))
        new = len(semantic_impact.get("new_relationships", []))
        maintained = len(semantic_impact.get("maintained_relationships", []))
        
        # Higher score for removing relationships
        impact["impact_score"] += removed * 0.2
        
        # Medium score for new relationships
        impact["impact_score"] += new * 0.1
        
        # Lower score if maintaining relationships
        impact["impact_score"] = max(0.0, impact["impact_score"] - (maintained * 0.05))
        
        # Cap at 1.0
        impact["impact_score"] = min(impact["impact_score"], 1.0)
        
        # Identify breaking changes
        if removed &gt; 0:
            impact["breaking_changes"].extend(
                semantic_impact.get("removed_relationships", [])
            )
            
        # Identify new patterns
        if new &gt; maintained:
            impact["new_patterns"].extend(
                semantic_impact.get("new_relationships", [])
            )
            
        # Identify affected patterns
        impact["affected_patterns"].extend(
            semantic_impact.get("maintained_relationships", [])
        )
        
        return impact
        
    def _record_validation(self, file_path: str, validation: ContextValidation):
        """Record validation for history and analysis"""
        self.validation_history.append((
            datetime.now().isoformat(),
            file_path,
            validation
        ))
        
        # Keep only last 100 validations
        if len(self.validation_history) &gt; 100:
            self.validation_history = self.validation_history[-100:]
            
    def get_validation_history(self, file_path: Optional[str] = None) -&gt; List[Tuple]:
        """Get validation history, optionally filtered by file"""
        if file_path:
            return [
                (ts, fp, val) for ts, fp, val in self.validation_history
                if fp == file_path
            ]
        return self.validation_history
        
    def get_last_validation(self, file_path: str) -&gt; Optional[ContextValidation]:
        """Get the last validation for a specific file"""
        history = self.get_validation_history(file_path)
        return history[-1][2] if history else None
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\semantic_context_checker.py</data>
</node>
<node id="semantic_context_manager">
  <data key="d0">module</data>
  <data key="d1">"""
Semantic Context Manager for code analysis and understanding
"""

from pathlib import Path
import json
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer, util
from .memory_enforcer import MemoryEnforcer
import logging
import numpy as np
import torch
from typing import List, Dict, Any, Optional
import os
import ast
import re

@dataclass
class SemanticContext:
    embedding: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = None

@dataclass
class CodeComponent:
    path: str
    content: str
    semantic_context: SemanticContext = None
    name: str = None
    type: str = "module"

    def __post_init__(self):
        if self.semantic_context is None:
            self.semantic_context = SemanticContext()
        if self.name is None:
            # Extract name from path - use the file name without extension
            self.name = os.path.splitext(os.path.basename(self.path))[0]

class SemanticContextManager:
    """Manages semantic context for code analysis"""
    
    def __init__(self, root_path: str, exclude_patterns: Optional[List[str]] = None):
        """Initialize semantic context manager"""
        self.root_path = root_path
        self.components: List[CodeComponent] = []
        self.context_dir = Path(root_path) / ".semantic_context"
        self.exclude_patterns = exclude_patterns or [
            r".*\.venv.*",
            r".*\.git.*",
            r".*__pycache__.*",
            r".*\.pytest_cache.*",
            r".*\.mypy_cache.*",
            r".*\.coverage.*",
            r".*\.idea.*",
            r".*\.vs.*",
            r".*node_modules.*",
            r".*\blib\b.*",
            r".*\bLibs\b.*",
            r".*\bsite-packages\b.*",
            r".*\.ts$",  # Exclude TypeScript files
            r".*\.tsx$",  # Exclude React TypeScript files
            r".*\.js$",   # Exclude JavaScript files
            r".*\.jsx$"   # Exclude React JavaScript files
        ]
        
        self.context_dir.mkdir(parents=True, exist_ok=True)
        # Use a lighter model variant
        self.model = SentenceTransformer('paraphrase-MiniLM-L3-v2')
        self.memory_enforcer = MemoryEnforcer(self.context_dir / "memories")
        self.embedding_cache_file = self.context_dir / "embedding_cache.json"
        self.batch_size = 32  # Process embeddings in batches
        self.duplication_threshold = 0.85
        self.orphan_threshold = 0.3
        self.divergence_threshold = 0.4
        self.embedding_cache = {}
        
        # Load existing cache if available
        self._load_embedding_cache()
        
    def _load_embedding_cache(self):
        """Load cached embeddings from disk"""
        if self.embedding_cache_file.exists():
            with open(self.embedding_cache_file, 'r') as f:
                cache_data = json.load(f)
                for path, embedding in cache_data.items():
                    self.embedding_cache[path] = np.array(embedding)
                    
    def _save_embedding_cache(self):
        """Save embeddings cache to disk"""
        cache_data = {
            path: embedding.tolist() 
            for path, embedding in self.embedding_cache.items()
        }
        with open(self.embedding_cache_file, 'w') as f:
            json.dump(cache_data, f)
            
    def _compute_embeddings_batch(self, components: List[CodeComponent]):
        """Compute embeddings in batches"""
        texts = [comp.content for comp in components]
        embeddings = []
        
        # Process in batches
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            embeddings.extend(batch_embeddings)
            
        # Update components and cache
        for comp, emb in zip(components, embeddings):
            comp.semantic_context.embedding = emb
            self.embedding_cache[comp.path] = emb
            
        # Save cache periodically
        self._save_embedding_cache()
        
    def load_context(self):
        """Load semantic context from files"""
        # Walk through the directory tree
        for root, dirs, files in os.walk(self.root_path):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if not any(re.match(pattern, d) for pattern in self.exclude_patterns)]
            
            # Process Python files
            for file in files:
                if not file.endswith('.py'):
                    continue
                    
                file_path = os.path.join(root, file)
                
                # Skip excluded files
                if any(re.match(pattern, file_path) for pattern in self.exclude_patterns):
                    continue
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # Create relative path for component name
                    rel_path = os.path.relpath(file_path, self.root_path)
                    component = CodeComponent(
                        path=file_path,
                        content=content,
                        name=os.path.splitext(os.path.basename(file_path))[0]
                    )
                    self.components.append(component)
                    
                except Exception as e:
                    logging.error(f"Error loading file {file_path}: {str(e)}")
                    continue

    def analyze_codebase_health(self, root_dir: str = ".") -&gt; Dict[str, Any]:
        """Analyze the health of the codebase"""
        logging.info("Starting codebase health analysis...")
        
        # Load components if not already loaded
        if not self.components:
            self.load_context()
        
        if not self.components:
            logging.warning("No components to analyze")
            return {
                "health_score": 100.0,
                "metrics": {
                    "duplication_rate": 0.0,
                    "orphan_rate": 0.0,
                    "divergence_rate": 0.0
                },
                "issues": []
            }
        
        # Compute embeddings in batches
        self._compute_embeddings_batch(self.components)
        
        # Calculate metrics
        duplication_rate = self._calculate_duplication_rate()
        orphan_rate = self._calculate_orphan_rate()
        divergence_rate = self._calculate_divergence_rate()
        
        logging.info(f"Analysis metrics - Duplication: {duplication_rate:.2%}, Orphans: {orphan_rate:.2%}, Divergence: {divergence_rate:.2%}")
        
        # Identify specific issues
        issues = self._identify_issues()
        logging.info(f"Found {len(issues)} potential issues")
        
        # Calculate overall health score
        health_score = 100.0 * (1.0 - (
            0.4 * duplication_rate +
            0.3 * orphan_rate +
            0.3 * divergence_rate
        ))
        
        return {
            "health_score": health_score,
            "metrics": {
                "duplication_rate": duplication_rate,
                "orphan_rate": orphan_rate,
                "divergence_rate": divergence_rate
            },
            "issues": issues
        }

    def _calculate_duplication_rate(self) -&gt; float:
        """Calculate the duplication rate across components."""
        if len(self.components) &lt; 2:
            return 0.0

        # Convert all embeddings to float32 tensors
        embeddings = torch.tensor([comp.semantic_context.embedding for comp in self.components], dtype=torch.float32)
        
        # Calculate similarity matrix
        sim = util.cos_sim(embeddings, embeddings)
        
        # Mask out self-similarities
        mask = torch.eye(len(self.components), dtype=torch.bool)
        sim.masked_fill_(mask, 0)
        
        # Count pairs above threshold
        duplicate_pairs = torch.sum(sim &gt; self.duplication_threshold)
        total_pairs = len(self.components) * (len(self.components) - 1) / 2
        
        # Convert tensor to native Python float
        return float(duplicate_pairs.item()) / total_pairs if total_pairs &gt; 0 else 0.0

    def _calculate_orphan_rate(self) -&gt; float:
        """Calculate rate of orphaned components."""
        if len(self.components) &lt; 2:
            return 0.0

        # Convert all embeddings to float32 tensors
        embeddings = torch.tensor([comp.semantic_context.embedding for comp in self.components], dtype=torch.float32)
        
        # Calculate similarity matrix
        sim = util.cos_sim(embeddings, embeddings)
        
        # Mask out self-similarities
        mask = torch.eye(len(self.components), dtype=torch.bool)
        sim.masked_fill_(mask, 0)
        
        # Find max similarity for each component
        max_sim = torch.max(sim, dim=1)[0]
        
        # Count components with no strong connections
        orphan_count = torch.sum(max_sim &lt; self.orphan_threshold).item()
        
        # Convert tensor to native Python float
        return float(orphan_count) / len(self.components)

    def _calculate_divergence_rate(self) -&gt; float:
        """Calculate component divergence rate."""
        if len(self.components) &lt; 2:
            return 0.0

        # Convert all embeddings to float32 tensors
        embeddings = torch.tensor([comp.semantic_context.embedding for comp in self.components], dtype=torch.float32)
        
        # Calculate similarity matrix
        sim = util.cos_sim(embeddings, embeddings)
        
        # Mask out self-similarities
        mask = torch.eye(len(self.components), dtype=torch.bool)
        sim.masked_fill_(mask, 0)
        
        # Calculate average similarity for each component
        avg_sim = torch.mean(sim, dim=1)
        
        # Count components with low average similarity
        divergent_mask = avg_sim &lt; self.divergence_threshold
        divergent_count = torch.sum(divergent_mask).item()
        
        # Convert tensor to native Python float
        return float(divergent_count) / len(self.components)

    def _identify_issues(self) -&gt; List[Dict[str, Any]]:
        """Identify specific issues in the codebase"""
        issues = []
        
        # Find duplicates
        for i, comp1 in enumerate(self.components):
            duplicates = []
            for j, comp2 in enumerate(self.components[i+1:], i+1):
                # Convert embeddings to float32 before similarity calculation
                emb1 = torch.tensor(comp1.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                emb2 = torch.tensor(comp2.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                sim = float(util.cos_sim(emb1, emb2)[0][0].item())  # Convert to native float
                if sim &gt; 0.85:
                    duplicates.append(comp2.path)
            
            if duplicates:
                issues.append({
                    "type": "duplication",
                    "component": comp1.path,
                    "related_components": duplicates,
                    "impact": len(duplicates) / len(self.components)
                })
        
        # Find orphans
        for comp in self.components:
            max_sim = 0.0
            for other in self.components:
                if comp != other:
                    # Convert embeddings to float32 before similarity calculation
                    emb1 = torch.tensor(comp.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                    emb2 = torch.tensor(other.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                    sim = float(util.cos_sim(emb1, emb2)[0][0].item())  # Convert to native float
                    max_sim = max(max_sim, sim)
            
            if max_sim &lt; self.orphan_threshold:
                issues.append({
                    "type": "orphaned",
                    "component": comp.path,
                    "impact": 1 - max_sim
                })
        
        # Find divergent components
        for comp in self.components:
            similarities = []
            for other in self.components:
                if comp != other:
                    # Convert embeddings to float32 before similarity calculation
                    emb1 = torch.tensor(comp.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                    emb2 = torch.tensor(other.semantic_context.embedding, dtype=torch.float32).reshape(1, -1)
                    sim = float(util.cos_sim(emb1, emb2)[0][0].item())  # Convert to native float
                    similarities.append(sim)
            
            avg_sim = sum(similarities) / len(similarities) if similarities else 0
            if avg_sim &lt; self.divergence_threshold:
                issues.append({
                    "type": "divergent",
                    "component": comp.path,
                    "impact": 1 - avg_sim
                })
        
        return issues

    def _generate_recommendations(self) -&gt; List[str]:
        """Generate recommendations based on identified issues"""
        recommendations = []
        
        # Count issue types
        issue_counts = {
            "duplication": 0,
            "orphaned": 0,
            "divergent": 0
        }
        
        for issue in self._identify_issues():
            issue_counts[issue["type"]] += 1
        
        # Generate specific recommendations
        if issue_counts["duplication"] &gt; 0:
            recommendations.append(
                f"Found {issue_counts['duplication']} instances of potential code duplication. "
                "Consider refactoring into shared utilities or base classes."
            )
            
        if issue_counts["orphaned"] &gt; 0:
            recommendations.append(
                f"Found {issue_counts['orphaned']} potentially orphaned components. "
                "Review these components for proper integration or removal."
            )
            
        if issue_counts["divergent"] &gt; 0:
            recommendations.append(
                f"Found {issue_counts['divergent']} components that diverge significantly "
                "from the codebase patterns. Consider standardizing their implementation."
            )
            
        # Add general recommendations if needed
        if sum(issue_counts.values()) &gt; 5:
            recommendations.append(
                "Consider implementing a systematic code review process to maintain "
                "consistency and reduce technical debt."
            )
            
        return recommendations
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\semantic_context_manager.py</data>
</node>
<node id="ts_analyzer">
  <data key="d0">module</data>
  <data key="d1">"""
TypeScript Code Analyzer
Parses TypeScript files and extracts code structure for knowledge graph generation
"""

import ast_parser
from typing import Dict, List, Any, Optional
import json
import subprocess
import tempfile
import os
from pathlib import Path
import logging
import networkx as nx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TypeScriptAnalyzer:
    def __init__(self):
        """Initialize the TypeScript analyzer"""
        self.ensure_typescript_tools()
        
    def ensure_typescript_tools(self):
        """Ensure TypeScript compiler and parser are installed"""
        try:
            # Check if TypeScript is installed
            subprocess.run(["tsc", "--version"], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.info("Installing TypeScript...")
            subprocess.run(["npm", "install", "-g", "typescript"], check=True)
            
        # Install required npm packages if not present
        packages = ["@typescript-eslint/parser", "@typescript-eslint/typescript-estree"]
        for package in packages:
            try:
                subprocess.run(
                    ["npm", "list", "-g", package],
                    capture_output=True,
                    check=True
                )
            except subprocess.CalledProcessError:
                logger.info(f"Installing {package}...")
                subprocess.run(["npm", "install", "-g", package], check=True)
                
    def parse_typescript(self, file_path: str) -&gt; Dict[str, Any]:
        """Parse TypeScript file and return AST"""
        # Create a temporary JavaScript file for parsing
        with tempfile.NamedTemporaryFile(suffix='.js', delete=False) as temp_js:
            # Compile TypeScript to JavaScript
            subprocess.run(
                ["tsc", "--target", "ES2020", "--module", "commonjs", file_path],
                check=True
            )
            
            # Get the compiled JS file path
            js_path = str(Path(file_path).with_suffix('.js'))
            
            # Parse the JavaScript file
            with open(js_path, 'r') as f:
                js_content = f.read()
                
            # Clean up the compiled JS file
            os.unlink(js_path)
            
            # Parse JavaScript AST
            cmd = [
                "npx",
                "@typescript-eslint/parser",
                "--ecma-version",
                "2020",
                "--source-type",
                "module",
                js_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return json.loads(result.stdout)
            
    def extract_nodes_and_edges(self, ast: Dict[str, Any], file_path: str) -&gt; nx.DiGraph:
        """Extract nodes and edges from TypeScript AST"""
        graph = nx.DiGraph()
        
        def process_node(node: Dict[str, Any], parent: Optional[str] = None):
            """Process an AST node and extract relevant information"""
            node_type = node.get("type")
            
            if node_type == "ClassDeclaration":
                name = node["id"]["name"]
                node_id = f"{file_path}:{name}"
                graph.add_node(
                    node_id,
                    node_type="class",
                    name=name,
                    file_path=file_path,
                    language="typescript"
                )
                
                # Process class methods
                for member in node.get("body", {}).get("body", []):
                    if member["type"] == "MethodDefinition":
                        method_name = member["key"]["name"]
                        method_id = f"{node_id}.{method_name}"
                        graph.add_node(
                            method_id,
                            node_type="method",
                            name=method_name,
                            file_path=file_path,
                            language="typescript"
                        )
                        graph.add_edge(node_id, method_id, edge_type="DEFINES")
                        
            elif node_type == "FunctionDeclaration":
                name = node["id"]["name"]
                node_id = f"{file_path}:{name}"
                graph.add_node(
                    node_id,
                    node_type="function",
                    name=name,
                    file_path=file_path,
                    language="typescript"
                )
                
            elif node_type == "CallExpression":
                if "callee" in node:
                    callee = node["callee"]
                    if callee["type"] == "Identifier":
                        called_name = callee["name"]
                        if parent:
                            graph.add_edge(parent, f"{file_path}:{called_name}", edge_type="CALLS")
                            
            # Process imports
            elif node_type == "ImportDeclaration":
                source = node["source"]["value"]
                for specifier in node.get("specifiers", []):
                    if specifier["type"] == "ImportSpecifier":
                        imported_name = specifier["imported"]["name"]
                        local_name = specifier["local"]["name"]
                        graph.add_node(
                            f"{file_path}:{local_name}",
                            node_type="import",
                            name=local_name,
                            imported_name=imported_name,
                            source=source,
                            file_path=file_path,
                            language="typescript"
                        )
                        
            # Recursively process child nodes
            for key, value in node.items():
                if isinstance(value, dict):
                    process_node(value, parent)
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            process_node(item, parent)
                            
        process_node(ast)
        return graph
        
    def analyze_file(self, file_path: str) -&gt; nx.DiGraph:
        """Analyze a TypeScript file and return a knowledge graph"""
        try:
            ast = self.parse_typescript(file_path)
            return self.extract_nodes_and_edges(ast, file_path)
        except Exception as e:
            logger.error(f"Error analyzing TypeScript file {file_path}: {e}")
            return nx.DiGraph()
            
def main():
    """Test the TypeScript analyzer"""
    analyzer = TypeScriptAnalyzer()
    
    # Test with a sample TypeScript file
    test_ts = """
    import { Component } from '@angular/core';
    
    class TestClass {
        constructor() {}
        
        testMethod() {
            console.log('test');
        }
    }
    
    function testFunction() {
        const test = new TestClass();
        test.testMethod();
    }
    """
    
    with tempfile.NamedTemporaryFile(suffix='.ts', mode='w', delete=False) as f:
        f.write(test_ts)
        test_file = f.name
        
    try:
        graph = analyzer.analyze_file(test_file)
        print(f"Nodes: {list(graph.nodes(data=True))}")
        print(f"Edges: {list(graph.edges(data=True))}")
    finally:
        os.unlink(test_file)

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\ts_analyzer.py</data>
</node>
<node id="__init__">
  <data key="d0">module</data>
  <data key="d1">"""Utility functions and helpers for the quote app."""
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\utils\__init__.py</data>
</node>
<node id="code_health_manager">
  <data key="d0">module</data>
  <data key="d1">"""
Code Health Manager
Unified interface for analyzing and improving code health
"""

from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict, field
from enum import Enum
from pathlib import Path
import ast
import networkx as nx
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import threading
import logging
from datetime import datetime
import pytz
import numpy as np
import json
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
import os
import matplotlib.pyplot as plt

from tools.kg.semantic_context_manager import SemanticContextManager, CodeComponent
from tools.kg.config.kg_config import PROJECT_ROOT, InterfaceType

# Default code health rules
DEFAULT_HEALTH_RULES = {
    'duplication': {
        'max_similarity': 0.85,  # 85% similarity threshold for duplication
        'exclude_patterns': [
            r'.*test.*',  # Exclude test files
            r'.*\.config\..*',  # Exclude config files
            r'.*\.types\..*',  # Exclude type definition files
            r'.*\.ts.*',  # Exclude TypeScript files
            r'.*\.js.*',  # Exclude JavaScript files
            r'.*\.lib\..*',  # Exclude library files
            r'.*\.vendor\..*',  # Exclude vendor files
            r'.*\.generated\..*',  # Exclude generated files
            r'.*\.template\..*',  # Exclude template files
            r'.*\.example\..*',  # Exclude example files
            r'.*\.mock\..*',  # Exclude mock files
            r'.*\.stub\..*',  # Exclude stub files
            r'.*migrations.*',  # Exclude database migrations
            r'.*__init__\.py'  # Exclude __init__.py files
        ]
    },
    'divergence': {
        'interface_drift_threshold': 0.3,  # 30% drift threshold
        'min_implementation_similarity': 0.7,  # 70% minimum similarity
        'exclude_patterns': [
            r'.*test.*',  # Exclude test files
            r'.*\.mock\..*',  # Exclude mock implementations
            r'.*\.stub\..*',  # Exclude stub implementations
            r'.*\.example\..*'  # Exclude example implementations
        ]
    },
    'orphaned': {
        'min_connections': 1,  # Minimum number of connections
        'exclude_patterns': [
            r'.*\.main\..*',  # Exclude main entry points
            r'.*\.cli\..*',  # Exclude CLI entry points
            r'.*__main__\.py',  # Exclude __main__.py files
            r'.*__init__\.py',  # Exclude __init__.py files
            r'.*\.config\..*',  # Exclude config files
            r'.*\.settings\..*',  # Exclude settings files
            r'.*\.generated\..*',  # Exclude generated files
            r'.*\.template\..*',  # Exclude template files
            r'.*\.example\..*',  # Exclude example files
            r'.*migrations.*',  # Exclude database migrations
            r'.*\.wsgi\..*',  # Exclude WSGI files
            r'.*\.asgi\..*',  # Exclude ASGI files
            r'.*manage\.py',  # Exclude Django manage.py
            r'.*setup\.py',  # Exclude setup.py
            r'.*conftest\.py'  # Exclude pytest configuration
        ]
    }
}

def get_code_health_rules() -&gt; Dict[str, Any]:
    """Get code health rules with defaults"""
    # TODO: Load from config file if exists
    return DEFAULT_HEALTH_RULES

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class HealthIssueType(str, Enum):
    DUPLICATION = "duplication"
    ORPHANED = "orphaned"
    DIVERGENT = "divergent"
    STRUCTURAL = "structural"
    COMPLEXITY = "complexity"

class SeverityLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class HealthIssue:
    """Represents a code health issue"""
    type: HealthIssueType
    severity: SeverityLevel
    component: str
    details: str
    impact: float
    related_components: Optional[List[str]] = None
    recommendations: Optional[List[str]] = None

    def to_dict(self) -&gt; Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'type': self.type,
            'severity': self.severity,
            'component': self.component,
            'details': self.details,
            'impact': self.impact,
            'related_components': self.related_components or [],
            'recommendations': self.recommendations or []
        }

@dataclass
class HealthReport:
    """Complete health report for the codebase"""
    health_score: float
    issues: List[HealthIssue]
    metrics: Dict[str, float]
    recommendations: List[str]
    timestamp: str = field(default_factory=lambda: datetime.now(pytz.UTC).isoformat())

    def to_dict(self) -&gt; Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            "health_score": self.health_score,
            "issues": [issue.to_dict() for issue in self.issues],
            "metrics": self.metrics,
            "recommendations": self.recommendations,
            "generated_at": self.timestamp,
            "version": "1.0.0"
        }

class CodeHealthManager:
    """Manages code health analysis and improvement"""
    
    def __init__(self):
        """Initialize code health manager"""
        self.semantic_manager = None
        self.dependency_graph = nx.DiGraph()
        
        # Initialize embedding models
        logger.info("Use pytorch device_name: cpu")
        logger.info("Load pretrained SentenceTransformer: paraphrase-MiniLM-L3-v2")
        self.code_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')
        logger.info("Use pytorch device_name: cpu")
        logger.info("Load pretrained SentenceTransformer: all-MiniLM-L6-v2")
        self.doc_model = SentenceTransformer('all-MiniLM-L6-v2')

    def analyze_codebase(self, root_path: str) -&gt; Dict[str, Any]:
        """Analyze the codebase and generate a health report"""
        logger.info("Starting codebase health analysis...")
        
        # Initialize semantic context if needed
        if not self.semantic_manager:
            self.semantic_manager = SemanticContextManager(root_path)
            self.semantic_manager.load_context()

        # Build dependency graph
        logger.info("Building dependency graph...")
        self._build_dependency_graph(root_path)
        logger.info(f"Built dependency graph with {self.dependency_graph.number_of_nodes()} nodes and {self.dependency_graph.number_of_edges()} edges")
        
        # Save visualization
        self._save_graph_visualization()
        
        # Analyze issues
        health_data = {
            'timestamp': datetime.now().isoformat(),
            'issues': []
        }
        
        # Run analyses
        health_data['issues'].extend(self._analyze_orphaned(health_data))
        health_data['issues'].extend(self._analyze_duplication(health_data))
        health_data['issues'].extend(self._analyze_divergent(health_data))
        
        # Calculate health score
        health_score = self._calculate_health_score(health_data)
        health_data['health_score'] = health_score
        
        # Log findings
        self._log_health_findings(health_data)
        
        # Save report
        report_path = self._save_health_report(health_data)
        logger.info(f"Health analysis complete. Report saved to: {report_path}")
        logger.info(f"Overall health score: {health_score:.2f}")
        
        return health_data

    def _log_health_findings(self, health_data: Dict[str, Any]) -&gt; None:
        """Log detailed findings from health analysis"""
        issues_by_type = defaultdict(list)
        for issue in health_data['issues']:
            issues_by_type[issue.type].append(issue)
            
        logger.info(f"\nHealth Analysis Summary:")
        logger.info(f"------------------------")
        logger.info(f"Total Components: {health_data['metrics']['total_components']}")
        logger.info(f"Health Score: {health_data['health_score']:.2f}")
        logger.info(f"Metrics:")
        for metric, value in health_data['metrics'].items():
            if metric != 'total_components':
                logger.info(f"  - {metric}: {value:.1f}%")
                
        if health_data['issues']:
            logger.info(f"\nIssues Found:")
            for issue_type, issues in issues_by_type.items():
                logger.info(f"\n{issue_type}:")
                for issue in issues:
                    logger.info(f"  - {issue.component} ({issue.severity}):")
                    logger.info(f"    {issue.details}")
                    if issue.recommendations:
                        logger.info("    Recommendations:")
                        for rec in issue.recommendations:
                            logger.info(f"      * {rec}")

    def _save_graph_visualization(self) -&gt; None:
        """Save visualization of the dependency graph"""
        try:
            # Create a new figure
            plt.figure(figsize=(12, 8))
            
            # Use spring layout for better visualization
            pos = nx.spring_layout(self.dependency_graph)
            
            # Draw nodes and edges
            nx.draw(self.dependency_graph, pos,
                   with_labels=True,
                   node_color='lightblue',
                   node_size=1000,
                   font_size=8,
                   font_weight='bold',
                   arrows=True,
                   edge_color='gray',
                   width=1)
            
            # Save the graph
            plt.savefig('dependency_graph.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("Saved dependency graph visualization to dependency_graph.png")
            
            # Also save as GraphML for more detailed analysis
            nx.write_graphml(self.dependency_graph, 'code_knowledge_graph.graphml')
            logger.info("Saved detailed graph data to code_knowledge_graph.graphml")
        except Exception as e:
            logger.error(f"Failed to save graph visualization: {str(e)}")

    def _build_dependency_graph(self, root_path: str):
        """Build dependency graph from imports using AST"""
        logger.info("Building dependency graph...")
        self.dependency_graph = nx.DiGraph()
        
        # First add all components as nodes
        for component in self.semantic_manager.components:
            if component.path.endswith('.py'):  # Only process Python files
                try:
                    with open(component.path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # Add node with content and metadata
                    self.dependency_graph.add_node(
                        component.name,
                        type=component.type,
                        content=content,
                        path=component.path
                    )
                    
                    # Check if it's an interface
                    if (
                        'interface' in component.name.lower() or
                        'abc' in component.name.lower() or
                        'protocol' in component.name.lower()
                    ):
                        self.dependency_graph.nodes[component.name]['interface_type'] = 'service_interface'
                        
                except Exception as e:
                    logger.error(f"Error reading file {component.path}: {str(e)}")
                    continue
            
        # Then analyze each component's imports and add edges
        for component in self.semantic_manager.components:
            if not component.path.endswith('.py'):  # Skip non-Python files
                continue
                
            try:
                with open(component.path, 'r', encoding='utf-8') as f:
                    tree = ast.parse(f.read())
                    
                # Find all import statements
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for name in node.names:
                            imported_name = name.name.split('.')[0]
                            # Only add edge if the imported module is in our components
                            if any(c.name == imported_name for c in self.semantic_manager.components):
                                self.dependency_graph.add_edge(component.name, imported_name)
                                
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module_name = node.module.split('.')[0]
                            # Only add edge if the module is in our components
                            if any(c.name == module_name for c in self.semantic_manager.components):
                                self.dependency_graph.add_edge(component.name, module_name)
                                
            except Exception as e:
                logger.error(f"Error parsing Python file {component.path}: {str(e)}")
                continue
                
        logger.info(f"Built dependency graph with {self.dependency_graph.number_of_nodes()} nodes and {self.dependency_graph.number_of_edges()} edges")

    def _analyze_duplication(self, health_data: Dict[str, Any]) -&gt; List[HealthIssue]:
        """Analyze code duplication"""
        issues = []
        rules = get_code_health_rules()['duplication']
        
        # Get all implementation nodes
        components = []
        contents = []
        
        for node, data in self.dependency_graph.nodes(data=True):
            if data.get('content') and not any(re.match(pattern, node) for pattern in rules['exclude_patterns']):
                components.append(node)
                contents.append(data['content'])
                
        if len(contents) &lt; 2:  # Need at least 2 components to compare
            return issues
            
        # Calculate embeddings for all contents
        embeddings = self.code_model.encode(contents)
        
        # Calculate pairwise similarities
        similarities = cosine_similarity(embeddings)
        
        # Find duplicates
        for i in range(len(components)):
            for j in range(i + 1, len(components)):
                similarity = similarities[i][j]
                
                if similarity &gt; rules['max_similarity']:
                    impact = (similarity - rules['max_similarity']) / (1 - rules['max_similarity'])
                    issues.append(HealthIssue(
                        type=HealthIssueType.DUPLICATION,
                        severity=self._calculate_severity(impact),
                        component=components[i],
                        details=f"High similarity ({similarity:.2f}) with {components[j]}",
                        impact=impact,
                        related_components=[components[j]],
                        recommendations=self._get_duplication_recommendations(components[i], [components[j]])
                    ))
                    
        return issues

    def _analyze_divergent(self, health_data: Dict[str, Any]) -&gt; List[HealthIssue]:
        """Analyze divergent components"""
        issues = []
        rules = get_code_health_rules()['divergence']
        
        # Find interface-implementation pairs
        for node, data in self.dependency_graph.nodes(data=True):
            if data.get('interface_type') == 'service_interface':
                # Get implementations
                implementations = []
                for impl, impl_data in self.dependency_graph.nodes(data=True):
                    if (
                        impl != node and  # Not the interface itself
                        impl_data.get('content') and  # Has content
                        not any(re.match(pattern, impl) for pattern in rules['exclude_patterns']) and  # Not excluded
                        (
                            node.lower() in impl.lower() or  # Implementation likely related to interface
                            self.dependency_graph.has_edge(impl, node)  # Direct dependency
                        )
                    ):
                        implementations.append(impl)
                
                if not implementations:
                    continue
                    
                # Calculate embeddings
                contents = [data['content']] + [
                    self.dependency_graph.nodes[impl]['content']
                    for impl in implementations
                ]
                embeddings = self.doc_model.encode(contents)
                
                # Compare interface with implementations
                interface_vec = embeddings[0].reshape(1, -1)
                for i, impl in enumerate(implementations, 1):
                    impl_vec = embeddings[i].reshape(1, -1)
                    similarity = cosine_similarity(interface_vec, impl_vec.reshape(1, -1))[0][0]
                    
                    if similarity &lt; rules['min_implementation_similarity']:
                        impact = (rules['min_implementation_similarity'] - similarity) / rules['min_implementation_similarity']
                        issues.append(HealthIssue(
                            type=HealthIssueType.DIVERGENT,
                            severity=self._calculate_severity(impact),
                            component=impl,
                            details=f"Implementation diverges from interface {node} (similarity: {similarity:.2f})",
                            impact=impact,
                            related_components=[node],
                            recommendations=[
                                f"Review implementation of {impl} against interface {node}",
                                "Consider updating implementation to better match interface contract",
                                "Add documentation explaining intentional deviations"
                            ]
                        ))
                    
        return issues

    def _analyze_orphaned(self, health_data: Dict[str, Any]) -&gt; List[HealthIssue]:
        """Analyze orphaned components"""
        issues = []
        rules = get_code_health_rules()['orphaned']
        
        for node in self.dependency_graph.nodes():
            in_degree = self.dependency_graph.in_degree(node)
            out_degree = self.dependency_graph.out_degree(node)
            
            # Skip components that match exclude patterns
            if any(re.match(pattern, node) for pattern in rules['exclude_patterns']):
                continue
                
            # Consider a component orphaned if it has very few connections
            if in_degree + out_degree &lt; rules['min_connections']:
                severity = SeverityLevel.HIGH if in_degree + out_degree == 0 else SeverityLevel.MEDIUM
                
                issues.append(HealthIssue(
                    type=HealthIssueType.ORPHANED,
                    severity=severity,
                    component=node,
                    details=f"Component has few references (in: {in_degree}, out: {out_degree})",
                    impact=1.0 - ((in_degree + out_degree) / rules['min_connections']),
                    recommendations=[
                        f"Consider integrating '{node}' with other components",
                        "Review if this component is still needed",
                        "Consider merging with related components if functionality overlaps"
                    ]
                ))
                
        logger.info(f"Found {len(issues)} orphaned components")
        return issues
        
    def _find_similar_components(self, component: CodeComponent, threshold: float) -&gt; List[str]:
        """Find components with similarity above threshold"""
        similar = []
        if not component.semantic_context.embedding:
            return similar
            
        # Use numpy for efficient similarity computation
        for other in self.semantic_manager.components:
            if other.path == component.path or not other.semantic_context.embedding:
                continue
                
            similarity = np.dot(component.semantic_context.embedding, other.semantic_context.embedding) / (
                np.linalg.norm(component.semantic_context.embedding) * np.linalg.norm(other.semantic_context.embedding)
            )
            
            if similarity &gt; threshold:
                similar.append(other.path)
                
        return similar
        
    def _get_duplication_recommendations(self, component_path: str, similar_components: List[str]) -&gt; List[str]:
        """Generate recommendations for handling duplicated code"""
        recommendations = []
        
        if len(similar_components) &gt; 3:
            # Many duplicates suggest need for shared module
            recommendations.append(
                f"Extract common functionality from {component_path} and {len(similar_components)} similar files into a shared module"
            )
            recommendations.append("Consider creating a utility class or function library")
        else:
            # Fewer duplicates might be handled case by case
            recommendations.append(
                f"Review {component_path} and similar files for code that can be consolidated"
            )
            
        return recommendations
        
    def _generate_recommendations(self, issues: List[HealthIssue]) -&gt; List[str]:
        """Generate overall recommendations based on issues found"""
        recommendations = set()  # Use set to avoid duplicates
        
        # Group issues by type
        issues_by_type = defaultdict(list)
        for issue in issues:
            issues_by_type[issue.type].append(issue)
            
        # Generate type-specific recommendations
        if issues_by_type[HealthIssueType.DUPLICATION]:
            dup_count = len(issues_by_type[HealthIssueType.DUPLICATION])
            if dup_count &gt; 5:
                recommendations.add(
                    f"High code duplication detected ({dup_count} instances). Consider a comprehensive refactoring to extract common patterns."
                )
            recommendations.add("Implement a code review process to prevent future duplications")
            
        if issues_by_type[HealthIssueType.ORPHANED]:
            orphan_count = len(issues_by_type[HealthIssueType.ORPHANED])
            recommendations.add(
                f"Found {orphan_count} orphaned components. Review and either integrate or remove unused code."
            )
            
        if issues_by_type[HealthIssueType.DIVERGENT]:
            div_count = len(issues_by_type[HealthIssueType.DIVERGENT])
            recommendations.add(
                f"Found {div_count} divergent components. Review for consistency and consider standardizing implementations."
            )
            
        if issues_by_type[HealthIssueType.STRUCTURAL]:
            struct_count = len(issues_by_type[HealthIssueType.STRUCTURAL])
            recommendations.add(
                f"Found {struct_count} structural issues. Review dependency patterns and consider architectural improvements."
            )
            
        return list(recommendations)
        
    def _calculate_structural_complexity(self) -&gt; float:
        """Calculate overall structural complexity score"""
        if not self.dependency_graph:
            return 0.0
            
        # Use graph metrics to calculate complexity
        n_nodes = self.dependency_graph.number_of_nodes()
        if n_nodes == 0:
            return 0.0
            
        n_edges = self.dependency_graph.number_of_edges()
        cycles = len(list(nx.simple_cycles(self.dependency_graph)))
        
        # Normalize metrics
        edge_density = n_edges / (n_nodes * (n_nodes - 1)) if n_nodes &gt; 1 else 0
        cycle_density = cycles / n_nodes if n_nodes &gt; 0 else 0
        
        # Weighted combination
        return 0.5 * edge_density + 0.5 * cycle_density
        
    def _calculate_severity(self, impact: float) -&gt; SeverityLevel:
        """Calculate severity level based on impact score"""
        if impact &gt; 0.8:
            return SeverityLevel.CRITICAL
        elif impact &gt; 0.6:
            return SeverityLevel.HIGH
        elif impact &gt; 0.4:
            return SeverityLevel.MEDIUM
        else:
            return SeverityLevel.LOW

    def _calculate_duplication_rate(self) -&gt; float:
        """Calculate duplication rate"""
        # TO DO: implement duplication rate calculation
        return 0.0

    def _calculate_orphan_rate(self) -&gt; float:
        """Calculate orphan rate"""
        total_nodes = self.dependency_graph.number_of_nodes()
        if total_nodes == 0:
            return 0.0
            
        orphan_count = sum(1 for node in self.dependency_graph.nodes()
                          if self.dependency_graph.in_degree(node) + self.dependency_graph.out_degree(node) &lt; 
                          get_code_health_rules()['orphaned']['min_connections'])
                          
        return (orphan_count / total_nodes) * 100.0

    def _calculate_divergence_rate(self) -&gt; float:
        """Calculate divergence rate"""
        # TO DO: implement divergence rate calculation
        return 0.0

    def _calculate_health_score(self, health_data: Dict[str, Any]) -&gt; float:
        """Calculate overall health score"""
        # Base score starts at 100
        score = 100.0
        
        # Calculate metrics
        total_nodes = self.dependency_graph.number_of_nodes()
        if total_nodes == 0:
            return 0.0
            
        orphaned_count = sum(1 for issue in health_data['issues'] if issue.type == HealthIssueType.ORPHANED)
        duplication_count = sum(1 for issue in health_data['issues'] if issue.type == HealthIssueType.DUPLICATION)
        divergent_count = sum(1 for issue in health_data['issues'] if issue.type == HealthIssueType.DIVERGENT)
        
        # Update metrics
        health_data['metrics'] = {
            'total_components': total_nodes,
            'orphaned_rate': (orphaned_count / total_nodes) * 100 if total_nodes &gt; 0 else 0,
            'duplication_rate': (duplication_count / total_nodes) * 100 if total_nodes &gt; 0 else 0,
            'divergent_rate': (divergent_count / total_nodes) * 100 if total_nodes &gt; 0 else 0
        }
        
        # Deduct points based on issues
        for issue in health_data['issues']:
            if issue.severity == SeverityLevel.LOW:
                score -= 1
            elif issue.severity == SeverityLevel.MEDIUM:
                score -= 2
            elif issue.severity == SeverityLevel.HIGH:
                score -= 5
            elif issue.severity == SeverityLevel.CRITICAL:
                score -= 10
                
        # Ensure score doesn't go below 0
        return max(0.0, score)

    def _save_health_report(self, health_data: Dict[str, Any]) -&gt; str:
        """Save health report to JSON file"""
        # Convert HealthIssue objects to dictionaries for JSON serialization
        serializable_data = {
            'timestamp': health_data.get('timestamp'),
            'health_score': health_data.get('health_score'),
            'metrics': health_data.get('metrics', {}),
            'issues': [issue.to_dict() for issue in health_data['issues']]
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(os.getcwd(), f'code_health_report_{timestamp}.json')
        with open(report_path, 'w') as f:
            json.dump(serializable_data, f, indent=2)
        return report_path

    def _log_health_findings(self, health_data: Dict[str, Any]) -&gt; None:
        """Log detailed findings from health analysis"""
        issues_by_type = defaultdict(list)
        for issue in health_data['issues']:
            issues_by_type[issue.type].append(issue)
            
        logger.info(f"\nHealth Analysis Summary:")
        logger.info(f"------------------------")
        logger.info(f"Total Components: {health_data['metrics']['total_components']}")
        logger.info(f"Health Score: {health_data['health_score']:.2f}")
        logger.info(f"Metrics:")
        for metric, value in health_data['metrics'].items():
            if metric != 'total_components':
                logger.info(f"  - {metric}: {value:.1f}%")
                
        if health_data['issues']:
            logger.info(f"\nIssues Found:")
            for issue_type, issues in issues_by_type.items():
                logger.info(f"\n{issue_type}:")
                for issue in issues:
                    logger.info(f"  - {issue.component} ({issue.severity}):")
                    logger.info(f"    {issue.details}")
                    if issue.recommendations:
                        logger.info("    Recommendations:")
                        for rec in issue.recommendations:
                            logger.info(f"      * {rec}")

def main():
    """Main entry point for code health analysis"""
    try:
        # Initialize and run health analysis
        health_manager = CodeHealthManager()
        report = health_manager.analyze_codebase(str(PROJECT_ROOT))
        
        # Log findings
        if report['issues']:
            logger.info(f"Found {len(report['issues'])} issues:")
            for issue in report['issues']:
                issue_dict = issue.to_dict()
                logger.info(f"- {issue_dict['type']} ({issue_dict['severity']}):")
                logger.info(f"  {issue_dict['details']}")
                
        # Save report to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(PROJECT_ROOT, f"code_health_report_{timestamp}.json")
        
        # Convert report to JSON-serializable format
        serializable_report = {
            'timestamp': report.get('timestamp'),
            'health_score': report.get('health_score'),
            'metrics': report.get('metrics', {}),
            'issues': [issue.to_dict() for issue in report['issues']]
        }
        
        with open(report_path, 'w') as f:
            json.dump(serializable_report, f, indent=2)
            
    except Exception as e:
        logger.error(f"Error during health analysis: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\cleanup\code_health_manager.py</data>
</node>
<node id="kg_config">
  <data key="d0">module</data>
  <data key="d1">"""
Knowledge Graph Configuration
"""

from pathlib import Path
import re
from enum import Enum, auto

class DomainGroup(Enum):
    """Domain groups for code organization"""
    QUOTE_MANAGEMENT = auto()
    RATE_CALCULATION = auto()
    STORAGE_MANAGEMENT = auto()
    USER_MANAGEMENT = auto()
    REPORTING = auto()
    CONFIGURATION = auto()
    UTILITIES = auto()

class InterfaceType(Enum):
    """Types of interfaces in the system"""
    DATA_MODEL = auto()
    API_CONTRACT = auto()
    SERVICE_INTERFACE = auto()
    EVENT_HANDLER = auto()
    NONE = auto()

# Interface definitions and their expected methods
INTERFACE_DEFINITIONS = {
    "DataModel": {
        "required_methods": ["to_dict", "from_dict"],
        "type": InterfaceType.DATA_MODEL
    },
    "APIContract": {
        "required_methods": ["validate"],
        "type": InterfaceType.API_CONTRACT
    },
    "ServiceInterface": {
        "required_methods": ["execute"],
        "type": InterfaceType.SERVICE_INTERFACE
    },
    "EventHandler": {
        "required_methods": ["handle_event"],
        "type": InterfaceType.EVENT_HANDLER
    }
}

# Project root directory
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent

# Directories to include in knowledge graph
INCLUDED_DIRS = [
    PROJECT_ROOT / "warehouse_quote_app/app",  # Core backend only
    PROJECT_ROOT / "frontend/src",             # Core frontend only
    PROJECT_ROOT / "config",                   # Essential configs
    PROJECT_ROOT / "shared/types",             # Shared type definitions
]

# File patterns to include
INCLUDED_PATTERNS = [
    "*.py",      # Python backend
    "*.tsx",     # React TypeScript components
    "*.ts",      # TypeScript utilities
    "*.json",    # Essential configs
]

# Directories and patterns to exclude
EXCLUDED_DIRS = [
    # Environment and build directories
    "**/.venv/**", 
    "**/node_modules/**",
    "**/build/**", 
    "**/dist/**",
    "**/.next/**",
    "**/__pycache__/**",
    "**/*.egg-info/**",
    "**/vendor/**",
    
    # Version control
    "**/.git/**",
    "**/.github/**",
    
    # Cache directories
    "**/.pytest_cache/**",
    "**/.mypy_cache/**",
    "**/logs/**",
    "**/.cache/**",
    
    # Test directories
    "**/test/**",
    "**/tests/**",
    
    # Documentation
    "**/docs/**",
    
    # Tool directories (except kg)
    "**/tools/cleanup/**",
    "**/tools/migrations/**"
]

# File patterns to exclude
EXCLUDED_PATTERNS = [
    # Generated and minified files
    "*.min.js",
    "*.chunk.js",
    "*.bundle.js",
    "*.generated.*",
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "*.so",
    "*.egg",
    
    # Configuration files
    "*.config.js",
    "tsconfig.json",
    "package.json",
    "package-lock.json",
    "yarn.lock",
    
    # Test files
    "*_test.py",
    "*.test.js",
    "*.test.ts",
    "*.test.tsx",
    "*.spec.js",
    "*.spec.ts",
    "*.spec.tsx",
    
    # Documentation
    "README.md",
    "*.md"
]

# Semantic grouping rules
SEMANTIC_GROUPS = {
    "QUOTE_MANAGEMENT": {
        "patterns": [
            r".*quote.*",
            r".*pricing.*",
            r".*estimate.*",
        ],
        "related_domains": [
            "RATE_CALCULATION",
            "BILLING",
        ]
    },
    "RATE_CALCULATION": {
        "patterns": [
            r".*rate.*",
            r".*calculator.*",
            r".*pricing.*",
        ],
        "related_domains": [
            "QUOTE_MANAGEMENT",
            "STORAGE_MANAGEMENT",
        ]
    },
    # Add other semantic groups...
}

# Code health rules
CODE_HEALTH_RULES = {
    "duplication": {
        "max_similarity": 0.85,  # Maximum allowed similarity between components
        "min_lines": 10,  # Minimum lines for duplication check
        "exclude_patterns": [
            r".*test.*",  # Exclude test files from duplication checks
        ]
    },
    "divergence": {
        "interface_drift_threshold": 0.3,  # Maximum allowed drift from interface
        "check_patterns": [
            "*/interfaces/*",
            "*/contracts/*",
            "*/schemas/*",
        ]
    },
    "orphan": {
        "min_references": 1,  # Minimum number of references required
        "exclude_patterns": [
            r".*index\.[tj]sx?$",  # Exclude index files
            r".*\.test\.[tj]sx?$",  # Exclude test files
        ]
    }
}

# High-priority paths focusing on interfaces
HIGH_PRIORITY_PATHS = [
    # API Contracts
    "warehouse_quote_app/app/schemas",
    "warehouse_quote_app/app/services/interfaces",
    # Core Domain Models
    "warehouse_quote_app/app/models",
    # Event Handlers
    "warehouse_quote_app/app/events",
]

# Relationship types with weights
RELATIONSHIP_TYPES = {
    "implements_interface": {
        "weight": 2.0,
        "description": "Implementation of an interface"
    },
    "extends_interface": {
        "weight": 1.8,
        "description": "Extension of an interface"
    },
    "api_contract": {
        "weight": 2.0,
        "description": "API contract definition/usage"
    },
    "event_handler": {
        "weight": 1.5,
        "description": "Event handling relationship"
    },
    "data_flow": {
        "weight": 1.3,
        "description": "Data flow between components"
    },
    "semantic_relation": {
        "weight": 1.2,
        "description": "Related by domain semantics"
    }
}

def get_domain_group(path: str) -&gt; str:
    """Determine the domain group for a given path based on semantic rules"""
    for domain, rules in SEMANTIC_GROUPS.items():
        for pattern in rules["patterns"]:
            if re.search(pattern, path, re.IGNORECASE):
                return domain
    return None

def get_interface_type(path: str) -&gt; str:
    """Determine the interface type for a given path"""
    if "/schemas/" in path or "/types/" in path:
        return "DATA_MODEL"
    elif "/api/" in path:
        return "API_CONTRACT"
    elif "/services/interfaces" in path:
        return "SERVICE_INTERFACE"
    elif "/events/" in path:
        return "EVENT_HANDLER"
    return None

def get_full_path(relative_path: str) -&gt; Path:
    """Convert relative path to full path from project root"""
    return PROJECT_ROOT / relative_path

def get_high_priority_paths() -&gt; list:
    """Get list of high priority paths as Path objects"""
    return [get_full_path(p) for p in HIGH_PRIORITY_PATHS]

def should_exclude_path(path: str) -&gt; bool:
    """Check if a path should be excluded based on configured patterns"""
    from pathlib import Path
    from fnmatch import fnmatch
    
    path_obj = Path(path)
    path_str = str(path_obj).replace('\\', '/')  # Normalize path separators
    
    # Check excluded directories
    for excluded_dir in EXCLUDED_DIRS:
        if fnmatch(path_str, excluded_dir):
            return True
            
    # Check excluded patterns
    for pattern in EXCLUDED_PATTERNS:
        if fnmatch(path_str, pattern):
            return True
            
    return False

def get_relationship_config() -&gt; dict:
    """Get relationship type configuration"""
    return RELATIONSHIP_TYPES

def get_code_health_rules() -&gt; dict:
    """Get code health rules configuration"""
    return CODE_HEALTH_RULES
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\kg\config\kg_config.py</data>
</node>
<node id="dashboard">
  <data key="d0">module</data>
  <data key="d1">"""
Knowledge Graph Visualization Dashboard
Provides interactive visualization and analysis of the code knowledge graph
"""

import streamlit as st
import networkx as nx
import plotly.graph_objects as go
from pathlib import Path
import sys
import json
from typing import Dict, List, Any
import pandas as pd

# Add tools directory to path
tools_dir = Path(__file__).parent.parent
sys.path.append(str(tools_dir))

from db.graph_db import GraphDatabaseManager

def create_graph_visualization(graph: nx.DiGraph) -&gt; go.Figure:
    """Create an interactive graph visualization using Plotly"""
    # Create edge traces
    edge_x = []
    edge_y = []
    edge_text = []
    
    pos = nx.spring_layout(graph)
    
    for edge in graph.edges(data=True):
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        edge_text.append(edge[2].get('edge_type', 'unknown'))
        
    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color='#888'),
        hoverinfo='text',
        text=edge_text,
        mode='lines'
    )
    
    # Create node traces
    node_x = []
    node_y = []
    node_text = []
    node_color = []
    
    for node, data in graph.nodes(data=True):
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(f"Name: {node}&lt;br&gt;Type: {data.get('node_type', 'unknown')}")
        # Color by node type
        if data.get('node_type') == 'class':
            node_color.append('#1f77b4')  # blue
        elif data.get('node_type') == 'function':
            node_color.append('#2ca02c')  # green
        else:
            node_color.append('#d62728')  # red
            
    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers',
        hoverinfo='text',
        text=node_text,
        marker=dict(
            size=10,
            color=node_color,
            line_width=2
        )
    )
    
    # Create the figure
    fig = go.Figure(
        data=[edge_trace, node_trace],
        layout=go.Layout(
            title='Code Knowledge Graph',
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20,l=5,r=5,t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
        )
    )
    
    return fig

def display_statistics(stats: Dict[str, Any]):
    """Display database statistics"""
    st.subheader("Knowledge Graph Statistics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Total Nodes", stats['node_count'])
        st.metric("Total Relationships", stats['relationship_count'])
        
    with col2:
        st.subheader("Node Types")
        for node_type, count in stats['node_types'].items():
            st.metric(node_type or 'unknown', count)
            
    st.subheader("Relationship Types")
    for rel_type, count in stats['relationship_types'].items():
        st.metric(rel_type or 'unknown', count)
        
def display_dead_code(dead_code: List[str]):
    """Display dead code analysis"""
    st.subheader("Dead Code Analysis")
    
    if dead_code:
        st.warning(f"Found {len(dead_code)} potentially dead code nodes")
        for node in dead_code:
            st.code(node, language="python")
    else:
        st.success("No dead code detected")
        
def display_duplicates(duplicates: List[Dict[str, Any]]):
    """Display code duplication analysis"""
    st.subheader("Code Duplication Analysis")
    
    if duplicates:
        st.warning(f"Found {len(duplicates)} potential code duplicates")
        df = pd.DataFrame(duplicates)
        st.dataframe(df)
    else:
        st.success("No code duplicates detected")
        
def main():
    """Main dashboard application"""
    st.set_page_config(
        page_title="Code Knowledge Graph Dashboard",
        page_icon="🧊",
        layout="wide"
    )
    
    st.title("Code Knowledge Graph Dashboard")
    
    # Initialize database connection
    db = GraphDatabaseManager()
    
    try:
        # Create a sample graph for testing if no graph exists
        if not Path("code_knowledge_graph.graphml").exists():
            graph = nx.DiGraph()
            graph.add_node("main", node_type="function")
            graph.add_node("helper", node_type="function")
            graph.add_edge("main", "helper", edge_type="CALLS")
            nx.write_graphml(graph, "code_knowledge_graph.graphml")
        
        # Load the graph
        graph = nx.read_graphml("code_knowledge_graph.graphml")
        
        # Import to database
        db.import_networkx_graph(graph)
        
        # Get statistics
        stats = db.get_statistics()
        display_statistics(stats)
        
        # Create tabs for different views
        tab1, tab2, tab3 = st.tabs(["Graph Visualization", "Dead Code", "Duplicates"])
        
        with tab1:
            st.subheader("Knowledge Graph Visualization")
            fig = create_graph_visualization(graph)
            st.plotly_chart(fig, use_container_width=True)
            
        with tab2:
            dead_code = db.find_dead_code()
            display_dead_code(dead_code)
            
        with tab3:
            duplicates = db.find_duplicates()
            display_duplicates(duplicates)
            
    except Exception as e:
        st.error(f"Error: {e}")
        
    finally:
        db.close()

if __name__ == "__main__":
    main()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\tools\visualization\dashboard.py</data>
</node>
<node id="main">
  <data key="d0">module</data>
  <data key="d1">"""
AU Logistics Warehouse Quote System - Main Application

This is the main entry point for the FastAPI application that handles warehouse quotes.
The system provides functionality for both administrators and customers to manage
warehouse quotes, rate cards, and customer information.

Key Components:
- Authentication &amp; Authorization
- Admin Dashboard
- Customer Portal
- Quote Management
- Rate Card System
- Real-time Features:
  - WebSocket Communication
  - Live Updates
  - Chat System
- AI-Powered Features:
  - FLAN-T5 Language Model
  - RAG (Retrieval Augmented Generation)
  - Intelligent Rate Calculation
  - Customer Service Assistance
"""

from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path

from warehouse_quote_app.app.core.config import settings
from warehouse_quote_app.app.api.v1.api import api_router
from warehouse_quote_app.app.core.monitoring import setup_monitoring
from warehouse_quote_app.app.database import sessionmanager

# Create FastAPI app
app = FastAPI(
    title=settings.PROJECT_NAME,
    description=__doc__,
    version="1.0.0",
    openapi_url=f"{settings.API_V1_STR}/openapi.json"
)

# Set up CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.BACKEND_CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API routes
app.include_router(api_router, prefix=settings.API_V1_STR)

# Serve static files for React app
frontend_path = Path(__file__).parent.parent / "frontend" / "dist"
if frontend_path.exists():
    app.mount("/assets", StaticFiles(directory=str(frontend_path / "assets")), name="static")

    @app.get("/{full_path:path}")
    async def serve_react_app(full_path: str):
        """Serve React app for any non-API route."""
        if full_path.startswith("api/"):
            return {"detail": "Not Found"}
        return FileResponse(str(frontend_path / "index.html"))

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize services on startup."""
    # Initialize database
    from warehouse_quote_app.app.database import init_db
    await init_db()

    # Initialize real-time service
    from warehouse_quote_app.app.services.realtime_service import RealTimeService
    app.state.realtime_service = RealTimeService()

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown."""
    # Close database connections
    from warehouse_quote_app.app.database import close_db
    await close_db()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\main.py</data>
</node>
<node id="deps">
  <data key="d0">module</data>
  <data key="d1">"""Dependencies for API endpoints."""
from typing import Generator
from sqlalchemy.orm import Session
from warehouse_quote_app.app.database import SessionLocal

def get_db() -&gt; Generator[Session, None, None]:
    """Get database session.
    
    Yields:
        Session: SQLAlchemy database session
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\deps.py</data>
</node>
<node id="api">
  <data key="d0">module</data>
  <data key="d1">"""API router configuration."""
from fastapi import APIRouter
from app.api.v1.endpoints import (
    admin,
    auth,
    client,
    customer,
    quotes,
    rate_cards
)
from app.api.v1 import ws

# Create main API router
api_router = APIRouter()

# Authentication and user management
api_router.include_router(auth.router, prefix="/auth", tags=["auth"])
api_router.include_router(admin.router, prefix="/admin", tags=["admin"])
api_router.include_router(customer.router, prefix="/customer", tags=["customer"])

# Business operations
api_router.include_router(client.router, prefix="/client", tags=["client"])
api_router.include_router(quotes.router, prefix="/quotes", tags=["quotes"])
api_router.include_router(rate_cards.router, prefix="/rate-cards", tags=["rate-cards"])

# WebSocket endpoints
api_router.include_router(ws.router, prefix="/ws", tags=["websocket"])
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\api.py</data>
</node>
<node id="chat">
  <data key="d0">module</data>
  <data key="d1">"""
API routes for quote generation chat functionality.
"""

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Dict, Any
from decimal import Decimal

from app.database import get_db
from app.schemas.conversation import (
    ChatSession,
    MessageCreate,
    MessageResponse,
    QuoteUpdate
)
from app.services.llm.chat_service import ChatService

router = APIRouter(prefix="/api/v1/chat", tags=["chat"])
chat_service = ChatService()

@router.post("/session")
async def create_session(
    db: Session = Depends(get_db)
) -&gt; ChatSession:
    """Create a new chat session for quote generation."""
    try:
        session_id = await chat_service.create_session(db.user_id, db)
        return chat_service.get_session_context(session_id)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create chat session: {str(e)}"
        )

@router.post("/message")
async def send_message(
    message: MessageCreate,
    db: Session = Depends(get_db)
) -&gt; MessageResponse:
    """Process a chat message and return AI response."""
    try:
        response = await chat_service.send_message(
            session_id=message.session_id,
            content=message.content,
            db=db
        )
        return MessageResponse(**response)
    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process message: {str(e)}"
        )

@router.post("/quote/update")
async def update_quote(
    update: QuoteUpdate,
    db: Session = Depends(get_db)
) -&gt; Dict[str, Any]:
    """Update an existing quote based on conversation."""
    try:
        return await chat_service.update_quote(
            session_id=update.session_id,
            updates=update.updates,
            db=db
        )
    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update quote: {str(e)}"
        )

@router.post("/quote/discount")
async def apply_discount(
    session_id: str,
    service_id: str,
    discount_percentage: Decimal,
    db: Session = Depends(get_db)
) -&gt; Dict[str, bool]:
    """Apply a discount to a specific service (max 10%)."""
    try:
        success = await chat_service.apply_discount(
            session_id=session_id,
            service_id=service_id,
            discount_percentage=discount_percentage,
            db=db
        )
        return {"applied": success}
    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to apply discount: {str(e)}"
        )

@router.get("/session/{session_id}")
async def get_session(
    session_id: str,
    db: Session = Depends(get_db)
) -&gt; Dict[str, Any]:
    """Get the current session context and memory."""
    try:
        return chat_service.get_session_context(session_id)
    except ValueError as e:
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get session: {str(e)}"
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\chat.py</data>
</node>
<node id="middleware">
  <data key="d0">module</data>
  <data key="d1">"""API middleware and dependencies."""
from typing import Callable, Optional
from fastapi import Request, Response
from fastapi.responses import JSONResponse
from fastapi_cache import FastAPICache
from fastapi_cache.decorator import cache
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter
import redis.asyncio as redis
from app.core.config import settings
from app.core.logging import get_logger

logger = get_logger(__name__)

# Rate limiting configuration
RATE_LIMIT_STANDARD = "100/minute"  # Standard API endpoints
RATE_LIMIT_AUTH = "5/minute"        # Authentication endpoints
RATE_LIMIT_ADMIN = "200/minute"     # Admin endpoints

# Cache configuration
CACHE_EXPIRE_TIME = 60  # seconds
CACHE_KEY_PREFIX = "aulogistics_"

async def init_redis():
    """Initialize Redis for rate limiting and caching."""
    redis_instance = redis.from_url(settings.REDIS_URL)
    await FastAPILimiter.init(redis_instance)
    FastAPICache.init(
        backend=redis_instance,
        prefix=CACHE_KEY_PREFIX,
        expire=CACHE_EXPIRE_TIME
    )

def rate_limit(limit: str = RATE_LIMIT_STANDARD) -&gt; Callable:
    """Rate limiting decorator with configurable limits."""
    return RateLimiter(times=int(limit.split('/')[0]),
                      seconds=60 if limit.split('/')[1] == 'minute' else 1)

def cache_response(expire: int = CACHE_EXPIRE_TIME,
                  key_prefix: Optional[str] = None) -&gt; Callable:
    """Cache decorator with configurable expiration."""
    def decorator(func: Callable) -&gt; Callable:
        prefix = key_prefix or func.__name__
        return cache(expire=expire, namespace=f"{CACHE_KEY_PREFIX}{prefix}")(func)
    return decorator

async def handle_rate_limit_exceeded(request: Request, exc: Exception) -&gt; Response:
    """Handle rate limit exceeded exceptions."""
    logger.warning(f"Rate limit exceeded for {request.url}")
    return JSONResponse(
        status_code=429,
        content={
            "detail": "Too many requests. Please try again later.",
            "type": "rate_limit_exceeded"
        }
    )

# Export dependencies
__all__ = [
    'init_redis',
    'rate_limit',
    'cache_response',
    'handle_rate_limit_exceeded',
    'RATE_LIMIT_STANDARD',
    'RATE_LIMIT_AUTH',
    'RATE_LIMIT_ADMIN'
]
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\middleware.py</data>
</node>
<node id="mixins">
  <data key="d0">module</data>
  <data key="d1">"""Repository mixins for common database operations."""

from typing import List, Optional, Dict, Any, TypeVar, Generic, Type
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import select, func

from app.models.base import BaseModel

ModelType = TypeVar("ModelType", bound=BaseModel)

class FilterMixin(Generic[ModelType]):
    """Mixin for common filtering operations."""

    model: Type[ModelType]

    async def get_by_field(
        self,
        db: Session,
        field: str,
        value: Any,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[ModelType]:
        """Get records by field value."""
        return (
            db.query(self.model)
            .filter(getattr(self.model, field) == value)
            .offset(skip)
            .limit(limit)
            .all()
        )

    async def get_by_date_range(
        self,
        db: Session,
        date_field: str,
        start_date: datetime,
        end_date: datetime,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[ModelType]:
        """Get records within a date range."""
        return (
            db.query(self.model)
            .filter(
                getattr(self.model, date_field) &gt;= start_date,
                getattr(self.model, date_field) &lt;= end_date
            )
            .offset(skip)
            .limit(limit)
            .all()
        )

class AggregationMixin(Generic[ModelType]):
    """Mixin for common aggregation operations."""

    model: Type[ModelType]

    async def count_by_field(
        self,
        db: Session,
        field: str,
        value: Any
    ) -&gt; int:
        """Count records by field value."""
        return (
            db.query(func.count())
            .filter(getattr(self.model, field) == value)
            .scalar()
        )

    async def get_field_stats(
        self,
        db: Session,
        field: str
    ) -&gt; Dict[str, Any]:
        """Get statistics for a numeric field."""
        result = (
            db.query(
                func.avg(getattr(self.model, field)).label("avg"),
                func.min(getattr(self.model, field)).label("min"),
                func.max(getattr(self.model, field)).label("max"),
                func.count(getattr(self.model, field)).label("count")
            )
            .first()
        )
        return {
            "average": float(result.avg) if result.avg else 0,
            "minimum": float(result.min) if result.min else 0,
            "maximum": float(result.max) if result.max else 0,
            "count": result.count
        }

class PaginationMixin(Generic[ModelType]):
    """Mixin for pagination operations."""

    model: Type[ModelType]

    async def get_paginated(
        self,
        db: Session,
        skip: int = 0,
        limit: int = 100,
        order_by: Optional[str] = None,
        descending: bool = False
    ) -&gt; List[ModelType]:
        """Get paginated records."""
        query = db.query(self.model)
        
        if order_by:
            order_field = getattr(self.model, order_by)
            if descending:
                order_field = order_field.desc()
            query = query.order_by(order_field)
            
        return query.offset(skip).limit(limit).all()

    async def get_total_count(self, db: Session) -&gt; int:
        """Get total count of records."""
        return db.query(func.count(self.model.id)).scalar()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\repositories\mixins.py</data>
</node>
<node id="quotes">
  <data key="d0">module</data>
  <data key="d1">"""
Quote generation and management service.
"""

from typing import List, Optional, Dict, Any
from sqlalchemy.orm import Session
from datetime import datetime
from decimal import Decimal

from app.core.monitoring import log_event
from app.models.quote import Quote
from app.schemas.quote import (
    QuoteCreate,
    QuoteUpdate,
    MultiServiceQuoteRequest,
    BulkQuoteRequest,
    BulkQuoteResponse
)
from app.repositories.quote import QuoteRepository
from app.services.base import BaseService
from app.services.llm.rate_integration import RateOptimizer
from app.services.business.storage import StorageService
from app.services.business.three_pl import ThreePLService
from app.services.business.transport import TransportService

class QuoteService(BaseService[Quote, QuoteCreate, QuoteUpdate]):
    """Service for managing quotes."""

    def __init__(self, db: Session):
        """Initialize with Quote model."""
        super().__init__(Quote, db)
        self.rate_optimizer = RateOptimizer()
        self.storage_service = StorageService()
        self.three_pl_service = ThreePLService()
        self.transport_service = TransportService()
        self.repository = QuoteRepository(Quote)

    async def calculate_quote(
        self,
        request: MultiServiceQuoteRequest,
        db: Session
    ) -&gt; Dict[str, Any]:
        """Calculate quote for multiple services."""
        # Calculate costs for each service type
        costs = {}
        total_cost = Decimal('0')

        # Storage costs
        if request.storage_requirements:
            storage_cost = await self.storage_service.calculate_storage_cost(
                request.storage_requirements,
                db
            )
            costs['storage'] = storage_cost
            total_cost += storage_cost

            await self.storage_service.log_storage_calculation(
                db,
                request.storage_requirements,
                storage_cost,
                request.user_id
            )

        # 3PL costs
        if request.three_pl_services:
            three_pl_cost = await self.three_pl_service.calculate_3pl_cost(
                request.three_pl_services,
                db
            )
            costs['3pl'] = three_pl_cost
            total_cost += three_pl_cost

            await self.three_pl_service.log_3pl_calculation(
                db,
                request.three_pl_services,
                three_pl_cost,
                request.user_id
            )

        # Transport costs
        if request.transport_services:
            transport_cost = await self.transport_service.calculate_transport_cost(
                request.transport_services,
                db
            )
            costs['transport'] = transport_cost
            total_cost += transport_cost

            await self.transport_service.log_transport_calculation(
                db,
                request.transport_services,
                transport_cost,
                request.user_id
            )

        # Use rate optimizer to get best rates
        optimized_rates = await self.rate_optimizer.optimize_rates(request)
        
        # Create quote in database
        quote = await self.create_quote_from_costs(
            db=db,
            costs=costs,
            total_cost=total_cost,
            optimized_rates=optimized_rates,
            request=request
        )
        
        # Log quote creation
        log_event(
            db=db,
            event_type="quote_created",
            user_id=request.user_id,
            resource_type="quote",
            resource_id=str(quote.id),
            details={
                "services": request.services,
                "costs": {k: str(v) for k, v in costs.items()},
                "total_cost": str(total_cost)
            }
        )
        
        return quote

    async def calculate_bulk_quotes(
        self,
        request: BulkQuoteRequest,
        db: Session
    ) -&gt; BulkQuoteResponse:
        """Calculate multiple quotes in bulk."""
        quotes = []
        for quote_request in request.quotes:
            quote = await self.calculate_quote(quote_request, db)
            quotes.append(quote)
        
        return BulkQuoteResponse(quotes=quotes)

    async def compare_service_combinations(
        self,
        request: MultiServiceQuoteRequest,
        db: Session
    ) -&gt; List[Dict[str, Any]]:
        """Compare different combinations of services."""
        combinations = self.rate_optimizer.generate_service_combinations(request)
        
        quotes = []
        for combo in combinations:
            modified_request = request.copy(update={"services": combo})
            quote = await self.calculate_quote(modified_request, db)
            quotes.append(quote)
        
        return quotes

    async def create_quote_from_costs(
        self,
        db: Session,
        costs: Dict[str, Decimal],
        total_cost: Decimal,
        optimized_rates: Dict[str, Any],
        request: MultiServiceQuoteRequest
    ) -&gt; Quote:
        """Create a quote record from calculated costs and rates."""
        quote_data = QuoteCreate(
            user_id=request.user_id,
            customer_id=request.customer_id,
            services=request.services,
            costs=costs,
            rates=optimized_rates,
            total_amount=total_cost,
            valid_until=datetime.utcnow().date(),
            status="draft"
        )
        
        quote = await self.repository.create(db, obj_in=quote_data)
        return quote

    async def finalize_quote(
        self,
        db: Session,
        quote_id: int,
        user_id: int
    ) -&gt; Quote:
        """Finalize a quote."""
        quote = await self.repository.get(db, quote_id)
        if not quote:
            raise ValueError("Quote not found")
        
        quote_update = QuoteUpdate(
            status="final",
            finalized_at=datetime.utcnow(),
            finalized_by=user_id
        )
        
        updated_quote = await self.repository.update(db, db_obj=quote, obj_in=quote_update)
        
        log_event(
            db=db,
            event_type="quote_finalized",
            user_id=user_id,
            resource_type="quote",
            resource_id=str(quote_id),
            details={"total_amount": updated_quote.total_amount}
        )
        
        return updated_quote
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\business\quotes.py</data>
</node>
<node id="users">
  <data key="d0">module</data>
  <data key="d1">"""
User management API routes.
"""

from typing import List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.core.auth import get_current_user, get_current_admin_user
from app.models.user import User
from app.services.user_service import UserService
from app.schemas.user import (
    User as UserSchema,
    UserCreate,
    UserUpdate
)
from app.schemas.user.customer import (
    Customer as CustomerSchema,
    CustomerCreate,
    CustomerUpdate
)

router = APIRouter(prefix="/users", tags=["users"])

# User Management Routes
@router.post("", response_model=UserSchema)
async def create_user(
    user_data: UserCreate,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Create new user. Admin only."""
    return await user_service.create_user(user_data)

@router.get("/me", response_model=UserSchema)
async def get_current_user_info(
    current_user: User = Depends(get_current_user)
):
    """Get current user info."""
    return current_user

@router.put("/me", response_model=UserSchema)
async def update_current_user(
    user_data: UserUpdate,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_user)
):
    """Update current user."""
    return await user_service.update_user(current_user.id, user_data)

@router.get("", response_model=List[UserSchema])
async def list_users(
    skip: int = 0,
    limit: int = 100,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """List all users. Admin only."""
    return await user_service.get_users(skip=skip, limit=limit)

@router.get("/{user_id}", response_model=UserSchema)
async def get_user(
    user_id: int,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Get user by ID. Admin only."""
    return await user_service.get_user(user_id)

@router.put("/{user_id}", response_model=UserSchema)
async def update_user(
    user_id: int,
    user_data: UserUpdate,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Update user. Admin only."""
    return await user_service.update_user(user_id, user_data)

@router.delete("/{user_id}")
async def delete_user(
    user_id: int,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Delete user. Admin only."""
    await user_service.delete_user(user_id)
    return {"message": "User deleted"}

# Customer Management Routes
@router.post("/customers", response_model=CustomerSchema)
async def create_customer(
    customer_data: CustomerCreate,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Create new customer. Admin only."""
    return await user_service.create_customer(customer_data)

@router.get("/customers", response_model=List[CustomerSchema])
async def list_customers(
    skip: int = 0,
    limit: int = 100,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_user)
):
    """List all customers."""
    return await user_service.get_customers(skip=skip, limit=limit)

@router.get("/customers/{customer_id}", response_model=CustomerSchema)
async def get_customer(
    customer_id: int,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_user)
):
    """Get customer by ID."""
    return await user_service.get_customer(customer_id)

@router.put("/customers/{customer_id}", response_model=CustomerSchema)
async def update_customer(
    customer_id: int,
    customer_data: CustomerUpdate,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Update customer. Admin only."""
    return await user_service.update_customer(customer_id, customer_data)

@router.delete("/customers/{customer_id}")
async def delete_customer(
    customer_id: int,
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_admin_user)
):
    """Delete customer. Admin only."""
    await user_service.delete_customer(customer_id)
    return {"message": "Customer deleted"}

# User Preferences Routes
@router.get("/me/preferences", response_model=Dict[str, Any])
async def get_user_preferences(
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_user)
):
    """Get current user preferences."""
    return await user_service.get_user_preferences(current_user.id)

@router.put("/me/preferences", response_model=UserSchema)
async def update_user_preferences(
    preferences: Dict[str, Any],
    user_service: UserService = Depends(),
    current_user: User = Depends(get_current_user)
):
    """Update current user preferences."""
    return await user_service.update_user_preferences(current_user.id, preferences)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\users.py</data>
</node>
<node id="ws">
  <data key="d0">module</data>
  <data key="d1">"""
WebSocket routes for real-time communication.
"""

from fastapi import APIRouter, WebSocket, Depends, HTTPException
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.core.auth import get_current_user
from app.models.user import User
from app.services.realtime_service import RealTimeService

router = APIRouter(prefix="/ws", tags=["websocket"])

@router.websocket("/")
async def websocket_endpoint(
    websocket: WebSocket,
    token: str,
    db: Session = Depends(get_db)
):
    """WebSocket endpoint for real-time communication."""
    try:
        # Get user from token
        user = await get_current_user(token, db)
        
        # Initialize real-time service
        realtime_service = RealTimeService(db)
        
        # Handle WebSocket connection
        await realtime_service.handle_websocket(websocket, user)
        
    except HTTPException:
        await websocket.close(code=1008)  # Policy Violation
    except Exception:
        await websocket.close(code=1011)  # Internal Error
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\ws.py</data>
</node>
<node id="admin">
  <data key="d0">module</data>
  <data key="d1">"""
Admin-related schema definitions.
"""

from datetime import datetime
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, ConfigDict, Field

from .quote import QuoteStatus
from .reports.quote_report import QuoteReport
from .reports.service_report import ServiceUsageReport
from .reports.customer_report import CustomerActivityReport

class AdminErrorResponse(BaseModel):
    """Standard error response model for admin routes."""
    model_config = ConfigDict(extra='forbid')
    
    detail: str

class AdminMetricsResponse(BaseModel):
    """Response model for admin metrics."""
    model_config = ConfigDict(extra='forbid')
    
    total_customers: int = Field(..., description="Total number of customers")
    active_customers: int = Field(..., description="Number of active customers")
    new_customers_30d: int = Field(..., description="Number of new customers in last 30 days")
    total_quotes: int = Field(..., description="Total number of quotes")
    pending_quotes: int = Field(..., description="Number of pending quotes")
    completed_quotes: int = Field(..., description="Number of completed quotes")
    total_revenue: float = Field(..., description="Total revenue from completed quotes")
    revenue_30d: float = Field(..., description="Revenue from completed quotes in last 30 days")

class CustomerResponse(BaseModel):
    """Response model for customer data."""
    model_config = ConfigDict(extra='forbid')
    
    id: int
    name: str
    email: str
    company: Optional[str] = None
    is_active: bool
    created_at: datetime
    total_quotes: int = Field(..., description="Total number of quotes by this customer")

class QuoteReport(BaseModel):
    """Response model for quote statistics."""
    model_config = ConfigDict(extra='forbid')
    
    total_quotes: int = Field(..., description="Total number of quotes in period")
    total_value: float = Field(..., description="Total value of quotes in period")
    average_value: float = Field(..., description="Average value of quotes in period")
    status_breakdown: Dict[str, int] = Field(..., description="Number of quotes by status")
    start_date: Optional[datetime] = Field(None, description="Start of reporting period")
    end_date: Optional[datetime] = Field(None, description="End of reporting period")

class ServiceUsageReport(BaseModel):
    """Response model for service usage statistics."""
    model_config = ConfigDict(extra='forbid')
    
    service_usage: Dict[str, int] = Field(..., description="Count of quotes by service type")
    total_quotes: int = Field(..., description="Total number of quotes in period")
    start_date: Optional[datetime] = Field(None, description="Start of reporting period")
    end_date: Optional[datetime] = Field(None, description="End of reporting period")

class CustomerActivityReport(BaseModel):
    """Response model for customer activity statistics."""
    model_config = ConfigDict(extra='forbid')
    
    customer_id: int = Field(..., description="ID of the customer")
    customer_name: str = Field(..., description="Name of the customer")
    total_quotes: int = Field(..., description="Total number of quotes")
    total_value: float = Field(..., description="Total value of quotes")
    quote_status: Dict[str, int] = Field(..., description="Number of quotes by status")
    start_date: Optional[datetime] = Field(None, description="Start of reporting period")
    end_date: Optional[datetime] = Field(None, description="End of reporting period")
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\admin.py</data>
</node>
<node id="auth">
  <data key="d0">module</data>
  <data key="d1">"""
Authentication schemas for the Warehouse Quote System.

This module defines Pydantic models for:
1. Authentication tokens
2. User data
3. User creation and updates
"""

from datetime import datetime
from typing import Optional
from pydantic import BaseModel, EmailStr, Field

from .user import (
    UserBase,
    UserCreate,
    UserUpdate,
    UserResponse,
    UserBaseSimple,
    UserCreateSimple,
    UserResponseSimple
)

class Token(BaseModel):
    """JWT token schema."""
    access_token: str
    token_type: str = "bearer"

class TokenData(BaseModel):
    """JWT token payload data."""
    email: Optional[str] = None
    exp: Optional[datetime] = None

class UserLogin(BaseModel):
    """Schema for user login credentials."""
    email: EmailStr
    password: str = Field(..., min_length=8, description="User password, minimum 8 characters")

class UserRegister(UserCreate):
    """Schema for user registration."""
    password_confirm: str = Field(..., min_length=8)

    def validate_passwords_match(self) -&gt; None:
        """Validate that passwords match."""
        if self.password != self.password_confirm:
            raise ValueError("Passwords do not match")

__all__ = [
    'Token',
    'TokenData',
    'UserLogin',
    'UserRegister',
    'UserBase',
    'UserCreate',
    'UserUpdate',
    'UserResponse',
    'UserBaseSimple',
    'UserCreateSimple',
    'UserResponseSimple'
]
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\user\auth.py</data>
</node>
<node id="client">
  <data key="d0">module</data>
  <data key="d1">"""Client endpoints."""
from datetime import datetime
from typing import List, Optional
from fastapi import APIRouter, Depends, Query, Path, HTTPException, status

from app.core.auth import get_current_client_user
from app.models.user import User
from app.models.quote import QuoteStatus
from app.schemas.client import (
    ClientProfile,
    ClientQuoteResponse,
    ClientServiceResponse,
    SupportTicketCreate,
    SupportTicketResponse
)
from app.services.client import get_client_service, ClientService
from app.core.logging import get_logger

router = APIRouter()
logger = get_logger("client")

@router.get(
    "/client/me",
    response_model=ClientProfile,
    dependencies=[Depends(get_current_client_user)]
)
async def get_profile(
    client_service: ClientService = Depends(get_client_service)
):
    """Get current client profile."""
    return await client_service.get_profile()

@router.get(
    "/client/quotes",
    response_model=List[ClientQuoteResponse],
    dependencies=[Depends(get_current_client_user)]
)
async def list_quotes(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    status: Optional[QuoteStatus] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    client_service: ClientService = Depends(get_client_service)
):
    """List client quotes with optional filters."""
    return await client_service.list_quotes(
        skip=skip,
        limit=limit,
        status=status,
        start_date=start_date,
        end_date=end_date
    )

@router.get(
    "/client/quotes/{quote_id}",
    response_model=ClientQuoteResponse,
    dependencies=[Depends(get_current_client_user)]
)
async def get_quote(
    quote_id: int,
    client_service: ClientService = Depends(get_client_service)
):
    """Get specific quote details."""
    return await client_service.get_quote(quote_id)

@router.get(
    "/client/services/active",
    response_model=List[ClientServiceResponse],
    dependencies=[Depends(get_current_client_user)]
)
async def list_active_services(
    client_service: ClientService = Depends(get_client_service)
):
    """List active services for the client."""
    return await client_service.list_active_services()

@router.get(
    "/client/services/{service_id}",
    response_model=ClientServiceResponse,
    dependencies=[Depends(get_current_client_user)]
)
async def get_service(
    service_id: int,
    client_service: ClientService = Depends(get_client_service)
):
    """Get specific service details."""
    return await client_service.get_service(service_id)

@router.post(
    "/client/support/tickets",
    response_model=SupportTicketResponse,
    dependencies=[Depends(get_current_client_user)]
)
async def create_support_ticket(
    ticket: SupportTicketCreate,
    client_service: ClientService = Depends(get_client_service)
):
    """Create a new support ticket."""
    return await client_service.create_support_ticket(ticket)

@router.get(
    "/client/support/tickets",
    response_model=List[SupportTicketResponse],
    dependencies=[Depends(get_current_client_user)]
)
async def list_support_tickets(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    status: Optional[str] = Query(None),
    client_service: ClientService = Depends(get_client_service)
):
    """List support tickets."""
    return await client_service.list_support_tickets(
        skip=skip,
        limit=limit,
        status=status
    )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\endpoints\client.py</data>
</node>
<node id="customer">
  <data key="d0">module</data>
  <data key="d1">"""Customer service module for handling customer-related business logic."""

from typing import Optional, List, Dict, Any
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.session import get_db
from app.models.customer import Customer
from app.repositories.customer import CustomerRepository
from app.schemas.customer import (
    CustomerCreate,
    CustomerUpdate,
    CustomerPreferenceUpdate,
    CustomerResponse,
    CustomerDashboardResponse,
    CustomerListResponse
)

class CustomerService:
    """Service for managing customer operations and business logic."""

    def __init__(
        self,
        db: AsyncSession = Depends(get_db),
        repository: CustomerRepository = Depends()
    ) -&gt; None:
        """Initialize the customer service.
        
        Args:
            db: Async database session
            repository: Customer repository for data access
        """
        self.db = db
        self.repository = repository

    async def create(
        self,
        *,
        customer_in: CustomerCreate
    ) -&gt; CustomerResponse:
        """Create a new customer.
        
        Args:
            customer_in: Customer creation data
            
        Returns:
            CustomerResponse: Created customer data
            
        Raises:
            ValueError: If required fields are missing
        """
        customer = await self.repository.create(self.db, obj_in=customer_in)
        return CustomerResponse.from_orm(customer)

    async def update(
        self,
        *,
        customer_id: int,
        customer_in: CustomerUpdate
    ) -&gt; Optional[CustomerResponse]:
        """Update customer details.
        
        Args:
            customer_id: ID of the customer to update
            customer_in: Customer update data
            
        Returns:
            Optional[CustomerResponse]: Updated customer data or None if not found
        """
        customer = await self.repository.get(self.db, id=customer_id)
        if not customer:
            return None
        updated = await self.repository.update(self.db, db_obj=customer, obj_in=customer_in)
        return CustomerResponse.from_orm(updated)

    async def get(
        self,
        customer_id: int
    ) -&gt; Optional[CustomerResponse]:
        """Get customer by ID.
        
        Args:
            customer_id: ID of the customer to retrieve
            
        Returns:
            Optional[CustomerResponse]: Customer data or None if not found
        """
        customer = await self.repository.get(self.db, id=customer_id)
        if not customer:
            return None
        return CustomerResponse.from_orm(customer)

    async def get_by_email(
        self,
        email: str
    ) -&gt; Optional[Customer]:
        """Get customer by email."""
        return await self.repository.get_by_email(self.db, email)

    async def get_dashboard(
        self,
        customer_id: int
    ) -&gt; Optional[CustomerDashboardResponse]:
        """Get customer dashboard data.
        
        Args:
            customer_id: ID of the customer
            
        Returns:
            Optional[CustomerDashboardResponse]: Dashboard data or None if not found
            
        Note:
            Dashboard includes aggregated metrics and recent activity
        """
        customer = await self.repository.get_with_quotes(self.db, customer_id)
        if not customer:
            return None
        
        # Get dashboard metrics
        metrics = await self.repository.get_customer_metrics(self.db, customer_id)
        return CustomerDashboardResponse(
            id=customer.id,
            total_quotes=metrics.get("total_quotes", 0),
            accepted_quotes=metrics.get("accepted_quotes", 0),
            rejected_quotes=metrics.get("rejected_quotes", 0),
            total_spent=metrics.get("total_spent", 0.0),
            last_quote_date=metrics.get("last_quote_date"),
            recent_quotes=customer.recent_quotes[:5]  # Last 5 quotes
        )

    async def update_preferences(
        self,
        *,
        customer_id: int,
        preferences: CustomerPreferenceUpdate
    ) -&gt; Optional[Customer]:
        """Update customer preferences."""
        customer = await self.repository.get(self.db, id=customer_id)
        if not customer:
            return None
        
        await customer.update_preferences(
            contact_method=preferences.preferred_contact_method,
            notifications=preferences.notification_preferences,
            requirements=preferences.special_requirements
        )
        
        self.db.add(customer)
        await self.db.commit()
        await self.db.refresh(customer)
        return customer

    async def list_active(
        self,
        *,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[Customer]:
        """List active customers."""
        return await self.repository.list_active(self.db, skip=skip, limit=limit)

def get_customer_service(
    db: AsyncSession = Depends(get_db),
    repository: CustomerRepository = Depends()
) -&gt; CustomerService:
    """Dependency injection for customer service.
    
    Args:
        db: Async database session
        repository: Customer repository instance
        
    Returns:
        CustomerService: Configured customer service instance
    """
    return CustomerService(db=db, repository=repository)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\customer.py</data>
</node>
<node id="rate_cards">
  <data key="d0">module</data>
  <data key="d1">"""Rate card and rate management endpoints."""
from typing import List, Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Query, Path
from sqlalchemy.orm import Session

from app.api import deps
from app.core.auth import get_current_admin_user
from app.models.user import User
from app.models.rate_optimization import (
    RateOptimizationHistory,
    RateValidationRule,
    MarketRateAnalysis
)
from app.schemas.rate.rate_card import (
    RateCardCreate,
    RateCardUpdate,
    RateCardResponse,
    RateCardSettingsCreate,
    RateCardSettingsUpdate,
    RateCardSettingsResponse
)
from app.schemas.rate.rate import (
    RateCreate,
    RateUpdate,
    RateResponse,
    RateCategory,
    RateUnit
)
from app.schemas.rate.rate_optimization import (
    OptimizationRequest,
    OptimizationResponse,
    ValidationRule,
    ValidationRuleCreate,
    MarketAnalysis,
    MarketAnalysisCreate
)
from app.services.business.rates import RateService
from app.services.llm.rate_integration import RateIntegrationService
from app.core.logging import get_logger

router = APIRouter(prefix="/rate-cards")
logger = get_logger("rate_cards")

# Rate Card Settings Endpoints
@router.post(
    "/settings",
    response_model=RateCardSettingsResponse,
    status_code=201,
    dependencies=[Depends(get_current_admin_user)]
)
async def create_rate_card_settings(
    settings_in: RateCardSettingsCreate,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """Create new rate card settings."""
    return await rate_service.create_rate_card_settings(settings_in, db)

@router.get(
    "/settings",
    response_model=List[RateCardSettingsResponse],
    dependencies=[Depends(get_current_admin_user)]
)
async def list_rate_card_settings(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """List rate card settings."""
    return await rate_service.list_rate_card_settings(skip, limit, db)

# Rate Card Management Endpoints
@router.post(
    "",
    response_model=RateCardResponse,
    status_code=201,
    dependencies=[Depends(get_current_admin_user)]
)
async def create_rate_card(
    rate_card_in: RateCardCreate,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """Create new rate card."""
    return await rate_service.create_rate_card(rate_card_in, db)

@router.get(
    "",
    response_model=List[RateCardResponse]
)
async def list_rate_cards(
    skip: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=100),
    is_active: Optional[bool] = Query(None),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """List rate cards."""
    return await rate_service.list_rate_cards(skip, limit, is_active, db)

@router.get(
    "/{rate_card_id}",
    response_model=RateCardResponse
)
async def get_rate_card(
    rate_card_id: int = Path(..., gt=0),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """Get rate card by ID."""
    return await rate_service.get_rate_card(rate_card_id, db)

@router.patch(
    "/{rate_card_id}",
    response_model=RateCardResponse,
    dependencies=[Depends(get_current_admin_user)]
)
async def update_rate_card(
    rate_card_id: int = Path(..., gt=0),
    rate_card_in: RateCardUpdate = None,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """Update rate card."""
    return await rate_service.update_rate_card(rate_card_id, rate_card_in, db)

# Rate Management Endpoints
@router.get("/rates", response_model=List[RateResponse])
async def get_rates(
    category: Optional[RateCategory] = None,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db)
):
    """Get all rates or filter by category."""
    return await rate_service.get_rates(category, db)

@router.post("/rates", response_model=RateResponse)
async def create_rate(
    rate: RateCreate,
    rate_service: RateService = Depends(),
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(deps.get_db)
):
    """Create a new rate. Admin only."""
    return await rate_service.create_rate(rate, current_user, db)

@router.put("/rates/{rate_id}", response_model=RateResponse)
async def update_rate(
    rate_id: int,
    rate: RateUpdate,
    rate_service: RateService = Depends(),
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(deps.get_db)
):
    """Update an existing rate. Admin only."""
    return await rate_service.update_rate(rate_id, rate, current_user, db)

@router.delete("/rates/{rate_id}")
async def delete_rate(
    rate_id: int,
    rate_service: RateService = Depends(),
    current_user: User = Depends(get_current_admin_user),
    db: Session = Depends(deps.get_db)
):
    """Delete a rate. Admin only."""
    await rate_service.delete_rate(rate_id, current_user, db)
    return {"message": "Rate deleted"}

# Rate Optimization Endpoints
@router.post("/optimization/{rate_card_id}", response_model=OptimizationResponse)
async def optimize_rate_card(
    rate_card_id: int,
    request: OptimizationRequest,
    rate_service: RateService = Depends(),
    rate_integration_service: RateIntegrationService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; OptimizationResponse:
    """Optimize a rate card using AI analysis."""
    try:
        result = await rate_integration_service.optimize_rate_card({
            "id": rate_card_id,
            **request.dict()
        }, db)
        
        # Create optimization history
        history = RateOptimizationHistory(
            rate_card_id=rate_card_id,
            optimization_type=request.optimization_type,
            original_rates=result["original"],
            optimized_rates=result["optimized"],
            confidence_score=result["metrics"]["confidence_score"],
            metrics=result["metrics"]
        )
        db.add(history)
        db.commit()
        
        return OptimizationResponse(**result)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to optimize rate card: {str(e)}"
        )

@router.get("/optimization/history/{rate_card_id}", response_model=List[OptimizationResponse])
async def get_optimization_history(
    rate_card_id: int,
    limit: int = Query(10, ge=1, le=100),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; List[OptimizationResponse]:
    """Get optimization history for a rate card."""
    history = db.query(RateOptimizationHistory)\
        .filter(RateOptimizationHistory.rate_card_id == rate_card_id)\
        .order_by(RateOptimizationHistory.created_at.desc())\
        .limit(limit)\
        .all()
        
    return [OptimizationResponse(
        original=h.original_rates,
        optimized=h.optimized_rates,
        metrics=h.metrics,
        applied=h.applied,
        created_at=h.created_at,
        applied_at=h.applied_at
    ) for h in history]

@router.post("/optimization/rules", response_model=ValidationRule)
async def create_validation_rule(
    rule: ValidationRuleCreate,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; ValidationRule:
    """Create a new validation rule."""
    db_rule = RateValidationRule(**rule.dict())
    db.add(db_rule)
    db.commit()
    db.refresh(db_rule)
    return ValidationRule.from_orm(db_rule)

@router.get("/optimization/rules", response_model=List[ValidationRule])
async def get_validation_rules(
    active_only: bool = Query(True),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; List[ValidationRule]:
    """Get all validation rules."""
    query = db.query(RateValidationRule)
    if active_only:
        query = query.filter(RateValidationRule.is_active == True)
    return [ValidationRule.from_orm(rule) for rule in query.all()]

@router.post("/optimization/market-analysis", response_model=MarketAnalysis)
async def create_market_analysis(
    analysis: MarketAnalysisCreate,
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; MarketAnalysis:
    """Create a new market analysis entry."""
    db_analysis = MarketRateAnalysis(**analysis.dict())
    db.add(db_analysis)
    db.commit()
    db.refresh(db_analysis)
    return MarketAnalysis.from_orm(db_analysis)

@router.get("/optimization/market-analysis", response_model=List[MarketAnalysis])
async def get_market_analysis(
    service_type: str,
    location: Optional[str] = None,
    limit: int = Query(10, ge=1, le=100),
    rate_service: RateService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
) -&gt; List[MarketAnalysis]:
    """Get market analysis for a service type."""
    query = db.query(MarketRateAnalysis)\
        .filter(MarketRateAnalysis.service_type == service_type)
    
    if location:
        query = query.filter(MarketRateAnalysis.location == location)
    
    analyses = query.order_by(MarketRateAnalysis.created_at.desc())\
        .limit(limit)\
        .all()
    
    return [MarketAnalysis.from_orm(analysis) for analysis in analyses]

@router.post("/optimization/apply/{optimization_id}")
async def apply_optimization(
    optimization_id: int,
    rate_service: RateService = Depends(),
    rate_integration_service: RateIntegrationService = Depends(),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(get_current_admin_user)
):
    """Apply an optimization to a rate card."""
    history = db.query(RateOptimizationHistory)\
        .filter(RateOptimizationHistory.id == optimization_id)\
        .first()
    
    if not history:
        raise HTTPException(
            status_code=404,
            detail="Optimization history not found"
        )
    
    if history.applied:
        raise HTTPException(
            status_code=400,
            detail="Optimization already applied"
        )
    
    try:
        await rate_integration_service.apply_optimization(history, db)
        
        history.applied = True
        history.applied_at = datetime.utcnow()
        history.applied_by = current_user.id
        db.commit()
        
        return {"message": "Optimization applied successfully"}
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to apply optimization: {str(e)}"
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\api\v1\endpoints\rate_cards.py</data>
</node>
<node id="config">
  <data key="d0">module</data>
  <data key="d1">"""Application configuration."""

import os
import secrets
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from pydantic import AnyHttpUrl, PostgresDsn, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    """Application settings."""
    
    model_config = SettingsConfigDict(
        env_file=[
            str(Path(__file__).parent.parent.parent.parent / "deployment" / "config" / ".env"),
            str(Path(__file__).parent.parent.parent.parent / "deployment" / "config" / f"{os.getenv('APP_ENV', 'development')}" / "backend.env"),
            str(Path(__file__).parent.parent.parent.parent / "deployment" / "config" / "shared" / "*.env"),
        ],
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore"
    )
    
    # Core Application Settings
    APP_ENV: str = "development"
    APP_NAME: str = "AUL Quote App"
    APP_VERSION: str = "1.0.0"
    API_V1_STR: str = "/api/v1"
    DEBUG: bool = True
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    TESTING: bool = bool(os.getenv("TESTING", ""))
    
    # CORS Configuration
    BACKEND_CORS_ORIGINS: List[AnyHttpUrl] = []

    @field_validator("BACKEND_CORS_ORIGINS", mode="before")
    def assemble_cors_origins(cls, v: Union[str, List[str]]) -&gt; Union[List[str], str]:
        """Validate CORS origins."""
        if isinstance(v, str) and not v.startswith("["):
            return [i.strip() for i in v.split(",")]
        elif isinstance(v, (list, str)):
            return v
        raise ValueError(v)

    # Database Configuration
    POSTGRES_HOST: str = "localhost"
    POSTGRES_PORT: str = "5432"
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = "postgres"
    POSTGRES_DB: str = "warehouse_quotes"
    DB_POOL_SIZE: int = 5
    DB_MAX_OVERFLOW: int = 10
    DB_POOL_TIMEOUT: int = 30
    DB_POOL_RECYCLE: int = 1800
    
    # Logging Configuration
    LOG_LEVEL: str = "INFO"
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_FILE: Optional[str] = None

    # Database Backup Configuration
    DB_BACKUP_RETENTION_DAYS: int = 7
    DB_BACKUP_TIME: str = "00:00"
    DB_BACKUP_COMPRESS: bool = True
    
    # Migration Settings
    ALEMBIC_CONFIG: str = "alembic.ini"
    
    # Security Settings
    SECRET_KEY: str = secrets.token_urlsafe(32)
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    REFRESH_TOKEN_EXPIRE_DAYS: int = 7
    ALGORITHM: str = "HS256"
    
    # Email Settings
    SMTP_TLS: bool = True
    SMTP_PORT: Optional[int] = None
    SMTP_HOST: Optional[str] = None
    SMTP_USER: Optional[str] = None
    SMTP_PASSWORD: Optional[str] = None
    EMAILS_FROM_EMAIL: Optional[str] = None
    EMAILS_FROM_NAME: Optional[str] = None
    
    # File Upload Settings
    UPLOAD_DIR: str = "uploads"
    MAX_UPLOAD_SIZE: int = 10 * 1024 * 1024  # 10MB
    ALLOWED_UPLOAD_EXTENSIONS: List[str] = [".pdf", ".doc", ".docx", ".xls", ".xlsx"]
    
    # Rate Card Settings
    RATE_CARD_CACHE_TTL: int = 3600  # 1 hour
    RATE_CALCULATION_TIMEOUT: int = 30  # seconds
    
    # AI/LLM Settings
    AI_MODEL: str = "gpt-4"
    AI_TEMPERATURE: float = 0.7
    AI_MAX_TOKENS: int = 2000
    AI_TIMEOUT: int = 60  # seconds
    
    # Monitoring Settings
    SENTRY_DSN: Optional[str] = None
    ENABLE_METRICS: bool = True
    METRICS_PORT: int = 9090

    # Redis Settings
    REDIS_HOST: str = "localhost"
    REDIS_PORT: int = 6379
    REDIS_DB: int = 0
    REDIS_PASSWORD: Optional[str] = None

    # Celery Settings
    CELERY_BROKER_URL: str = "redis://localhost:6379/0"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/0"
    
    @property
    def database_url(self) -&gt; str:
        """Get database URL."""
        if self.TESTING:
            return "sqlite+aiosqlite:///./test.db"
        return str(
            PostgresDsn.build(
                scheme="postgresql+asyncpg",
                username=self.POSTGRES_USER,
                password=self.POSTGRES_PASSWORD,
                host=self.POSTGRES_HOST,
                port=int(self.POSTGRES_PORT),
                path=self.POSTGRES_DB,
            )
        )

    @property
    def redis_url(self) -&gt; str:
        """Get Redis URL."""
        password_str = f":{self.REDIS_PASSWORD}@" if self.REDIS_PASSWORD else ""
        return f"redis://{password_str}{self.REDIS_HOST}:{self.REDIS_PORT}/{self.REDIS_DB}"

    class Config:
        env_prefix = ""
        case_sensitive = True


@lru_cache()
def get_settings() -&gt; Settings:
    """Get cached settings instance."""
    return Settings()


# Create settings instance
settings = get_settings()
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\config.py</data>
</node>
<node id="dependencies">
  <data key="d0">module</data>
  <data key="d1">"""
Authentication dependencies for FastAPI.
"""

from typing import Optional
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from sqlalchemy.orm import Session
from passlib.context import CryptContext

from warehouse_quote_app.app.database import get_db
from warehouse_quote_app.app.models.user import User
from .jwt import decode_access_token

# Password hashing
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def get_password_hash(password: str) -&gt; str:
    """Get password hash."""
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -&gt; bool:
    """Verify password against hash."""
    return pwd_context.verify(plain_password, hashed_password)

async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db: Session = Depends(get_db)
) -&gt; User:
    """Get current user from token."""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = decode_access_token(token)
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except Exception:
        raise credentials_exception
        
    user = db.query(User).filter(User.username == username).first()
    if user is None:
        raise credentials_exception
    return user

async def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -&gt; User:
    """Get current active user."""
    if not current_user.is_active:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Inactive user"
        )
    return current_user

async def get_current_admin_user(
    current_user: User = Depends(get_current_active_user)
) -&gt; User:
    """Get current admin user."""
    if not current_user.is_admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )
    return current_user
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\auth\dependencies.py</data>
</node>
<node id="jwt">
  <data key="d0">module</data>
  <data key="d1">"""
JWT token handling utilities.
"""

from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from jose import jwt

from app.core.config import settings

def create_access_token(
    data: Dict[str, Any],
    expires_delta: Optional[timedelta] = None
) -&gt; str:
    """Create JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM
    )
    return encoded_jwt

def decode_access_token(token: str) -&gt; Dict[str, Any]:
    """Decode JWT access token."""
    return jwt.decode(
        token,
        settings.SECRET_KEY,
        algorithms=[settings.ALGORITHM]
    )

def get_token_payload(token: str) -&gt; Optional[Dict[str, Any]]:
    """Get payload from JWT token."""
    try:
        return decode_access_token(token)
    except jwt.JWTError:
        return None
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\auth\jwt.py</data>
</node>
<node id="service">
  <data key="d0">module</data>
  <data key="d1">"""
Authentication service for handling user authentication and authorization.
"""

from datetime import datetime, timedelta
from typing import Optional
from fastapi import Depends, HTTPException, status
from sqlalchemy.orm import Session
from jose import JWTError

from app.core.config import settings
from app.core.database import get_db
from app.models.user import User
from app.schemas.user.auth import Token, TokenData
from .jwt import create_access_token, decode_access_token
from .dependencies import get_password_hash, verify_password

class AuthService:
    """Service for handling authentication and authorization."""

    def __init__(self, db: Session = Depends(get_db)):
        self.db = db

    async def authenticate_user(self, username: str, password: str) -&gt; Optional[User]:
        """Authenticate a user by username and password."""
        user = self.db.query(User).filter(User.username == username).first()
        if not user or not verify_password(password, user.hashed_password):
            return None
        return user

    async def create_user(self, username: str, email: str, password: str, is_admin: bool = False) -&gt; User:
        """Create a new user."""
        hashed_password = get_password_hash(password)
        user = User(
            username=username,
            email=email,
            hashed_password=hashed_password,
            is_admin=is_admin
        )
        self.db.add(user)
        self.db.commit()
        self.db.refresh(user)
        return user

    async def get_login_token(self, username: str, password: str) -&gt; Token:
        """Get login token for user."""
        user = await self.authenticate_user(username, password)
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
        access_token = create_access_token(
            data={"sub": user.username, "is_admin": user.is_admin},
            expires_delta=access_token_expires
        )
        return Token(access_token=access_token, token_type="bearer")

    async def change_password(self, user: User, current_password: str, new_password: str) -&gt; bool:
        """Change user password."""
        if not verify_password(current_password, user.hashed_password):
            return False
        
        user.hashed_password = get_password_hash(new_password)
        self.db.commit()
        return True

    async def request_password_reset(self, email: str) -&gt; Optional[str]:
        """Request password reset for user."""
        user = self.db.query(User).filter(User.email == email).first()
        if not user:
            return None
        
        # Generate reset token valid for 1 hour
        reset_token = create_access_token(
            data={"sub": user.username, "type": "reset"},
            expires_delta=timedelta(hours=1)
        )
        return reset_token

    async def reset_password(self, token: str, new_password: str) -&gt; bool:
        """Reset user password using reset token."""
        try:
            payload = decode_access_token(token)
            if payload.get("type") != "reset":
                return False
            
            username = payload.get("sub")
            if not username:
                return False
            
            user = self.db.query(User).filter(User.username == username).first()
            if not user:
                return False
            
            user.hashed_password = get_password_hash(new_password)
            self.db.commit()
            return True
            
        except JWTError:
            return False

    async def revoke_token(self, token: str) -&gt; bool:
        """Revoke an access token."""
        # TODO: Implement token blacklist using Redis
        return True

    async def verify_token(self, token: str) -&gt; Optional[TokenData]:
        """Verify an access token."""
        try:
            payload = decode_access_token(token)
            username = payload.get("sub")
            if not username:
                return None
            return TokenData(username=username, is_admin=payload.get("is_admin", False))
        except JWTError:
            return None
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\auth\service.py</data>
</node>
<node id="audit">
  <data key="d0">module</data>
  <data key="d1">"""
Audit logging functionality.

Note: Audit logging is temporarily disabled until the full audit system is implemented.
The models and schemas for audit logs will be implemented as part of the audit system.
"""

from datetime import datetime
from typing import Optional, Dict, Any
from sqlalchemy.orm import Session

# TODO: Implement audit models and schemas
# from warehouse_quote_app.app.models.audit import AuditLog
# from warehouse_quote_app.app.schemas.audit import AuditLogCreate

def log_event(
    db: Session,
    event_type: str,
    user_id: Optional[int],
    resource_type: str,
    resource_id: Optional[str],
    details: Dict[str, Any],
    ip_address: Optional[str] = None
) -&gt; None:
    """
    Log an audit event.
    
    Note: Currently a no-op until audit system is implemented.
    """
    # TODO: Implement audit logging
    pass

__all__ = ['log_event']
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\monitoring\audit.py</data>
</node>
<node id="logging">
  <data key="d0">module</data>
  <data key="d1">"""
Logging configuration and utilities.
"""

import logging
import sys
from typing import Dict, Any
from pathlib import Path

from warehouse_quote_app.app.core.config import settings

# Configure logging format
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format=settings.LOG_FORMAT,
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("app.log")
    ]
)

def get_logger(name: str) -&gt; logging.Logger:
    """Get a logger instance."""
    return logging.getLogger(name)

def log_error(
    logger: logging.Logger,
    error: Exception,
    context: Dict[str, Any]
) -&gt; None:
    """Log an error with context."""
    error_type = type(error).__name__
    error_message = str(error)
    
    logger.error(
        f"Error: {error_type} - {error_message}",
        extra={
            "error_type": error_type,
            "error_message": error_message,
            **context
        }
    )

__all__ = ['get_logger', 'log_error']
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\monitoring\logging.py</data>
</node>
<node id="rate_limit">
  <data key="d0">module</data>
  <data key="d1">"""
Rate limiting functionality.
"""

from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse
import time
from typing import Dict, Tuple, Optional
from collections import defaultdict

from warehouse_quote_app.app.core.config import settings

# Store for rate limiting data
# key: IP address, value: (request_count, window_start)
rate_limit_store: Dict[str, Tuple[int, float]] = defaultdict(lambda: (0, time.time()))

def get_client_ip(request: Request) -&gt; str:
    """Get client IP address from request."""
    forwarded = request.headers.get("X-Forwarded-For")
    if forwarded:
        return forwarded.split(",")[0]
    return request.client.host if request.client else "unknown"  # type: ignore

async def rate_limit(
    request: Request,
    requests: Optional[int] = None,
    period: Optional[int] = None
) -&gt; None:
    """Rate limit middleware."""
    if not settings.RATE_LIMIT_ENABLED:
        return

    ip = get_client_ip(request)
    now = time.time()
    
    # Get rate limit settings
    max_requests = requests or settings.RATE_LIMIT_REQUESTS
    window_size = period or settings.RATE_LIMIT_PERIOD
    
    # Get current state
    count, window_start = rate_limit_store[ip]
    
    # Reset if window has expired
    if now - window_start &gt;= window_size:
        count = 0
        window_start = now
    
    # Check limit
    if count &gt;= max_requests:
        reset_time = window_start + window_size - now
        raise HTTPException(
            status_code=429,
            detail={
                "error": "Too many requests",
                "reset_in_seconds": int(reset_time)
            }
        )
    
    # Update store
    rate_limit_store[ip] = (count + 1, window_start)

__all__ = ['rate_limit', 'get_client_ip']
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\security\rate_limit.py</data>
</node>
<node id="security">
  <data key="d0">module</data>
  <data key="d1">"""
General security utilities.
"""

import secrets
from typing import Optional
from fastapi import Request, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from passlib.context import CryptContext

from warehouse_quote_app.app.core.config import settings

# Password hashing context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -&gt; bool:
    """Verify a password against its hash."""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -&gt; str:
    """Hash a password."""
    return pwd_context.hash(password)

def generate_secure_token(length: int = 32) -&gt; str:
    """Generate a secure random token."""
    return secrets.token_urlsafe(length)

def validate_api_key(api_key: str) -&gt; bool:
    """Validate an API key."""
    # TODO: Implement proper API key validation
    return api_key == settings.SECRET_KEY

class CustomHTTPBearer(HTTPBearer):
    """Custom HTTP Bearer authentication."""
    
    async def __call__(
        self,
        request: Request
    ) -&gt; Optional[HTTPAuthorizationCredentials]:
        """Validate bearer token."""
        credentials = await super().__call__(request)
        
        if not credentials:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Invalid authorization credentials"
            )
        
        if not validate_api_key(credentials.credentials):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API key"
            )
        
        return credentials

__all__ = [
    'verify_password',
    'get_password_hash',
    'generate_secure_token',
    'validate_api_key',
    'CustomHTTPBearer'
]
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\security\security.py</data>
</node>
<node id="celery">
  <data key="d0">module</data>
  <data key="d1">"""
Celery configuration and task queue setup.
"""

from celery import Celery
from celery.schedules import crontab
from pathlib import Path
import os
from typing import Any, Dict

from warehouse_quote_app.app.core.config import settings

# Create Celery app
celery_app = Celery(
    'warehouse_quote_app',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0',
    include=[
        'app.services.business.tasks.rates',  # Rate-related tasks
        'app.core.tasks.cleanup',            # General cleanup tasks
        'app.core.tasks.monitoring',          # Monitoring tasks
        'warehouse_quote_app.app.core.tasks.quotes',  # Quote-related tasks
        'warehouse_quote_app.app.core.tasks.rates',  # Rate-related tasks
        'warehouse_quote_app.app.core.tasks.admin',  # Admin tasks
    ]
)

# Configure Celery
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)

# Configure periodic tasks
celery_app.conf.beat_schedule = {
    "cleanup-expired-quotes": {
        "task": "warehouse_quote_app.app.core.tasks.quotes.cleanup_expired_quotes",
        "schedule": crontab(hour=0, minute=0),  # Run daily at midnight
    },
    "update-rate-cards": {
        "task": "warehouse_quote_app.app.core.tasks.rates.update_rate_cards",
        "schedule": crontab(hour="*/6"),  # Run every 6 hours
    },
    "backup-database": {
        "task": "warehouse_quote_app.app.core.tasks.admin.backup_database",
        "schedule": crontab(hour=3, minute=0),  # Run daily at 3 AM
    },
}

def init_celery(app=None) -&gt; None:
    """Initialize Celery with FastAPI app context."""
    if app:
        class ContextTask(celery_app.Task):
            def __call__(self, *args: Any, **kwargs: Dict[str, Any]) -&gt; Any:
                with app.app_context():
                    return self.run(*args, **kwargs)

        celery_app.Task = ContextTask

__all__ = ['celery_app', 'init_celery']
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\core\tasks\celery.py</data>
</node>
<node id="session">
  <data key="d0">module</data>
  <data key="d1">"""Database session management."""

from contextlib import asynccontextmanager
from typing import AsyncGenerator, AsyncContextManager, Optional
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    AsyncEngine,
    async_sessionmaker
)
from sqlalchemy.orm import declarative_base, registry
from sqlalchemy.pool import AsyncAdaptedQueuePool

from warehouse_quote_app.app.core.config import settings

# Create registry for declarative models
mapper_registry = registry()
Base = mapper_registry.generate_base()

class DatabaseSessionManager:
    """Database session manager."""

    def __init__(self):
        """Initialize session manager."""
        self._engine: Optional[AsyncEngine] = None
        self._sessionmaker: Optional[async_sessionmaker] = None

    def init(self, db_url: str = None):
        """Initialize database engine and session maker."""
        if self._engine is not None:
            return

        # Use provided URL or get from settings
        if db_url is None:
            db_url = settings.database_url

        # Handle SQLite differently
        is_sqlite = "sqlite" in db_url
        if is_sqlite:
            self._engine = create_async_engine(
                db_url,
                connect_args={"check_same_thread": False},
                echo=settings.DEBUG
            )
        else:
            self._engine = create_async_engine(
                db_url,
                poolclass=AsyncAdaptedQueuePool,
                pool_pre_ping=True,
                pool_size=settings.DB_POOL_SIZE,
                max_overflow=settings.DB_MAX_OVERFLOW,
                pool_timeout=settings.DB_POOL_TIMEOUT,
                pool_recycle=settings.DB_POOL_RECYCLE,
                echo=settings.DEBUG
            )

        self._sessionmaker = async_sessionmaker(
            autocommit=False,
            autoflush=False,
            bind=self._engine,
            class_=AsyncSession
        )

    async def close(self):
        """Close database connections."""
        if self._engine is None:
            return

        await self._engine.dispose()
        self._engine = None
        self._sessionmaker = None

    @asynccontextmanager
    async def session(self) -&gt; AsyncGenerator[AsyncSession, None]:
        """Get database session."""
        if self._sessionmaker is None:
            self.init()

        session: AsyncSession = self._sessionmaker()
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

    @property
    def engine(self) -&gt; AsyncEngine:
        """Get database engine."""
        if self._engine is None:
            self.init()
        return self._engine

# Global session manager instance
sessionmanager = DatabaseSessionManager()

async def get_db() -&gt; AsyncGenerator[AsyncSession, None]:
    """Get database session."""
    async with sessionmanager.session() as session:
        yield session
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\database\session.py</data>
</node>
<node id="base">
  <data key="d0">module</data>
  <data key="d1">"""Base service class with CRUD operations."""

from sqlalchemy.orm import Session
from typing import Any, Generic, TypeVar, Optional, Type, List, Dict
from pydantic import BaseModel
from app.models.base import Base

ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)

class BaseService(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    """Base service class that includes CRUD operations."""

    def __init__(self, model: Type[ModelType]):
        """Initialize service with model class."""
        self.model = model

    def get(self, db: Session, id: int) -&gt; Optional[ModelType]:
        """Get a single record by ID."""
        return self.model.get(db, id)

    def get_multi(self, db: Session, *, skip: int = 0, limit: int = 100) -&gt; List[ModelType]:
        """Get multiple records with pagination."""
        return self.model.get_multi(db, skip=skip, limit=limit)

    def create(self, db: Session, *, obj_in: CreateSchemaType) -&gt; ModelType:
        """Create a new record."""
        obj_in_data = obj_in.model_dump()
        return self.model.create(db, data=obj_in_data)

    def update(self, db: Session, *, db_obj: ModelType, obj_in: UpdateSchemaType) -&gt; ModelType:
        """Update an existing record."""
        update_data = obj_in.model_dump(exclude_unset=True)
        return self.model.update(db, db_obj=db_obj, data=update_data)

    def delete(self, db: Session, *, id: int) -&gt; Optional[ModelType]:
        """Delete a record by ID."""
        return self.model.delete(db, id=id)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\base.py</data>
</node>
<node id="quote">
  <data key="d0">module</data>
  <data key="d1">"""
Quote repository for database operations.
"""

from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import select, func

from app.models.quote import Quote
from app.schemas.quote import QuoteCreate, QuoteUpdate
from .base import BaseRepository
from .mixins import FilterMixin, AggregationMixin, PaginationMixin

class QuoteRepository(
    BaseRepository[Quote, QuoteCreate, QuoteUpdate],
    FilterMixin[Quote],
    AggregationMixin[Quote],
    PaginationMixin[Quote]
):
    """Repository for quote-related database operations."""

    async def get_quotes_by_customer(
        self,
        db: Session,
        customer_id: int,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[Quote]:
        """Get quotes for a specific customer."""
        return await self.get_by_field(db, "customer_id", customer_id, skip, limit)

    async def get_quotes_by_status(
        self,
        db: Session,
        status: str,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[Quote]:
        """Get quotes by status."""
        return await self.get_by_field(db, "status", status, skip, limit)

    async def get_quotes_by_date_range(
        self,
        db: Session,
        start_date: datetime,
        end_date: datetime,
        skip: int = 0,
        limit: int = 100
    ) -&gt; List[Quote]:
        """Get quotes within a date range."""
        return await self.get_by_date_range(db, "created_at", start_date, end_date, skip, limit)

    async def get_total_quote_value(
        self,
        db: Session,
        customer_id: Optional[int] = None
    ) -&gt; float:
        """Get total value of all quotes, optionally filtered by customer."""
        query = select(func.sum(self.model.total_amount))
        
        if customer_id:
            query = query.where(self.model.customer_id == customer_id)
            
        return db.scalar(query) or 0.0

    async def get_quote_count_by_status(
        self,
        db: Session
    ) -&gt; Dict[str, int]:
        """Get count of quotes grouped by status."""
        result = (
            db.query(
                self.model.status,
                func.count(self.model.id)
            )
            .group_by(self.model.status)
            .all()
        )
        
        return {status: count for status, count in result}

    async def get_quote_stats_by_customer(
        self,
        db: Session,
        customer_id: int
    ) -&gt; dict:
        """Get quote statistics for a customer."""
        return {
            "total_quotes": await self.count_by_field(db, "customer_id", customer_id),
            "total_amount_stats": await self.get_field_stats(db, "total_amount")
        }
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\repositories\quote.py</data>
</node>
<node id="rate">
  <data key="d0">module</data>
  <data key="d1">"""Rate service for managing rate operations with AI optimization."""

from datetime import datetime
from decimal import Decimal
from typing import List, Optional, Dict, Any, Tuple, cast
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import HTTPException, status

from warehouse_quote_app.app.models.rate import Rate
from warehouse_quote_app.app.repositories.rate import RateRepository
from warehouse_quote_app.app.services.validation.validation import ValidationService, ValidationResult
from warehouse_quote_app.app.services.ai.llm import LLMService
from warehouse_quote_app.app.services.ai.rag import RAGService
from warehouse_quote_app.app.schemas.rate.rate import (
    RateCreate,
    RateUpdate,
    RateResponse,
    RateOptimizationResult,
    RateValidationResponse,
    RateListResponse
)
from warehouse_quote_app.app.core.monitoring import log_event
from warehouse_quote_app.app.core.config import Settings

class RateService:
    """Service for managing rate operations with AI optimization."""

    def __init__(
        self,
        db: AsyncSession,
        llm: Optional[LLMService] = None,
        rag: Optional[RAGService] = None,
        settings: Optional[Settings] = None
    ) -&gt; None:
        """Initialize rate service with AI capabilities.
        
        Args:
            db: Database session
            llm: Language model service for rate optimization
            rag: Retrieval-augmented generation service
            settings: Application settings
            
        Note:
            LLM and RAG services will be initialized with defaults if not provided
        """
        self.db = db
        self.repository = RateRepository(db)
        self.validator = ValidationService()
        self.llm = llm or LLMService()
        self.rag = rag or RAGService()
        self.settings = settings or Settings()

    async def create_rate(
        self,
        rate_data: RateCreate,
        user_id: int
    ) -&gt; RateResponse:
        """Create a new rate with validation and optimization.
        
        Args:
            rate_data: Rate creation data
            user_id: ID of the user creating the rate
            
        Returns:
            RateResponse: Created rate data
            
        Raises:
            ValueError: If rate data is invalid
            HTTPException: If rate creation fails
        """
        # Validate rate data
        validation_result = await self.validator.validate_rate(rate_data)
        if not validation_result.is_valid:
            raise ValueError(f"Invalid rate data: {validation_result.errors}")

        # Create rate
        rate = await self.repository.create(rate_data)
        
        # Log event
        await log_event(
            "rate_created",
            {"rate_id": rate.id, "user_id": user_id}
        )
        
        return RateResponse.from_orm(rate)

    async def update_rate(
        self,
        rate_id: int,
        rate_data: RateUpdate,
        user_id: int
    ) -&gt; Optional[RateResponse]:
        """Update an existing rate.
        
        Args:
            rate_id: ID of the rate to update
            rate_data: Rate update data
            user_id: ID of the user making the update
            
        Returns:
            Optional[RateResponse]: Updated rate data or None if not found
            
        Raises:
            ValueError: If rate data is invalid
            HTTPException: If update fails
        """
        # Validate update data
        validation_result = await self.validator.validate_rate_update(rate_data)
        if not validation_result.is_valid:
            raise ValueError(f"Invalid rate data: {validation_result.errors}")

        # Update rate
        rate = await self.repository.update(rate_id, rate_data)
        if not rate:
            return None

        # Log event
        await log_event(
            "rate_updated",
            {"rate_id": rate_id, "user_id": user_id}
        )

        return RateResponse.from_orm(rate)

    async def get_rate(
        self,
        rate_id: int
    ) -&gt; Optional[RateResponse]:
        """Get rate by ID.
        
        Args:
            rate_id: ID of the rate to retrieve
            
        Returns:
            Optional[RateResponse]: Rate data or None if not found
        """
        rate = await self.repository.get(rate_id)
        if not rate:
            return None
        return RateResponse.from_orm(rate)

    async def list_rates(
        self,
        skip: int = 0,
        limit: int = 100,
        filters: Optional[Dict[str, Any]] = None
    ) -&gt; Tuple[List[RateListResponse], int]:
        """List rates with pagination and filtering.
        
        Args:
            skip: Number of records to skip
            limit: Maximum number of records to return
            filters: Optional filtering criteria
            
        Returns:
            Tuple[List[RateListResponse], int]: List of rates and total count
        """
        rates, total = await self.repository.get_multi(
            skip=skip,
            limit=limit,
            filters=filters or {}
        )
        return [RateListResponse.from_orm(r) for r in rates], total

    async def optimize_rate(
        self,
        rate_id: int,
        context: Dict[str, Any]
    ) -&gt; RateOptimizationResult:
        """Optimize rate using AI models.
        
        Args:
            rate_id: ID of the rate to optimize
            context: Contextual data for optimization
            
        Returns:
            RateOptimizationResult: Optimization suggestions and confidence scores
            
        Raises:
            HTTPException: If rate not found or optimization fails
        """
        rate = await self.repository.get(rate_id)
        if not rate:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Rate not found"
            )

        # Get historical context
        historical_data = await self.rag.get_rate_context(rate_id)
        
        # Run optimization
        optimization_result = await self.llm.optimize_rate(
            rate=rate,
            context=context,
            historical_data=historical_data
        )
        
        # Log optimization attempt
        await log_event(
            "rate_optimization",
            {
                "rate_id": rate_id,
                "confidence": optimization_result.confidence_score,
                "suggested_changes": len(optimization_result.suggestions)
            }
        )
        
        return optimization_result

    async def validate_rate_rules(
        self,
        rate_id: int
    ) -&gt; RateValidationResponse:
        """Validate rate against business rules.
        
        Args:
            rate_id: ID of the rate to validate
            
        Returns:
            RateValidationResponse: Validation results and suggestions
            
        Raises:
            HTTPException: If rate not found
        """
        rate = await self.repository.get(rate_id)
        if not rate:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Rate not found"
            )

        # Run validation
        validation_result = await self.validator.validate_rate_rules(rate)
        
        # Log validation
        await log_event(
            "rate_validation",
            {
                "rate_id": rate_id,
                "is_valid": validation_result.is_valid,
                "error_count": len(validation_result.errors)
            }
        )
        
        return RateValidationResponse(
            is_valid=validation_result.is_valid,
            errors=validation_result.errors,
            warnings=validation_result.warnings,
            suggestions=validation_result.suggestions
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\rate.py</data>
</node>
<node id="user">
  <data key="d0">module</data>
  <data key="d1">"""User schemas."""
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, EmailStr, Field, ConfigDict

class UserBase(BaseModel):
    """Base user schema."""
    email: EmailStr
    first_name: str = Field(..., min_length=1)
    last_name: str = Field(..., min_length=1)
    is_active: bool = True
    is_admin: bool = False
    company_name: Optional[str] = None
    phone: Optional[str] = None

class UserCreate(UserBase):
    """User creation schema."""
    password: str = Field(..., min_length=8)

class UserUpdate(BaseModel):
    """Schema for updating an existing user."""
    email: Optional[EmailStr] = None
    first_name: Optional[str] = Field(None, min_length=1)
    last_name: Optional[str] = Field(None, min_length=1)
    password: Optional[str] = Field(None, min_length=8)
    is_active: Optional[bool] = None
    is_admin: Optional[bool] = None
    company_name: Optional[str] = None
    phone: Optional[str] = None

    model_config = ConfigDict(
        from_attributes=True,
        populate_by_name=True
    )

class UserResponse(UserBase):
    """User response schema."""
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None

    model_config = ConfigDict(
        from_attributes=True,
        populate_by_name=True
    )

class UserBaseSimple(BaseModel):
    """Simple base user schema."""
    email: EmailStr
    is_active: bool = True
    is_admin: bool = False

class UserCreateSimple(UserBaseSimple):
    """Simple user creation schema."""
    password: str = Field(..., min_length=8)

class UserResponseSimple(UserBaseSimple):
    """Simple user response schema."""
    id: int

    model_config = ConfigDict(
        from_attributes=True,
        populate_by_name=True
    )

__all__ = [
    'UserBase',
    'UserCreate',
    'UserUpdate',
    'UserResponse',
    'UserBaseSimple',
    'UserCreateSimple',
    'UserResponseSimple'
]
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\user\user.py</data>
</node>
<node id="rate_card">
  <data key="d0">module</data>
  <data key="d1">"""Rate card schemas."""
from pydantic import BaseModel, ConfigDict, Field, field_validator
from typing import List, Optional, Dict, Any, Union, Annotated
from datetime import datetime
from decimal import Decimal
from enum import Enum
from .base import BaseSchema, BaseResponseSchema
from app.models.types import RateCategory, RateType

class ClientType(str, Enum):
    """Client type enumeration."""
    RETAIL = "retail"
    WHOLESALE = "wholesale"
    ENTERPRISE = "enterprise"
    GOVERNMENT = "government"

class VolumeDiscount(BaseSchema):
    """Volume discount configuration."""
    min_quantity: Annotated[int, Field(gt=0, description="Minimum quantity for discount")]
    discount_percentage: Annotated[Decimal, Field(gt=0, le=100, description="Discount percentage as decimal")]

class Rate(BaseSchema):
    """Individual rate configuration."""
    item_type: Annotated[str, Field(min_length=1, description="Type of item being rated")]
    base_rate: Annotated[Decimal, Field(gt=0, description="Base rate as decimal")]
    volume_discounts: List[VolumeDiscount] = Field(default_factory=list)

class RateCardSettingsBase(BaseSchema):
    """Base schema for rate card settings."""
    name: Annotated[str, Field(min_length=1)]
    description: Optional[str] = None
    is_active: bool = True
    minimum_charge: Annotated[Decimal, Field(default=Decimal("0.00"), ge=0, description="Minimum charge amount")]
    handling_fee_percentage: Annotated[Decimal, Field(default=Decimal("0.00"), ge=0, le=100, description="Handling fee percentage")]
    tax_rate: Annotated[Decimal, Field(default=Decimal("0.10"), ge=0, le=100, description="Tax rate percentage")]
    volume_discount_tiers: Dict[str, Dict[str, Union[int, Decimal]]] = Field(
        default_factory=lambda: {
            "tier1": {"min_amount": 1000, "discount": Decimal("0.05")},
            "tier2": {"min_amount": 5000, "discount": Decimal("0.10")},
            "tier3": {"min_amount": 10000, "discount": Decimal("0.15")}
        }
    )
    max_quote_items: Annotated[int, Field(default=100, gt=0)]
    max_quote_value: Annotated[Decimal, Field(default=Decimal("100000.00"), gt=0)]
    requires_approval_above: Annotated[Decimal, Field(default=Decimal("10000.00"), gt=0)]

class RateCardSettingsCreate(RateCardSettingsBase):
    """Schema for creating rate card settings."""
    max_quote_items: Annotated[int, Field(gt=0, description="Maximum number of items per quote")]
    max_quote_value: Annotated[Decimal, Field(gt=0, description="Maximum quote value allowed")]
    requires_approval_above: Annotated[Decimal, Field(gt=0, description="Quote value that requires approval")]

class RateCardSettingsUpdate(BaseSchema):
    """Schema for updating rate card settings."""
    name: Optional[Annotated[str, Field(min_length=1)]] = None
    description: Optional[str] = None
    is_active: Optional[bool] = None
    minimum_charge: Optional[Annotated[Decimal, Field(ge=0)]] = None
    handling_fee_percentage: Optional[Annotated[Decimal, Field(ge=0, le=100)]] = None
    tax_rate: Optional[Annotated[Decimal, Field(ge=0, le=100)]] = None
    volume_discount_tiers: Optional[Dict[str, Dict[str, Union[int, Decimal]]]] = None
    max_quote_items: Optional[Annotated[int, Field(gt=0)]] = None
    max_quote_value: Optional[Annotated[Decimal, Field(gt=0)]] = None
    requires_approval_above: Optional[Annotated[Decimal, Field(gt=0)]] = None

class RateCardSettingsResponse(RateCardSettingsBase, BaseResponseSchema):
    """Schema for rate card settings response."""
    model_config = ConfigDict(from_attributes=True)
    id: int

class RateCardBase(BaseSchema):
    """Base schema for rate card."""
    name: Annotated[str, Field(min_length=1)]
    description: Optional[str] = None
    rate_type: RateType = Field(default=RateType.ALL)
    category: RateCategory = Field(default=RateCategory.STANDARD)
    rates: Dict[str, Decimal] = Field(default_factory=dict)
    additional_charges: Dict[str, Any] = Field(default_factory=dict)
    is_active: bool = True
    effective_from: datetime = Field(..., description="When the rate card becomes effective")
    effective_until: Optional[datetime] = None
    settings_id: Optional[int] = None

    @field_validator('rates')
    def validate_rates(cls, v: Dict[str, Decimal]) -&gt; Dict[str, Decimal]:
        """Validate that all rates are positive."""
        for rate in v.values():
            if rate &lt;= 0:
                raise ValueError("Rates must be positive")
        return v

class RateCardCreate(RateCardBase):
    """Schema for creating rate card."""
    rates: Dict[str, Dict[str, Dict[str, Any]]] = Field(
        default_factory=dict,
        description="Rate configuration by service type and item type"
    )

class RateCardUpdate(BaseSchema):
    """Schema for updating rate card."""
    name: Optional[Annotated[str, Field(min_length=1)]] = None
    description: Optional[str] = None
    rate_type: Optional[RateType] = None
    category: Optional[RateCategory] = None
    rates: Optional[Dict[str, Decimal]] = None
    additional_charges: Optional[Dict[str, Any]] = None
    is_active: Optional[bool] = None
    effective_until: Optional[datetime] = None
    settings_id: Optional[int] = None

class RateCardResponse(RateCardBase, BaseResponseSchema):
    """Schema for rate card response."""
    model_config = ConfigDict(from_attributes=True)
    id: int
    rates: Dict[str, Dict[str, Dict[str, Any]]]
    settings: Optional[RateCardSettingsResponse] = None

class CalculationItem(BaseSchema):
    """Schema for rate calculation item."""
    item_type: Annotated[str, Field(min_length=1, description="Type of item to calculate rate for")]
    quantity: Annotated[int, Field(gt=0, description="Quantity of items")]

class CalculateQuoteRequest(BaseSchema):
    """Schema for quote calculation request."""
    items: Annotated[List[CalculationItem], Field(min_items=1)]

class PriceBreakdown(BaseSchema):
    """Schema for price breakdown."""
    item_type: str
    quantity: Annotated[int, Field(gt=0)]
    base_rate: Annotated[Decimal, Field(gt=0)]
    base_price: Annotated[Decimal, Field(gt=0)]
    discount_percentage: Annotated[Decimal, Field(ge=0, le=100)]
    discount_amount: Annotated[Decimal, Field(ge=0)]
    final_price: Annotated[Decimal, Field(gt=0)]

class QuotePriceResponse(BaseSchema):
    """Schema for quote price response."""
    total_price: Annotated[Decimal, Field(gt=0)]
    breakdown: List[PriceBreakdown]

class CustomerRateCardBase(BaseSchema):
    """Base schema for customer-specific rate card."""
    customer_id: int = Field(..., description="ID of the customer")
    rate_card_id: int = Field(..., description="Original rate card ID")
    custom_rates: Optional[Dict[str, Decimal]] = Field(default_factory=dict, description="Override rates if applicable")


class CustomerRateCardCreate(CustomerRateCardBase):
    """Schema for creating a customer-specific rate card."""
    pass


class CustomerRateCardUpdate(BaseSchema):
    """Schema for updating a customer-specific rate card."""
    customer_id: Optional[int] = Field(None, description="ID of the customer")
    rate_card_id: Optional[int] = Field(None, description="Original rate card ID")
    custom_rates: Optional[Dict[str, Decimal]] = Field(None, description="Override rates if applicable")


class CustomerRateCardResponse(CustomerRateCardBase, BaseResponseSchema):
    """Schema for customer-specific rate card response."""
    id: int
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\rate\rate_card.py</data>
</node>
<node id="rate_optimization">
  <data key="d0">module</data>
  <data key="d1">"""
Schemas for rate optimization. Removed market analysis functionality.
"""

from pydantic import BaseModel, ConfigDict, Field
from typing import Dict, Any, List, Optional
from datetime import datetime
from decimal import Decimal

class OptimizationRequest(BaseModel):
    model_config = ConfigDict(extra='forbid')
    
    optimization_type: str = Field(
        ...,
        description="Type of optimization to perform",
        examples=["seasonal", "volume_based"]
    )
    parameters: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional parameters for optimization"
    )

class OptimizationResponse(BaseModel):
    model_config = ConfigDict(extra='forbid')
    
    original: Dict[str, Any]
    optimized: Dict[str, Any]
    metrics: Dict[str, Any]
    applied: bool = False
    created_at: datetime
    applied_at: Optional[datetime] = None

class ValidationRuleBase(BaseModel):
    model_config = ConfigDict(extra='forbid')
    
    name: str
    description: str
    rule_type: str = Field(
        ...,
        description="Type of validation rule",
        examples=["rate_range", "seasonal_adjustment"]
    )
    parameters: Dict[str, Any]
    is_active: bool = True
    severity: str = Field(
        "warning",
        description="Severity level of rule violation",
        examples=["info", "warning", "error"]
    )

class ValidationRuleCreate(ValidationRuleBase):
    pass

class ValidationRule(ValidationRuleBase):
    model_config = ConfigDict(from_attributes=True)
    
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None

class OptimizationMetrics(BaseModel):
    model_config = ConfigDict(extra='forbid')
    
    confidence_score: Decimal = Field(ge=0, le=1)
    potential_revenue_impact: Decimal
    seasonal_factors: Dict[str, Any]
    volume_analysis: Dict[str, Any]
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\rate\rate_optimization.py</data>
</node>
<node id="customer_report">
  <data key="d0">module</data>
  <data key="d1">"""Customer activity report schemas."""

from typing import Dict
from datetime import datetime
from pydantic import BaseModel, ConfigDict

from app.schemas.reports.base import BaseReport, ActivityMetrics, ValueMetrics, ReportMetadata

class CustomerActivityMetrics(ActivityMetrics):
    """Customer activity metrics."""
    total_quotes: int = 0
    accepted_quotes: int = 0
    rejected_quotes: int = 0

class CustomerValueMetrics(ValueMetrics):
    """Customer value metrics."""
    total_spent: float = 0.0
    average_order_value: float = 0.0
    lifetime_value: float = 0.0

class CustomerActivityReport(BaseReport):
    """Customer activity report."""
    metadata: ReportMetadata
    activity_metrics: CustomerActivityMetrics
    value_metrics: CustomerValueMetrics
    customer_segments: Dict[str, int] = {}
    customer_activity: Dict[str, CustomerActivityMetrics] = {}
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\reports\customer_report.py</data>
</node>
<node id="quote_report">
  <data key="d0">module</data>
  <data key="d1">"""Quote report schemas."""

from pydantic import BaseModel, ConfigDict
from datetime import datetime

class StatusCounts(BaseModel):
    model_config = ConfigDict(extra='forbid')
    PENDING: int
    ACCEPTED: int
    REJECTED: int

class QuoteReport(BaseModel):
    model_config = ConfigDict(extra='forbid')
    total_quotes: int
    accepted_quotes: int
    conversion_rate: float
    status_counts: StatusCounts
    start_date: datetime
    end_date: datetime
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\reports\quote_report.py</data>
</node>
<node id="service_report">
  <data key="d0">module</data>
  <data key="d1">"""Service usage report schemas."""

from typing import Dict, List
from datetime import datetime
from pydantic import BaseModel, ConfigDict

from app.schemas.reports.base import BaseReport, ActivityMetrics, ValueMetrics, ReportMetadata

class ServiceActivityMetrics(ActivityMetrics):
    """Service activity metrics."""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    error_rate: float = 0.0

class ServiceValueMetrics(ValueMetrics):
    """Service value metrics."""
    total_compute_time: float = 0.0
    average_compute_cost: float = 0.0
    total_api_calls: int = 0

class ServiceUsageReport(BaseReport):
    """Service usage report."""
    metadata: ReportMetadata
    activity_metrics: ServiceActivityMetrics
    value_metrics: ServiceValueMetrics
    service_metrics: Dict[str, ServiceActivityMetrics] = {}
    endpoint_metrics: Dict[str, ServiceActivityMetrics] = {}
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\schemas\reports\service_report.py</data>
</node>
<node id="rates">
  <data key="d0">module</data>
  <data key="d1">"""
Rate-related background tasks.
"""

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.tasks.celery import celery_app
from app.models.rate import Rate, RateCard
from app.core.db import get_db
from app.services.business.rates import RateService

@celery_app.task
async def update_rate_cards() -&gt; int:
    """Update all rate cards with latest rates.
    
    Returns:
        int: Number of rate cards updated
    """
    session = next(get_db())
    rate_service = RateService(session)
    
    # Get all active rate cards
    rate_cards = await session.execute(
        select(RateCard).where(RateCard.is_active == True)
    ).scalars().all()
    
    # Update each rate card with latest rates
    updated_count = 0
    for card in rate_cards:
        try:
            await rate_service.update_rate_card(card.id)
            updated_count += 1
        except Exception as e:
            # Log error but continue processing other cards
            print(f"Error updating rate card {card.id}: {str(e)}")
    
    await session.commit()
    return updated_count

@celery_app.task
async def optimize_rate_cards() -&gt; int:
    """Run optimization on all active rate cards.
    
    Returns:
        int: Number of rate cards optimized
    """
    session = next(get_db())
    rate_service = RateService(session)
    
    # Get all active rate cards
    rate_cards = await session.execute(
        select(RateCard).where(RateCard.is_active == True)
    ).scalars().all()
    
    # Optimize each rate card
    optimized_count = 0
    for card in rate_cards:
        try:
            await rate_service.optimize_rate(card.id, {"time_range": 90})
            optimized_count += 1
        except Exception as e:
            # Log error but continue processing other cards
            print(f"Error optimizing rate card {card.id}: {str(e)}")
    
    await session.commit()
    return optimized_count

__all__ = ['update_rate_cards', 'optimize_rate_cards']
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\business\tasks\rates.py</data>
</node>
<node id="storage">
  <data key="d0">module</data>
  <data key="d1">"""
Storage service for managing warehouse storage operations.
"""

from typing import Optional, Dict, Any
from decimal import Decimal
from sqlalchemy.orm import Session

from app.core.monitoring import log_event
from app.schemas.storage import StorageRequirements
from app.services.base import BaseService

class StorageService(BaseService):
    """Service for managing storage operations."""

    async def calculate_storage_cost(
        self,
        storage_req: StorageRequirements,
        db: Session
    ) -&gt; Decimal:
        """Calculate storage cost based on requirements."""
        if not storage_req:
            return Decimal('0')

        # Base rate per m²
        base_rate = Decimal('50.00')
        
        # Calculate floor area if not provided directly
        floor_area = storage_req.floor_area
        if not floor_area and storage_req.length and storage_req.width:
            floor_area = storage_req.length * storage_req.width
        
        if not floor_area:
            return Decimal('0')
            
        # Calculate base cost
        base_cost = base_rate * Decimal(str(floor_area))
        
        # Apply height surcharge if exceptionally tall
        if storage_req.is_exceptionally_tall:
            base_cost *= Decimal('1.5')
            
        # Add cost for bulky items
        if storage_req.num_bulky_items:
            bulky_item_rate = Decimal('100.00')
            base_cost += bulky_item_rate * Decimal(str(storage_req.num_bulky_items))
            
        return base_cost

    async def log_storage_calculation(
        self,
        db: Session,
        storage_req: StorageRequirements,
        cost: Decimal,
        user_id: int
    ) -&gt; None:
        """Log storage cost calculation."""
        log_event(
            db=db,
            event_type="storage_cost_calculated",
            user_id=user_id,
            resource_type="storage",
            resource_id="calculation",
            details={
                "requirements": storage_req.model_dump(),
                "calculated_cost": str(cost)
            }
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\business\storage.py</data>
</node>
<node id="three_pl">
  <data key="d0">module</data>
  <data key="d1">"""
Third-party logistics (3PL) service for managing warehouse operations.
"""

from typing import Optional, Dict, Any
from decimal import Decimal
from sqlalchemy.orm import Session

from app.core.monitoring import log_event
from app.schemas.three_pl import ThreePLServices
from app.services.base import BaseService

class ThreePLService(BaseService):
    """Service for managing 3PL operations."""

    async def calculate_3pl_cost(
        self,
        three_pl: ThreePLServices,
        db: Session
    ) -&gt; Decimal:
        """Calculate 3PL services cost."""
        if not three_pl:
            return Decimal('0')

        total_cost = Decimal('0')
        
        # Unpacking costs
        if three_pl.unpacking:
            container_costs = {
                "20ft": Decimal('200.00'),
                "40ft": Decimal('350.00'),
                "40ft HC": Decimal('400.00')
            }
            total_cost += container_costs.get(
                three_pl.container_size,
                Decimal('200.00')  # Default to 20ft container cost
            )
            
        # Barcode scanning
        if three_pl.barcode_scanning:
            total_cost += Decimal('50.00')
            
        # Cargo labelling
        if three_pl.cargo_labelling:
            total_cost += Decimal('30.00')
            
        # Order picking
        if three_pl.order_picking:
            total_cost += Decimal('75.00')
            
        return total_cost

    async def log_3pl_calculation(
        self,
        db: Session,
        three_pl: ThreePLServices,
        cost: Decimal,
        user_id: int
    ) -&gt; None:
        """Log 3PL cost calculation."""
        log_event(
            db=db,
            event_type="3pl_cost_calculated",
            user_id=user_id,
            resource_type="3pl",
            resource_id="calculation",
            details={
                "services": three_pl.model_dump(),
                "calculated_cost": str(cost)
            }
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\business\three_pl.py</data>
</node>
<node id="transport">
  <data key="d0">module</data>
  <data key="d1">"""
Transport service for managing logistics operations.
"""

from typing import Optional, Dict, Any
from decimal import Decimal
from sqlalchemy.orm import Session

from app.core.monitoring import log_event
from app.schemas.transport import TransportServices
from app.services.base import BaseService

class TransportService(BaseService):
    """Service for managing transport operations."""

    async def calculate_transport_cost(
        self,
        transport: TransportServices,
        db: Session
    ) -&gt; Decimal:
        """Calculate transport cost."""
        if not transport or not transport.destination_postcode:
            return Decimal('0')

        # Base rate per shipment
        base_rate = Decimal('100.00')
        
        # Calculate distance factor based on postcode
        # This is a simplified example - in reality, you'd use a proper distance calculation
        distance_factor = self._calculate_distance_factor(transport.destination_postcode)
        
        # Calculate number of shipments
        num_shipments = transport.num_shipments or 1
        
        # Calculate total cost
        total_cost = base_rate * Decimal(str(distance_factor)) * Decimal(str(num_shipments))
        
        return total_cost

    def _calculate_distance_factor(self, postcode: str) -&gt; float:
        """Calculate distance factor based on postcode."""
        # This is a simplified example
        # In reality, you'd use a proper distance calculation service
        try:
            postcode_num = int(postcode)
            if postcode_num &lt; 2000:  # Sydney metro
                return 1.0
            elif postcode_num &lt; 3000:  # NSW regional
                return 1.5
            elif postcode_num &lt; 4000:  # VIC
                return 2.0
            elif postcode_num &lt; 5000:  # QLD
                return 2.5
            else:  # Other states
                return 3.0
        except ValueError:
            return 1.0  # Default to metro rate if invalid postcode

    async def log_transport_calculation(
        self,
        db: Session,
        transport: TransportServices,
        cost: Decimal,
        user_id: int
    ) -&gt; None:
        """Log transport cost calculation."""
        log_event(
            db=db,
            event_type="transport_cost_calculated",
            user_id=user_id,
            resource_type="transport",
            resource_id="calculation",
            details={
                "services": transport.model_dump(),
                "calculated_cost": str(cost)
            }
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\business\transport.py</data>
</node>
<node id="email">
  <data key="d0">module</data>
  <data key="d1">"""
Email service for sending various types of emails.
"""

from typing import List, Optional, Dict, Any
from pathlib import Path
import jinja2
from fastapi_mail import FastMail, MessageSchema, ConnectionConfig
from pydantic import EmailStr

from app.core.config import settings
from app.core.monitoring import log_event
from app.models.quote import Quote
from app.models.user import User

class EmailService:
    """Service for sending emails."""

    def __init__(self):
        """Initialize email service with configuration."""
        self.config = ConnectionConfig(
            MAIL_USERNAME=settings.MAIL_USERNAME,
            MAIL_PASSWORD=settings.MAIL_PASSWORD,
            MAIL_FROM=settings.MAIL_FROM,
            MAIL_PORT=settings.MAIL_PORT,
            MAIL_SERVER=settings.MAIL_SERVER,
            MAIL_TLS=settings.MAIL_TLS,
            MAIL_SSL=settings.MAIL_SSL,
            TEMPLATE_FOLDER=Path(__file__).parent / "templates"
        )
        self.fastmail = FastMail(self.config)
        self.template_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(str(self.config.TEMPLATE_FOLDER))
        )

    async def send_email(
        self,
        email_to: List[EmailStr],
        subject: str,
        template_name: str,
        template_data: Dict[str, Any],
        cc: Optional[List[EmailStr]] = None,
        bcc: Optional[List[EmailStr]] = None
    ) -&gt; None:
        """Send an email using a template."""
        template = self.template_env.get_template(template_name)
        html = template.render(**template_data)
        
        message = MessageSchema(
            subject=subject,
            recipients=email_to,
            cc=cc or [],
            bcc=bcc or [],
            body=html,
            subtype="html"
        )
        
        await self.fastmail.send_message(message)

    async def send_quote_email(
        self,
        email_to: List[EmailStr],
        quote_data: Dict[str, Any],
        user_id: int
    ) -&gt; None:
        """Send a quote email to a customer."""
        await self.send_email(
            email_to=email_to,
            subject="Your Quote is Ready",
            template_name="quote.html",
            template_data=quote_data
        )
        
        log_event(
            event_type="quote_email_sent",
            user_id=user_id,
            resource_type="email",
            resource_id=quote_data["quote_id"],
            details={"recipients": email_to}
        )

    async def send_rate_update_notification(
        self,
        email_to: List[EmailStr],
        rate_changes: Dict[str, Any],
        user_id: int
    ) -&gt; None:
        """Send notification about rate changes."""
        await self.send_email(
            email_to=email_to,
            subject="Rate Card Updates",
            template_name="rate_update.html",
            template_data=rate_changes
        )
        
        log_event(
            event_type="rate_update_email_sent",
            user_id=user_id,
            resource_type="email",
            resource_id=None,
            details={"changes": rate_changes}
        )

    async def send_welcome_email(
        self,
        user: User
    ) -&gt; None:
        """Send welcome email to new users."""
        template_data = {
            "username": user.username,
            "first_name": user.first_name,
            "login_url": f"{settings.FRONTEND_URL}/login"
        }
        
        await self.send_email(
            email_to=[user.email],
            subject="Welcome to AUL Quote App",
            template_name="welcome.html",
            template_data=template_data
        )
        
        log_event(
            event_type="welcome_email_sent",
            user_id=user.id,
            resource_type="email",
            resource_id=None,
            details={"email": user.email}
        )

    async def send_quote_confirmation(
        self,
        quote: Quote,
        user: User
    ) -&gt; None:
        """Send quote confirmation email."""
        template_data = {
            "quote_id": quote.id,
            "customer_name": quote.customer.name,
            "total_amount": quote.total_amount,
            "services": quote.services,
            "quote_url": f"{settings.FRONTEND_URL}/quotes/{quote.id}"
        }
        
        await self.send_email(
            email_to=[user.email],
            subject=f"Quote Confirmation #{quote.id}",
            template_name="quote_confirmation.html",
            template_data=template_data
        )
        
        log_event(
            event_type="quote_confirmation_sent",
            user_id=user.id,
            resource_type="email",
            resource_id=str(quote.id),
            details={"email": user.email}
        )

    async def send_admin_notification(
        self,
        admin_email: EmailStr,
        subject: str,
        notification_data: Dict[str, Any]
    ) -&gt; None:
        """Send notification to admin."""
        await self.send_email(
            email_to=[admin_email],
            subject=subject,
            template_name="admin_notification.html",
            template_data=notification_data
        )
        
        log_event(
            event_type="admin_notification_sent",
            user_id=None,
            resource_type="email",
            resource_id=None,
            details={"email": admin_email, "subject": subject}
        )

    async def send_password_reset(
        self,
        user: User,
        reset_token: str
    ) -&gt; None:
        """Send password reset email."""
        template_data = {
            "username": user.username,
            "reset_url": f"{settings.FRONTEND_URL}/reset-password?token={reset_token}"
        }
        
        await self.send_email(
            email_to=[user.email],
            subject="Password Reset Request",
            template_name="password_reset.html",
            template_data=template_data
        )
        
        log_event(
            event_type="password_reset_sent",
            user_id=user.id,
            resource_type="email",
            resource_id=None,
            details={"email": user.email}
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\communication\email.py</data>
</node>
<node id="realtime">
  <data key="d0">module</data>
  <data key="d1">"""
Realtime service for WebSocket communication.
"""

from typing import Dict, Any, List, Optional
from fastapi import WebSocket
import json

from app.core.monitoring import log_event
from app.schemas.websocket import WebSocketMessage

class RealtimeService:
    """Service for managing realtime WebSocket connections."""

    def __init__(self):
        """Initialize realtime service."""
        self.active_connections: Dict[int, List[WebSocket]] = {}
        self.user_channels: Dict[int, List[str]] = {}

    async def connect(
        self,
        websocket: WebSocket,
        user_id: int,
        channels: Optional[List[str]] = None
    ) -&gt; None:
        """Connect a user to WebSocket."""
        await websocket.accept()
        
        if user_id not in self.active_connections:
            self.active_connections[user_id] = []
        self.active_connections[user_id].append(websocket)
        
        if channels:
            self.user_channels[user_id] = channels
        
        log_event(
            event_type="websocket_connected",
            user_id=user_id,
            resource_type="websocket",
            resource_id=None,
            details={"channels": channels}
        )

    async def disconnect(
        self,
        websocket: WebSocket,
        user_id: int
    ) -&gt; None:
        """Disconnect a user from WebSocket."""
        if user_id in self.active_connections:
            self.active_connections[user_id].remove(websocket)
            if not self.active_connections[user_id]:
                del self.active_connections[user_id]
                if user_id in self.user_channels:
                    del self.user_channels[user_id]
        
        log_event(
            event_type="websocket_disconnected",
            user_id=user_id,
            resource_type="websocket",
            resource_id=None,
            details={}
        )

    async def broadcast_to_users(
        self,
        users: List[int],
        message: WebSocketMessage
    ) -&gt; None:
        """Broadcast message to specific users."""
        for user_id in users:
            if user_id in self.active_connections:
                for connection in self.active_connections[user_id]:
                    try:
                        await connection.send_text(message.model_dump_json())
                    except Exception as e:
                        await self.handle_connection_error(connection, user_id, e)

    async def broadcast_to_channel(
        self,
        channel: str,
        message: WebSocketMessage
    ) -&gt; None:
        """Broadcast message to all users in a channel."""
        for user_id, channels in self.user_channels.items():
            if channel in channels:
                await self.broadcast_to_users([user_id], message)

    async def handle_connection_error(
        self,
        websocket: WebSocket,
        user_id: int,
        error: Exception
    ) -&gt; None:
        """Handle WebSocket connection errors."""
        log_event(
            event_type="websocket_error",
            user_id=user_id,
            resource_type="websocket",
            resource_id=None,
            details={"error": str(error)}
        )
        await self.disconnect(websocket, user_id)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\communication\realtime.py</data>
</node>
<node id="chat_service">
  <data key="d0">module</data>
  <data key="d1">"""
Multi-turn conversation service for quote generation with memory management.
"""

from typing import Dict, Any, Optional, List
from datetime import datetime
import uuid
from decimal import Decimal

from app.core.config import settings
from app.services.llm.model import FLANT5Service
from app.services.rate_calculator import RateCalculator
from app.models.conversation import LLMMessage
from app.models.memory import ConversationMemory
from app.schemas.conversation import ChatSession, QuoteContext
from app.database import Session

class ChatService:
    def __init__(self):
        self.llm = FLANT5Service()
        self.sessions: Dict[str, ChatSession] = {}
        
    async def create_session(self, user_id: int, db: Session) -&gt; str:
        """Create a new chat session with memory management."""
        session_id = str(uuid.uuid4())
        
        # Initialize session with system prompt
        system_prompt = """You are a warehouse quote assistant. Your goal is to:
        1. Collect all necessary information for generating a quote
        2. Use the rate calculator to ensure accurate pricing
        3. Help users modify their quotes by adding/removing services
        4. Only offer discounts up to 10% when explicitly asked, and only on specific services
        5. Maintain context of the conversation and remember key details
        
        Important rules:
        - Always verify service requirements before generating a quote
        - Confirm quantity and duration for each service
        - Ask for clarification when information is ambiguous
        - Keep track of any offered discounts
        - Remember client preferences for future reference"""
        
        # Create session with short-term memory (current conversation)
        self.sessions[session_id] = ChatSession(
            id=session_id,
            user_id=user_id,
            messages=[],
            context=QuoteContext(
                system_prompt=system_prompt,
                current_quote=None,
                collected_info={},
                offered_discounts={}
            ),
            created_at=datetime.utcnow()
        )
        
        # Initialize long-term memory
        memory = ConversationMemory(
            user_id=user_id,
            preferences={},
            service_history=[],
            common_requirements={}
        )
        db.add(memory)
        db.commit()
        
        return session_id
        
    async def send_message(
        self,
        session_id: str,
        content: str,
        db: Session
    ) -&gt; Dict[str, Any]:
        """Process a message with memory context."""
        if session_id not in self.sessions:
            raise ValueError("Invalid session ID")
            
        session = self.sessions[session_id]
        
        # Get long-term memory
        memory = db.query(ConversationMemory)\
            .filter(ConversationMemory.user_id == session.user_id)\
            .first()
        
        # Prepare context for LLM
        context = {
            "current_conversation": session.messages[-5:],  # Short-term memory
            "collected_info": session.context.collected_info,
            "current_quote": session.context.current_quote,
            "preferences": memory.preferences,  # Long-term memory
            "service_history": memory.service_history,
            "common_requirements": memory.common_requirements
        }
        
        # Generate response
        response = await self.llm.generate_response(
            prompt=content,
            system_prompt=session.context.system_prompt,
            context=context
        )
        
        # Extract and save any new information
        if response.extracted_info:
            session.context.collected_info.update(response.extracted_info)
        
        # Update long-term memory if new preferences or requirements are identified
        if response.new_preferences:
            memory.preferences.update(response.new_preferences)
            db.commit()
        
        # Generate quote if all required information is collected
        if response.generate_quote and session.context.collected_info:
            calculator = RateCalculator(db)
            quote = await calculator.generate_quote(
                user_id=session.user_id,
                services=session.context.collected_info,
                discounts=session.context.offered_discounts
            )
            session.context.current_quote = quote
            response.quote_details = quote
        
        # Add messages to conversation history
        user_message = LLMMessage(
            content=content,
            role="user",
            timestamp=datetime.utcnow()
        )
        assistant_message = LLMMessage(
            content=response.content,
            role="assistant",
            timestamp=datetime.utcnow(),
            metadata={
                "extracted_info": response.extracted_info,
                "quote_details": response.quote_details,
                "suggestions": response.suggestions
            }
        )
        session.messages.extend([user_message, assistant_message])
        
        return {
            "content": response.content,
            "quote_details": response.quote_details,
            "suggestions": response.suggestions
        }
        
    async def update_quote(
        self,
        session_id: str,
        updates: Dict[str, Any],
        db: Session
    ) -&gt; Dict[str, Any]:
        """Update an existing quote based on conversation."""
        if session_id not in self.sessions:
            raise ValueError("Invalid session ID")
            
        session = self.sessions[session_id]
        if not session.context.current_quote:
            raise ValueError("No active quote to update")
            
        # Apply updates to collected information
        session.context.collected_info.update(updates)
        
        # Recalculate quote
        calculator = RateCalculator(db)
        quote = await calculator.generate_quote(
            user_id=session.user_id,
            services=session.context.collected_info,
            discounts=session.context.offered_discounts
        )
        session.context.current_quote = quote
        
        return {"quote": quote}
        
    async def apply_discount(
        self,
        session_id: str,
        service_id: str,
        discount_percentage: Decimal,
        db: Session
    ) -&gt; bool:
        """Apply a discount to a specific service."""
        if session_id not in self.sessions:
            raise ValueError("Invalid session ID")
            
        if discount_percentage &gt; Decimal("10.0"):
            return False
            
        session = self.sessions[session_id]
        session.context.offered_discounts[service_id] = discount_percentage
        return True
        
    def get_session_context(self, session_id: str) -&gt; Dict[str, Any]:
        """Get the current session context."""
        if session_id not in self.sessions:
            raise ValueError("Invalid session ID")
            
        session = self.sessions[session_id]
        return {
            "collected_info": session.context.collected_info,
            "current_quote": session.context.current_quote,
            "offered_discounts": session.context.offered_discounts
        }
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\llm\chat_service.py</data>
</node>
<node id="model">
  <data key="d0">module</data>
  <data key="d1">"""
FLAN-T5 model service for warehouse quote processing.
"""

import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from typing import Dict, List, Optional, Union
import json
from pathlib import Path

class WarehouseLLM:
    def __init__(self, model_name: str = "google/flan-t5-base"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Load rate card templates and prompts
        self.templates_path = Path(__file__).parent / "templates"
        self.load_templates()
        
    def load_templates(self):
        """Load prompt templates and examples"""
        with open(self.templates_path / "prompts.json", "r") as f:
            self.templates = json.load(f)
            
    def generate_response(
        self,
        input_text: str,
        max_length: int = 512,
        template_key: str = "general",
        context: Optional[Dict] = None
    ) -&gt; str:
        """Generate response using FLAN-T5"""
        # Get template and format with context
        template = self.templates[template_key]
        if context:
            template = template.format(**context)
            
        # Combine template and input
        prompt = f"{template}\nUser: {input_text}\nAssistant:"
        
        # Generate response
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model.generate(
            **inputs,
            max_length=max_length,
            num_beams=4,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            early_stopping=True
        )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.strip()
    
    def extract_rate_info(self, conversation_history: List[Dict]) -&gt; Dict:
        """Extract rate-relevant information from conversation"""
        # Format conversation history
        formatted_history = "\n".join([
            f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
            for msg in conversation_history
        ])
        
        # Use specific template for rate extraction
        response = self.generate_response(
            formatted_history,
            template_key="rate_extraction",
            max_length=256
        )
        
        try:
            # Response should be JSON format
            rate_info = json.loads(response)
            return rate_info
        except json.JSONDecodeError:
            return {}
            
    def validate_rate_card(self, rate_card: Dict) -&gt; Dict[str, Union[bool, List[str]]]:
        """Validate rate card configuration"""
        rate_card_str = json.dumps(rate_card, indent=2)
        response = self.generate_response(
            rate_card_str,
            template_key="rate_validation",
            max_length=512
        )
        
        try:
            validation_result = json.loads(response)
            return validation_result
        except json.JSONDecodeError:
            return {
                "valid": False,
                "errors": ["Failed to parse validation response"]
            }
            
    def suggest_rate_improvements(self, rate_card: Dict, historical_data: List[Dict]) -&gt; List[str]:
        """Suggest improvements for rate card based on historical data"""
        context = {
            "rate_card": json.dumps(rate_card, indent=2),
            "historical_data": json.dumps(historical_data, indent=2)
        }
        
        response = self.generate_response(
            "",
            template_key="rate_improvement",
            context=context,
            max_length=1024
        )
        
        try:
            suggestions = json.loads(response)
            return suggestions if isinstance(suggestions, list) else []
        except json.JSONDecodeError:
            return []
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\llm\model.py</data>
</node>
<node id="rag">
  <data key="d0">module</data>
  <data key="d1">"""
RAG (Retrieval Augmented Generation) service for warehouse quotes.
"""

from typing import List, Dict, Optional
import json
from pathlib import Path
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
from datetime import datetime
import re

class RAGService:
    def __init__(self, knowledge_base_path: Optional[str] = None):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.knowledge_base_path = Path(knowledge_base_path) if knowledge_base_path else Path(__file__).parent / "knowledge"
        self.rag_doc_path = Path(__file__).parent.parent.parent.parent / "rag_document.txt"
        self.load_knowledge_base()
        
    def load_knowledge_base(self):
        """Load and index the knowledge base"""
        # Load JSON documents
        with open(self.knowledge_base_path / "warehouse_knowledge.json", "r") as f:
            self.documents = json.load(f)
            
        # Load and parse RAG document
        with open(self.rag_doc_path, "r") as f:
            rag_content = f.read()
            
        # Parse sections from RAG document
        sections = self._parse_rag_document(rag_content)
        
        # Combine both sources
        for section in sections:
            self.documents["documents"].append({
                "id": f"rag_{section['title'].lower().replace(' ', '_')}",
                "category": section["category"],
                "content": section["content"],
                "updated_at": datetime.utcnow().isoformat()
            })
            
        # Create FAISS index
        self.texts = [doc["content"] for doc in self.documents["documents"]]
        embeddings = self.encoder.encode(self.texts)
        
        # Initialize FAISS index
        self.index = faiss.IndexFlatL2(embeddings.shape[1])
        self.index.add(np.array(embeddings).astype('float32'))
        
    def _parse_rag_document(self, content: str) -&gt; List[Dict]:
        """Parse RAG document into sections"""
        sections = []
        current_section = None
        current_content = []
        
        for line in content.split('\n'):
            # Main section (h1)
            if line.startswith('# '):
                if current_section:
                    sections.append({
                        "title": current_section,
                        "category": current_section.lower().replace(' ', '_'),
                        "content": '\n'.join(current_content)
                    })
                current_section = line[2:].strip()
                current_content = []
            # Subsection (h2)
            elif line.startswith('## '):
                if current_content:
                    sections.append({
                        "title": current_section + " - " + line[3:].strip(),
                        "category": current_section.lower().replace(' ', '_'),
                        "content": '\n'.join(current_content)
                    })
                current_content = []
            else:
                current_content.append(line)
                
        # Add last section
        if current_section and current_content:
            sections.append({
                "title": current_section,
                "category": current_section.lower().replace(' ', '_'),
                "content": '\n'.join(current_content)
            })
            
        return sections
            
    def get_relevant_context(self, query: str, k: int = 3) -&gt; List[Dict]:
        """Retrieve relevant documents for a query"""
        # Encode query
        query_embedding = self.encoder.encode([query])
        
        # Search index
        distances, indices = self.index.search(
            np.array(query_embedding).astype('float32'), 
            k
        )
        
        # Return relevant documents with scores
        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            doc = self.documents["documents"][idx]
            results.append({
                **doc,
                "relevance_score": float(1 / (1 + dist))  # Convert distance to similarity score
            })
            
        return sorted(results, key=lambda x: x["relevance_score"], reverse=True)
        
    def get_rate_card_context(self, service_type: str, include_edge_cases: bool = True) -&gt; List[Dict]:
        """Get relevant rate card information"""
        queries = [
            f"rate calculation {service_type}",
            f"pricing {service_type}",
            "rate card validation"
        ]
        
        if include_edge_cases:
            queries.append("edge cases handling")
            
        all_results = []
        for query in queries:
            results = self.get_relevant_context(query, k=2)
            all_results.extend(results)
            
        # Remove duplicates and sort by relevance
        seen = set()
        unique_results = []
        for doc in all_results:
            if doc["id"] not in seen:
                seen.add(doc["id"])
                unique_results.append(doc)
                
        return sorted(unique_results, key=lambda x: x["relevance_score"], reverse=True)[:4]
        
    def get_customer_service_guidelines(self, specific_topic: Optional[str] = None) -&gt; List[Dict]:
        """Get customer service guidelines"""
        query = "customer service guidelines"
        if specific_topic:
            query += f" {specific_topic}"
            
        return self.get_relevant_context(query, k=2)
        
    def format_context_for_llm(self, documents: List[Dict], max_length: int = 2048) -&gt; str:
        """Format retrieved documents for LLM prompt"""
        formatted = []
        current_length = 0
        
        for doc in sorted(documents, key=lambda x: x.get("relevance_score", 0), reverse=True):
            content = f"Category: {doc['category']}\n{doc['content']}"
            content_length = len(content)
            
            if current_length + content_length &gt; max_length:
                # Truncate content to fit
                available_length = max_length - current_length
                if available_length &gt; 100:  # Only add if we can include meaningful content
                    content = content[:available_length] + "..."
                    formatted.append(content)
                break
                
            formatted.append(content)
            current_length += content_length
            
        return "\n\n".join(formatted)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\llm\rag.py</data>
</node>
<node id="rate_integration">
  <data key="d0">module</data>
  <data key="d1">"""
Integration service between LLM and rate service.
"""

from typing import Dict, List, Optional, Any
from datetime import datetime
from sqlalchemy.orm import Session

from app.services.business.rates import RateService
from .model import WarehouseLLM
from .rag import RAGService
from app.schemas.rate.rate import RateCreate, RateUpdate, RateCategory
from app.core.monitoring import log_event

class RateIntegrationService:
    """Service for rate optimization and integration with LLM."""

    def __init__(
        self,
        db: Session,
        llm: Optional[WarehouseLLM] = None,
        rag: Optional[RAGService] = None
    ):
        """Initialize with optional dependencies."""
        self.db = db
        self.llm = llm or WarehouseLLM()
        self.rag = rag or RAGService()
        self.rate_service = RateService(db, llm, rag)

    async def process_quote_request(
        self,
        conversation_history: List[Dict],
        current_context: Dict
    ) -&gt; Dict:
        """Process a quote request using LLM and rate service."""
        # Get relevant rate card context
        service_type = current_context.get("service_type")
        rate_context = self.rag.get_rate_card_context(
            conversation_history,
            service_type
        )
        
        # Extract rate information
        rate_info = self.llm.extract_rate_info(rate_context)
        
        # Calculate rates
        try:
            rates = await self.rate_service.get_rates(
                category=rate_info.get("service_type")
            )
            
            # Get quote-specific suggestions
            suggestions = await self._get_quote_suggestions(
                current_context,
                rate_info
            )
            
            return {
                "rates": rates,
                "suggestions": suggestions,
                "context": rate_context
            }
            
        except Exception as e:
            log_event(
                db=self.db,
                event_type="quote_request_error",
                details={"error": str(e)}
            )
            raise

    async def validate_rate_card(
        self,
        rate_card: Dict
    ) -&gt; Dict[str, Any]:
        """Validate rate card using LLM."""
        return await self.rate_service.validate_rate(rate_card)

    async def optimize_rate_card(
        self,
        rate_card: Dict
    ) -&gt; Dict[str, Any]:
        """Optimize rate card using LLM."""
        # Get optimization suggestions
        optimization_params = {
            "time_range": 90
        }
        
        result = await self.rate_service.optimize_rate(
            rate_card["id"],
            optimization_params
        )
        
        return result

    async def _get_quote_suggestions(
        self,
        quote_details: Dict,
        rate_info: Dict
    ) -&gt; List[str]:
        """Get quote-specific suggestions."""
        context = {
            "quote": quote_details,
            "rate_info": rate_info
        }
        return await self.llm.get_quote_suggestions(context)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\llm\rate_integration.py</data>
</node>
<node id="metrics">
  <data key="d0">module</data>
  <data key="d1">"""
Service for collecting and managing application metrics.
"""

from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func
from prometheus_client import Counter, Histogram, Gauge

from app.models.quote import Quote
from app.models.rate import Rate
from app.models.user import User
from app.core.monitoring import log_event
from app.database import (
    get_total_revenue,
    get_number_of_quotes_accepted,
    get_number_of_quotes_created,
    get_total_handling_fees
)

class MetricsService:
    """Service for collecting and managing application metrics."""

    def __init__(self):
        """Initialize metrics collectors."""
        # Prometheus metrics
        self.quote_counter = Counter(
            'quotes_total',
            'Total number of quotes generated',
            ['status']
        )
        self.quote_amount = Histogram(
            'quote_amount_dollars',
            'Distribution of quote amounts',
            buckets=[100, 500, 1000, 5000, 10000, 50000]
        )
        self.active_users = Gauge(
            'active_users',
            'Number of active users'
        )

    async def collect_quote_metrics(
        self,
        db: Session,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -&gt; Dict[str, Any]:
        """Collect quote-related metrics."""
        if not start_date:
            start_date = datetime.utcnow() - timedelta(days=30)
        if not end_date:
            end_date = datetime.utcnow()

        # Query metrics
        total_quotes = db.query(func.count(Quote.id)).scalar()
        avg_quote_amount = db.query(func.avg(Quote.total_amount)).scalar()
        quotes_by_status = (
            db.query(Quote.status, func.count(Quote.id))
            .group_by(Quote.status)
            .all()
        )
        
        # Update Prometheus metrics
        for status, count in quotes_by_status:
            self.quote_counter.labels(status=status).inc(count)
        
        metrics = {
            "total_quotes": total_quotes,
            "average_quote_amount": float(avg_quote_amount) if avg_quote_amount else 0,
            "quotes_by_status": dict(quotes_by_status),
            "period": {
                "start": start_date,
                "end": end_date
            }
        }
        
        log_event(
            db=db,
            event_type="metrics_collected",
            user_id=None,
            resource_type="metrics",
            resource_id="quotes",
            details=metrics
        )
        
        return metrics

    async def collect_rate_metrics(
        self,
        db: Session
    ) -&gt; Dict[str, Any]:
        """Collect rate-related metrics."""
        total_rates = db.query(func.count(Rate.id)).scalar()
        active_rates = (
            db.query(func.count(Rate.id))
            .filter(Rate.is_active == True)
            .scalar()
        )
        rates_by_category = (
            db.query(Rate.category, func.count(Rate.id))
            .group_by(Rate.category)
            .all()
        )
        
        metrics = {
            "total_rates": total_rates,
            "active_rates": active_rates,
            "rates_by_category": dict(rates_by_category)
        }
        
        log_event(
            db=db,
            event_type="metrics_collected",
            user_id=None,
            resource_type="metrics",
            resource_id="rates",
            details=metrics
        )
        
        return metrics

    async def collect_user_metrics(
        self,
        db: Session,
        days: int = 30
    ) -&gt; Dict[str, Any]:
        """Collect user-related metrics."""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        total_users = db.query(func.count(User.id)).scalar()
        active_users = (
            db.query(func.count(User.id))
            .filter(User.last_login &gt;= cutoff_date)
            .scalar()
        )
        users_by_role = (
            db.query(User.role, func.count(User.id))
            .group_by(User.role)
            .all()
        )
        
        # Update Prometheus metrics
        self.active_users.set(active_users)
        
        metrics = {
            "total_users": total_users,
            "active_users": active_users,
            "users_by_role": dict(users_by_role),
            "period_days": days
        }
        
        log_event(
            db=db,
            event_type="metrics_collected",
            user_id=None,
            resource_type="metrics",
            resource_id="users",
            details=metrics
        )
        
        return metrics

    async def get_system_health(
        self,
        db: Session
    ) -&gt; Dict[str, Any]:
        """Get overall system health metrics."""
        # Collect all metrics
        quote_metrics = await self.collect_quote_metrics(db)
        rate_metrics = await self.collect_rate_metrics(db)
        user_metrics = await self.collect_user_metrics(db)
        
        return {
            "quotes": quote_metrics,
            "rates": rate_metrics,
            "users": user_metrics,
            "timestamp": datetime.utcnow()
        }

    async def get_quote_metrics(
        self,
        db: Session,
        start_date: datetime,
        end_date: datetime
    ) -&gt; Dict[str, Any]:
        """Get quote-related metrics for a date range."""
        return {
            "average_quote_value": await self.calculate_average_quote_value(db),
            "total_handling_fees": await self.calculate_total_handling_fees(db),
            "quote_acceptance_rate": await self.calculate_quote_acceptance_rate(db),
            "total_quotes": await self.get_number_of_quotes_created(db),
            "accepted_quotes": await self.get_number_of_quotes_accepted(db),
            "total_revenue": await self.get_total_revenue(db)
        }

    async def calculate_average_quote_value(self, db: Session) -&gt; float:
        """Calculates the average quote value."""
        total_revenue = await self.get_total_revenue(db)
        number_of_quotes_accepted = await self.get_number_of_quotes_accepted(db)
        if number_of_quotes_accepted == 0:
            return 0
        return total_revenue / number_of_quotes_accepted

    async def calculate_total_handling_fees(self, db: Session) -&gt; float:
        """Calculates the total handling fees from accepted quotes."""
        return await get_total_handling_fees(db)

    async def calculate_quote_acceptance_rate(self, db: Session) -&gt; float:
        """Calculates the quote acceptance rate."""
        number_of_quotes_accepted = await self.get_number_of_quotes_accepted(db)
        number_of_quotes_created = await self.get_number_of_quotes_created(db)
        if number_of_quotes_created == 0:
            return 0
        return number_of_quotes_accepted / number_of_quotes_created

    async def get_total_revenue(self, db: Session) -&gt; float:
        """Get total revenue from accepted quotes."""
        return await get_total_revenue(db)

    async def get_number_of_quotes_accepted(self, db: Session) -&gt; int:
        """Get number of accepted quotes."""
        return await get_number_of_quotes_accepted(db)

    async def get_number_of_quotes_created(self, db: Session) -&gt; int:
        """Get total number of quotes created."""
        return await get_number_of_quotes_created(db)

    async def log_metrics(
        self,
        db: Session,
        metrics: Dict[str, Any],
        user_id: int
    ) -&gt; None:
        """Log metrics collection event."""
        log_event(
            db=db,
            event_type="metrics_collected",
            user_id=user_id,
            resource_type="metrics",
            resource_id="quote_metrics",
            details=metrics
        )
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\metrics\metrics.py</data>
</node>
<node id="validation">
  <data key="d0">module</data>
  <data key="d1">"""Validation service for quotes and rates."""

from datetime import datetime
from typing import Dict, Any, Optional, List, Union
from pydantic import BaseModel, ValidationError, Field, validator
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.monitoring import logger
from app.schemas.quote import QuoteCreate, QuoteUpdate
from app.schemas.rate import RateCreate, RateUpdate
from app.models.rate import Rate
from app.core.exceptions import ValidationException

class ValidationResult(BaseModel):
    """Validation result model."""
    is_valid: bool = Field(default=False, description="Whether validation passed")
    errors: List[str] = Field(default_factory=list, description="List of validation errors")
    warnings: List[str] = Field(default_factory=list, description="List of validation warnings")
    suggestions: List[str] = Field(default_factory=list, description="List of improvement suggestions")

class StorageRequirements(BaseModel):
    """Storage requirements validation model."""
    floor_area: float = Field(..., gt=0, description="Floor area in square meters")
    length: Optional[float] = Field(None, gt=0, description="Length in meters")
    width: Optional[float] = Field(None, gt=0, description="Width in meters")
    height: Optional[float] = Field(None, gt=0, description="Height in meters")

    @validator("floor_area")
    def validate_floor_area(cls, v: float) -&gt; float:
        """Validate floor area is reasonable."""
        if v &gt; 10000:  # 10,000 square meters
            raise ValueError("Floor area seems unreasonably large")
        return v

class TransportServices(BaseModel):
    """Transport services validation model."""
    destination_postcode: str = Field(..., min_length=4, max_length=10)
    num_shipments: int = Field(..., gt=0, le=1000)

    @validator("destination_postcode")
    def validate_postcode(cls, v: str) -&gt; str:
        """Validate postcode format."""
        if not v.replace(" ", "").isalnum():
            raise ValueError("Postcode must contain only letters and numbers")
        return v

class ValidationService:
    """Service for validating quotes and rates."""

    async def validate_rate(
        self,
        rate_data: Union[RateCreate, RateUpdate]
    ) -&gt; ValidationResult:
        """Validate rate data against schema and business rules.
        
        Args:
            rate_data: Rate data to validate
            
        Returns:
            ValidationResult: Validation results including errors and suggestions
            
        Raises:
            ValidationException: If validation fails with critical errors
        """
        result = ValidationResult()
        
        try:
            # Schema validation is handled by Pydantic
            if isinstance(rate_data, RateUpdate):
                # For updates, we need to check if any field is set
                if not any(
                    getattr(rate_data, field) is not None
                    for field in rate_data.__fields__
                ):
                    result.errors.append("At least one field must be provided for update")
                    return result
            
            # Business rules validation
            if hasattr(rate_data, "valid_from") and hasattr(rate_data, "valid_until"):
                if rate_data.valid_from and rate_data.valid_until:
                    if rate_data.valid_from &gt;= rate_data.valid_until:
                        result.errors.append(
                            "Valid from date must be before valid until date"
                        )
            
            if hasattr(rate_data, "rate") and rate_data.rate is not None:
                if rate_data.rate &lt;= 0:
                    result.errors.append("Rate must be greater than 0")
                elif rate_data.rate &gt; 10000:
                    result.warnings.append(
                        "Rate seems unusually high, please verify"
                    )
            
            # Set validation status
            result.is_valid = len(result.errors) == 0
            
            # Add suggestions if appropriate
            if result.is_valid and len(result.warnings) &gt; 0:
                result.suggestions.append(
                    "Consider reviewing the warnings before proceeding"
                )
            
        except ValidationError as e:
            result.errors.extend(str(err) for err in e.errors())
            logger.error(f"Rate validation error: {e}")
        
        return result

    async def validate_quote(
        self,
        quote_data: Union[QuoteCreate, QuoteUpdate]
    ) -&gt; ValidationResult:
        """Validate quote data against schema and business rules.
        
        Args:
            quote_data: Quote data to validate
            
        Returns:
            ValidationResult: Validation results including errors and suggestions
            
        Raises:
            ValidationException: If validation fails with critical errors
        """
        result = ValidationResult()
        
        try:
            # Schema validation is handled by Pydantic
            if isinstance(quote_data, QuoteUpdate):
                if not any(
                    getattr(quote_data, field) is not None
                    for field in quote_data.__fields__
                ):
                    result.errors.append("At least one field must be provided for update")
                    return result
            
            # Business rules validation
            if hasattr(quote_data, "storage_requirements"):
                try:
                    StorageRequirements(**quote_data.storage_requirements)
                except ValidationError as e:
                    result.errors.extend(
                        f"Storage requirements: {err}" for err in e.errors()
                    )
            
            if hasattr(quote_data, "transport_services"):
                try:
                    TransportServices(**quote_data.transport_services)
                except ValidationError as e:
                    result.errors.extend(
                        f"Transport services: {err}" for err in e.errors()
                    )
            
            # Set validation status
            result.is_valid = len(result.errors) == 0
            
            # Add suggestions
            if result.is_valid:
                if hasattr(quote_data, "notes") and not quote_data.notes:
                    result.suggestions.append(
                        "Consider adding notes for better tracking"
                    )
            
        except ValidationError as e:
            result.errors.extend(str(err) for err in e.errors())
            logger.error(f"Quote validation error: {e}")
        
        return result

    async def validate_rate_rules(
        self,
        rate: Rate
    ) -&gt; ValidationResult:
        """Validate rate against configurable business rules.
        
        Args:
            rate: Rate model to validate
            
        Returns:
            ValidationResult: Validation results including errors and suggestions
        """
        result = ValidationResult()
        
        # Get active rules for rate type
        rules = await self._get_active_rules(rate.type)
        
        for rule in rules:
            # Apply rule validation
            rule_result = await self._apply_rule(rate, rule)
            if not rule_result.is_valid:
                result.errors.extend(rule_result.errors)
                result.warnings.extend(rule_result.warnings)
        
        # Set validation status
        result.is_valid = len(result.errors) == 0
        
        return result

    async def _get_active_rules(self, rate_type: str) -&gt; List[Dict[str, Any]]:
        """Get active validation rules for rate type.
        
        Args:
            rate_type: Type of rate to get rules for
            
        Returns:
            List[Dict[str, Any]]: List of active rules
        """
        # TODO: Implement rule retrieval from database or configuration
        return []

    async def _apply_rule(
        self,
        rate: Rate,
        rule: Dict[str, Any]
    ) -&gt; ValidationResult:
        """Apply a single validation rule to a rate.
        
        Args:
            rate: Rate to validate
            rule: Rule to apply
            
        Returns:
            ValidationResult: Result of applying the rule
        """
        # TODO: Implement rule application logic
        return ValidationResult(is_valid=True)
</data>
  <data key="d2">C:\Users\Seth R\Desktop\AUL Quote App\warehouse_quote_app\app\services\validation\validation.py</data>
</node>
<edge source="add_rate_card" target="base_script"/>
<edge source="seed_rate_cards" target="logging"/>
<edge source="analyze_health" target="logging"/>
<edge source="watch_codebase" target="logging"/>
<edge source="file_watcher" target="logging"/>
<edge source="file_watcher" target="semantic_context_manager"/>
<edge source="generate_graph" target="logging"/>
<edge source="generate_graph" target="kg_generator"/>
<edge source="generate_graph" target="config"/>
<edge source="graph_updater" target="logging"/>
<edge source="graph_updater" target="ts_analyzer"/>
<edge source="graph_updater" target="semantic_context_manager"/>
<edge source="graph_updater" target="semantic_context_checker"/>
<edge source="kg_generator" target="logging"/>
<edge source="kg_generator" target="config"/>
<edge source="memory_enforcer" target="logging"/>
<edge source="semantic_context_checker" target="logging"/>
<edge source="semantic_context_checker" target="semantic_context_manager"/>
<edge source="semantic_context_manager" target="memory_enforcer"/>
<edge source="semantic_context_manager" target="logging"/>
<edge source="ts_analyzer" target="logging"/>
<edge source="__init__" target="semantic_context_manager"/>
<edge source="__init__" target="code_health_manager"/>
<edge source="__init__" target="ws"/>
<edge source="__init__" target="config"/>
<edge source="__init__" target="security"/>
<edge source="__init__" target="auth"/>
<edge source="__init__" target="admin"/>
<edge source="__init__" target="audit"/>
<edge source="__init__" target="logging"/>
<edge source="__init__" target="rate_limit"/>
<edge source="__init__" target="celery"/>
<edge source="__init__" target="session"/>
<edge source="__init__" target="base"/>
<edge source="__init__" target="user"/>
<edge source="__init__" target="customer"/>
<edge source="__init__" target="quote"/>
<edge source="__init__" target="rate"/>
<edge source="__init__" target="quote_report"/>
<edge source="__init__" target="service_report"/>
<edge source="__init__" target="customer_report"/>
<edge source="__init__" target="metrics"/>
<edge source="__init__" target="quotes"/>
<edge source="__init__" target="rates"/>
<edge source="__init__" target="email"/>
<edge source="__init__" target="realtime"/>
<edge source="__init__" target="validation"/>
<edge source="code_health_manager" target="logging"/>
<edge source="admin" target="quote"/>
<edge source="auth" target="logging"/>
<edge source="auth" target="user"/>
<edge source="customer" target="base"/>
<edge source="customer" target="quote"/>
<edge source="dependencies" target="jwt"/>
<edge source="service" target="jwt"/>
<edge source="service" target="dependencies"/>
<edge source="logging" target="logging"/>
<edge source="celery" target="celery"/>
<edge source="quote" target="base"/>
<edge source="quote" target="mixins"/>
<edge source="rate" target="base"/>
<edge source="user" target="base"/>
<edge source="rate_card" target="base"/>
<edge source="rate_integration" target="model"/>
<edge source="rate_integration" target="rag"/>
</graph></graphml>